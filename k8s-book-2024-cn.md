**《Kubernetes之书》**

### 尼格尔·波尔顿

本书可在http://leanpub.com/thekubernetesbook购买。

此版本发布于2024年2月。

#### ISBN 9781916585195

这是一本Leanpub书籍。Leanpub为作者和出版商提供了Lean Publishing过程。Lean Publishing是使用轻量级工具和多次迭代来发布正在进行中的电子书的行为，以获取读者反馈，调整至最佳状态，并在一旦达到目标后实现推广。

© 2017 - 2024 尼格尔·波尔顿

**推特此书！**

请通过推特帮助尼格尔·波尔顿传播这本书！

推荐推特内容：

我刚从@nigelpoulton这里购买了《Kubernetes之书》，迫不及待地开始阅读！

这本书的推荐标签是#kubernetes。

点击以下链接搜索此标签在推特上的相关讨论，了解其他人对这本书的评价：

#kubernetes

_教育是关于激发和创造机会的。我希望这本书和我录制的视频培训课程能激励您，并为您创造许多机会！非常感谢我的家人对我包容。我是一个认为自己是运行在中等生物硬件上的软件的极客。我知道和我一起生活并不容易。感谢所有观看我Pluralsight和A Cloud Guru培训视频的人。我喜欢与您们联系并且非常感谢多年来收到的所有反馈。这些反馈是激励我写这本书的动力。我相信您会喜欢它，并希望它能推动您的职业发展。@nigelpoulton_

## 目录



## 0：前言

Kubernetes发展迅速，因此我每年都会更新这本书。当我说“更新”时，
我是指真正的更新-我会审查每一个字和每一个概念，并测试每一个例子
以符合Kubernetes的最新版本。我百分之百致力于使这本书成为世界上最好的
Kubernetes书籍。

作为一名作者，我希望写一本书，并在五年内再也不用碰它。不幸的是，
一本两年的Kubernetes书籍可能已经过时了。

### 版本 纸质版、精装版、电子书、音频和翻译
以下版本的书籍可供选择：

- **平装本：**英文、简体中文、西班牙文、葡萄牙文
- **精装本：**英文
- **电子书：**英文、俄文、西班牙文、葡萄牙文

电子书可以在Kindle和Leanpub上获得。

以下是可供选择的收藏版。每个版本都有一个主题封面，但内容与常规的英文版完全相同。

- 克林贡平装本
- 博格精装本
- 斯特科特平装本

### 示例应用和GitHub仓库

有一个GitHub仓库，其中包含本书中使用的所有YAML和代码。

您可以使用以下命令克隆它。您需要安装**git**。这将在当前工作目录中创建一个名为**TheK8sBook**的新文件夹，其中包含您需要跟随示例的所有文件。

```
0:前言 2
```

```
$ git clone https://github.com/nigelpoulton/TheK8sBook.git
```

```
如果您从未使用过git，请不要担心。本书会引导您完成一切。
```

### Windows用户

几乎所有操作部分的命令都适用于Linux、Mac和Windows。然而，其中少部分命令在Windows上需要进行轻微的更改才能正常工作。每当出现这种情况时，我都会解释您需要在Windows上做些什么才能使其正常工作。然而，为了避免重复多次相同的内容，我并不总是告诉Windows用户将反斜杠替换为重音符号进行换行。

```
每次书中使用反斜杠将命令拆分成多行时，Windows用户应该执行以下操作之一：
- 删除反斜杠并将命令放在一行上运行
- 将反斜杠替换为重音符号
```

```
其他所有更改每次都会进行详细解释。
```

### 负责任的语言

```
本书遵循包容性命名倡议（inclusivenaming.org）的指南，旨在避免有害的术语并推广负责任的语言。
```

## 1: Kubernetes简介

本章将带您了解Kubernetes的基础知识和背景，分为以下几个部分：

- 重要的Kubernetes背景知识
- Kubernetes：云计算的操作系统

### 重要的Kubernetes背景知识

Kubernetes是一个容器化云原生微服务应用的编排器。

这里有很多术语，我们来解释一下。

**编排**

一个_编排器_是一个部署应用并根据变化动态响应的系统或平台。例如，Kubernetes能够：

- 部署应用
- 根据需求进行扩缩容
- 当应用出现故障时自动修复
- 执行零停机滚动更新和回滚
- 还有更多

最好的部分是，所有这些都不需要**你**参与。您需要在开始时配置一些东西，但一旦配置完成，您只需坐下来让Kubernetes施展其魔力。

**容器化**

_容器化_是将应用及其依赖项打包为映像，然后作为一个容器运行的过程。

可以将容器视为虚拟机（VM）的下一代。两者都是打包和运行应用程序的方式，但容器更小、更快、更便携。

尽管容器具有这些优势，但它们并未取代虚拟机，在大多数云原生环境中，容器和虚拟机通常并存。然而，对于大多数新应用程序来说，容器是首选解决方案。

**云原生**

_云原生应用_具备自动扩缩容、自愈、自动更新、回滚等云计算特性。

在公共云中简单运行常规应用程序**并不**使其成为_云原生应用程序_。

**微服务**

_微服务应用程序_由许多小型、专业化、独立的部分组成，这些部分共同构成一个有用的应用程序。

考虑一个电子商务应用程序，具有以下六个功能：

- Web前端
- 目录
- 购物车
- 身份验证
- 日志记录
- 商店

要将其设计为_微服务应用程序_，您需要将每个功能设计、开发、部署和管理为其自己的小型应用程序。我们将每个小应用程序称为_微服务_，这意味着该应用程序将有六个微服务。

这种设计通过允许这六个微服务拥有自己的小型开发团队和独立的发布周期，为您带来了巨大的灵活性。它还可以让您独立扩展和更新每个微服务。
最常见的模式是将每个微服务部署为独立的容器。这意味着一个或多个网页前端容器，一个或多个目录容器，一个或多个购物车容器等。扩展应用的任何部分都可以通过添加或删除容器来实现。

既然我们已经解释了一些内容，让我们重新改写一下开头那个充满行话的句子。

“Kubernetes是一个用于容器化云原生微服务应用程序的编排器。”我们现在知道这意味着：Kubernetes部署和管理打包为容器的应用程序，并可以轻松扩展、自动修复和更新。这应该能澄清一些主要的行业术语。但如果还有一些需要澄清的地方，不要担心，我们将在本书中详细讨论所有内容。

Kubernetes的起源

Kubernetes是由一群谷歌工程师开发的，部分原因是为了应对亚马逊网络服务（AWS）和Docker。

当亚马逊发明现代云计算时，改变了世界，每个人都需要赶上。谷歌是其中一家迎头赶上的公司。他们构建了自己的云计算平台，但需要一种方式来抽象出AWS的价值，并使客户尽可能轻松地从AWS迁移到他们的云平台上。他们还在数十亿个容器上运行一些生产应用程序，例如搜索和Gmail。

与此同时，Docker风靡全球，用户需要帮助管理爆炸性的容器增长。

在发生所有这些事情的同时，一群谷歌工程师利用他们在使用内部容器管理工具时所学到的经验，创建了一个名为Kubernetes的新工具。2014年，他们将Kubernetes开源，并将其捐赠给新成立的云原生计算基金会（CNCF）。

截至撰写本文时，Kubernetes已经有10年的历史，并且经历了令人难以置信的增长和采用。然而，在其核心，它仍然满足谷歌和整个行业的两个需求：

1. 抽象基础设施（例如AWS）
2. 简化在不同云平台之间迁移应用程序

这就是Kubernetes对行业来说如此重要的两个最大原因。

Kubernetes和Docker

早期的所有Kubernetes版本都与Docker捆绑在一起，并使用它作为运行时。这意味着Kubernetes使用Docker来执行创建、启动和停止容器等低级任务。然而，发生了两件事：

1. Docker变得臃肿
2. 人们创建了许多Docker的替代品

因此，Kubernetes项目创建了容器运行时接口（CRI），使运行时层可插拔。这意味着您可以根据需要选择最佳的运行时。例如，某些运行时提供更好的隔离性，而其他运行时则提供更好的性能。

Kubernetes 1.24最终删除了对Docker作为运行时的支持，因为它臃肿且超出了Kubernetes所需的范畴。从那以后，大多数新的Kubernetes集群都默认使用containerd（发音为“container dee”）作为运行时。幸运的是，containerd是针对Kubernetes进行优化的精简版Docker，并完全支持由Docker容器化的应用程序。实际上，Docker、containerd和Kubernetes都与实施开放容器倡议（OCI）标准的镜像和容器一起工作。

图1.2展示了一个运行多个容器运行时的四节点集群。

请注意，一些节点具有多个运行时。这种配置是完全支持的，并且越来越常见。在第14章中，当您将一个WebAssembly（Wasm）应用程序部署到Kubernetes时，您将使用这样的配置。

Docker Swarm如何？

在2016年和2017年，Docker Swarm、Mesosphere DCOS和Kubernetes竞争成为行业标准的容器编排器，而Kubernetes获胜。然而，Docker Swarm仍在积极开发中，并且在想要简单替代Kubernetes的小型公司中很受欢迎。

Kubernetes和Borg：抵抗是徒劳的！
我们已经说过，Google在很长时间内一直在大规模运行容器。嗯，用于编排这些数十亿个容器的两个内部工具称为_Borg_和_Omega_。因此，很容易将Kubernetes与它们联系起来-这三者都在大规模编排容器，并且与Google有关。
然而，Kubernetes并不是Borg或Omega的开源版本。它更像是与它们共享基因和家族历史。

```
图1.3-共享基因
```

```
目前，Kubernetes是由CNCF拥有的开源项目。它遵循Apache 2.0许可证，版本1.0于2015年7月发布，截至撰写本文时，我们已经到达1.29版本，并平均每年发布三个新版本。
```

**Kubernetes-名称的含义**

```
大多数人将Kubernetes发音为“koo-ber-net-eez”，但社区非常友好，人们不会介意您以不同的方式发音。
```

```
1：Kubernetes入门8
```

```
Kubernetes一词源于希腊语中的舵手或驾驶船只的人。您可以在徽标中看到这一点，那是一个舵轮。
```

```
图1.4-Kubernetes徽标
```

一些最初的工程师希望将Kubernetes命名为《星际迷航：航海家号》中著名的_Borg_无人机之后的《九号七》。版权法不允许这样做，所以他们给徽标添加了七个辐条，作为对《九号七》的微妙参考。
关于名称的最后一点。您经常会看到它被缩写为_K8s_，并发音为“kates”。数字8代替了“K”和“s”之间的八个字符。

### Kubernetes：云的操作系统

Kubernetes是云原生应用的事实上平台，我们有时将其称为“云的操作系统（OS）”。这是因为Kubernetes像Linux和Windows等操作系统一样，将云平台之间的差异抽象出来：

- Linux和Windows抽象服务器资源并调度应用程序进程
- Kubernetes抽象云资源并调度应用程序微服务

```
举个快速的例子，您可以将应用程序调度到Kubernetes上，而不必关心它是否在AWS，Azure，Civo Cloud，GCP还是您的本地数据中心上运行。这使得Kubernetes成为以下功能的关键推动者：
```

- 混合云
- 多云
- 云迁移

```
总之，Kubernetes使得今天部署到一个云端，明天迁移到另一个云端变得更容易。
```

1：Kubernetes入门9

**应用程序调度**

操作系统的主要功能之一是简化工作任务的调度。

计算机是由CPU、内存、存储和网络等硬件资源组成的复杂集合。值得庆幸的是，现代操作系统隐藏了大部分这些硬件细节，使应用程序开发的世界变得更加友好。例如，有多少开发人员需要关心他们的代码使用哪个CPU核心、内存DIMM或闪存芯片？大多数情况下，我们都将其交给操作系统处理。

Kubernetes在云和数据中心方面也做了类似的事情。

从高层次上讲，云或数据中心是一个复杂的资源和服务集合。Kubernetes可以抽象出其中的很多内容，并使其更易于使用。同样，您有多少次需要关心应用程序使用哪个计算节点、哪个失败区域或哪个存储卷？大多数情况下，我们乐意让Kubernetes来决定。

### 章节总结

Kubernetes由Google工程师根据多年来运行容器的经验创建。它作为开源项目捐赠给社区，现在是部署和管理云原生应用程序的行业标准平台。它可以在任何云端或本地数据中心上运行，并抽象底层基础设施。这使您可以构建混合云，并在不同云端之间进行迁移。它在Apache 2.0许可证下开源，并由云原生计算基金会（CNCF）拥有和管理。

不要害怕所有这些新术语。我在这里提供帮助，您可以通过以下任何方式与我联系：
- Twitter: @nigelpoulton
- LinkedIn: linkedin.com/in/nigelpoulton/
- Mastodon: @nigelpoulton@hachyderm.io
- 网站: nigelpoulton.com
- 电子邮件: tkb@nigelpoulton.com

## 2: Kubernetes运行原理

```
本章将介绍主要的Kubernetes技术，并为接下来的章节做准备。在本章结束时，您不需要成为一个专家。
```

我们将涵盖以下内容：

- Kubernetes从40,000英尺的高度
- 控制平面节点和工作节点
- 打包应用程序以适用于Kubernetes
- 声明式模型和期望状态
- Pod
- 部署
- 服务

### Kubernetes从40,000英尺的高度

```
Kubernetes既是一个集群，也是一个编排器。
```

**Kubernetes：集群**

```
Kubernetes集群是一个或多个节点，它们提供CPU、内存和其他资源供应用程序使用。
Kubernetes支持两种节点类型：
```

- 控制平面节点
- 工作节点

```
这两种类型可以是物理服务器、虚拟机或云实例，并且可以在ARM和AMD64/x86-64上运行。控制平面节点必须是Linux，但工作节点可以是Linux或Windows。
```

2: Kubernetes运行原理 11

_控制平面节点_ 实现了Kubernetes的智能功能，每个集群至少需要一个。但是，为了高可用性，您应该有三个或五个控制平面节点。

每个控制平面节点都运行每个控制平面服务，包括API服务器、调度器以及实现云原生功能（如自愈、自动缩放和滚动更新）的控制器。

_工作节点_ 用于运行用户应用程序。

图2.1显示了一个包含三个控制平面节点和三个工作节点的集群。

```
图2.1
```

在开发和测试环境中，将用户应用程序运行在控制平面节点上是常见的。然而，许多生产环境将用户应用程序限制在工作节点上，使得控制平面节点完全专注于集群操作。

控制平面节点也可以运行用户应用程序，但是在生产环境中，您可能应该强制用户应用程序在工作节点上运行。这样做可以使控制平面节点专注于管理集群。

**Kubernetes：编排器**

_编排器_ 是一个管理部署和管理应用程序的系统。

Kubernetes是业界标准的编排器，可以智能地在节点和故障区域之间部署应用程序，以实现最佳性能和可用性。它还可以在应用程序发生故障时修复它们，在需求变化时扩展它们，并管理零停机时间的滚动更新。

这就是大局观。让我们深入一点。

```
2: Kubernetes运行原理 12
```

### 控制平面节点和工作节点

我们已经说过Kubernetes集群是一个或多个_控制平面节点_和_工作节点_。

控制平面节点必须是Linux，而工作节点可以是Linux或Windows。
几乎所有云原生应用程序都是Linux的，并且将在Linux工作节点上运行。然而，如果您有云原生的Windows应用程序，您将需要运行一个或多个Windows工作节点。幸运的是，单个Kubernetes集群可以同时具有Linux和Windows工作节点的混合，并且Kubernetes足够智能，可以将应用程序调度到正确的节点上。

**控制平面**

```
控制平面是Kubernetes的系统服务集合，它实现了Kubernetes的核心功能。它提供API、调度任务、实现自愈、管理扩展操作等。
最简单的设置是运行单个控制平面节点，最适合实验室和测试环境。然而，正如前面提到的，您应该在生产环境中运行三个或五个控制平面节点，并将它们分布在可用性区域中以实现高可用性，如图2所示。
```

```
图2.2 控制平面高可用性
```

```
如前所述，有时将所有用户应用程序运行在工作节点上被认为是生产最佳实践，这样控制平面节点可以将所有资源分配给集群相关操作。
大多数集群在每个控制平面节点上运行每个控制平面服务，以实现高可用性。
```

2: Kubernetes运行原理 13

让我们来看看组成控制平面的服务。

**API服务器**

_API服务器_ 是Kubernetes的前端，所有更改和查询集群状态的请求都通过它进行。即使内部的控制平面服务也是通过API服务器相互通信。

它通过HTTPS提供RESTful API，所有请求都需要进行身份验证和授权。例如，部署或更新应用程序的过程如下：

```
1. 使用YAML配置文件描述需求
```
2. **将配置文件发布到API服务器**
   3. 请求将进行身份验证和授权
   4. 更新将持久保存在集群存储中
   5. 更新将被安排到集群中

**集群存储**

集群存储保存了所有应用程序和集群组件的期望状态，是控制平面中唯一有状态的部分。

它基于_etcd_分布式数据库，大多数Kubernetes集群在每个控制平面节点上运行一个etcd副本以实现高可用性（HA）。然而，经历高频变化的大型集群可能会运行一个单独的etcd集群以获得更好的性能。

需要注意的是，高可用性的集群存储并不能替代备份和恢复。当出现问题时，您仍然需要适当的方式来恢复集群存储。

关于可用性，etcd更倾向于使用奇数个副本以避免“分裂大脑”情况。这是指副本之间出现通信问题，无法确定是否具备大多数（quorum）。

图2.3显示了两个etcd配置遇到网络分区的情况。左侧集群有四个节点，正在遇到分裂大脑的情况，两个节点分别位于两侧，均无法取得多数。右侧集群只有三个节点，但没有出现分裂大脑情况，因为**NodeA**知道自己没有多数，而**NodeB**和**NodeC**知道自己具备多数。

2：Kubernetes操作原则 14

```
图2.3. HA和分裂大脑情况
```

如果发生分裂大脑情况，etcd将进入只读模式，阻止对集群的更新。用户应用程序仍将继续运行，只是您将无法进行集群更新，如添加或修改应用程序和服务。

与所有分布式数据库一样，写入的一致性至关重要。例如，需要处理来自不同源的对同一值的多次写入。etcd使用_RAFT_一致性算法来实现这一点。

**控制器和控制器管理器**

Kubernetes使用控制器来实现大部分集群智能功能。它们都在控制平面上运行，其中一些常见的控制器包括：

- 部署控制器
- 有状态集控制器
- 副本集控制器

还有其他控制器，我们将在本书后面介绍其中一些。然而，它们都作为后台监视循环运行，将观察到的状态与期望的状态进行调和。

这是许多术语，我们将在本章后面详细介绍。但是现在，它意味着控制器确保集群运行您要求运行的内容。例如，如果您要求三个应用程序副本，控制器将确保有三个健康的副本运行，并在出现问题时采取适当的措施。

Kubernetes还运行一个控制器管理器，负责生成和管理各个控制器。

图2.4概述了控制器管理器和控制器的高级视图。

2：Kubernetes操作原则 15

```
图2.4. 控制器管理器和控制器
```

**调度器**

调度器会监视API服务器以获取新的工作任务，并将其分配给健康的工作节点。

它执行以下过程：

```
1. 监视API服务器以获取新的任务
2. 确定合适的节点
3. 将任务分配给节点
```

确定合适的节点涉及谓词检查、过滤和排名算法。它会检查节点的限制、亲和性和反亲和性规则、网络端口可用性以及可用的CPU和内存。它会忽略无法运行任务的节点，并根据因素（例如是否已具备所需镜像、可用的CPU和内存量，以及当前运行的任务数量）对其余节点进行排名。每个因素都有所得分，得分最高的节点将被选中来运行任务。

如果调度器找不到合适的节点，它会将任务标记为待定。

如果集群配置为进行节点自动扩展，待定任务将触发集群自动扩展事件，添加一个新节点并将任务安排到新节点上。

2：Kubernetes操作原则 16

**云控制器管理器**

如果您的集群在公共云上，例如AWS、Azure、GCP或Civo Cloud，它将运行一个云控制器管理器，将集群与云服务（例如实例、负载均衡器和存储）进行集成。例如，如果您在云上，并且应用程序请求一个负载均衡器，云控制器管理器将提供云负载均衡器之一，并将其连接到您的应用程序。

**控制平面概述**
控制平面实现了Kubernetes的核心功能，包括API服务器、调度器和集群存储。它还实现了确保集群运行我们要求的内容的控制器。

图2.5显示了一个Kubernetes控制平面节点的高级视图。

```
图2.5-控制平面节点
```

为了实现高可用性，您应该运行三个或五个控制平面节点，而大型繁忙的集群可能会运行一个单独的etcd集群以提供更好的集群存储性能。

API服务器是Kubernetes的前端，**所有**通信都通过它进行。

**工作节点**

_工作节点_用于运行用户应用程序，如图2.6所示。

2：Kubernetes的工作原理 17

```
图2.6-工作节点
```

让我们来看看主要的工作节点组件。

**Kubelet**

_kubelet_是主要的Kubernetes代理，负责与集群的所有通信。

它执行以下关键任务：

- 监视API服务器以获取新任务
- 指示适当的运行时执行任务
- 向API服务器报告任务的状态

如果任务无法运行，kubelet会向API服务器报告问题，并让控制平面决定采取什么操作。

**运行时**

每个工作节点都有一个或多个运行时来执行任务。

大多数新的Kubernetes集群预先安装了**containerd**运行时，并使用它来执行任务。这些任务包括：

- 拉取容器镜像
- 管理生命周期操作，如启动和停止容器

```
2：Kubernetes的工作原理 18
```

```
旧版本的集群使用Docker运行时，但现在不再受支持。RedHat OpenShift集群使用CRI-O运行时。还有很多其他的运行时，每种运行时都有其优缺点。
```

在Wasm章节中，我们将使用一些不同的运行时。

```
Kube-proxy
```

```
每个工作节点都运行一个kube-proxy服务，用于实现集群网络和将流量负载均衡到运行在该节点上的任务。
现在您已经了解了控制平面和工作节点的基本原理，让我们切换到如何打包应用程序以在Kubernetes上运行。
```

### 打包应用程序以适应Kubernetes。

```
Kubernetes可以运行容器、虚拟机、Wasm应用程序等等。但是，它们都必须被包装在Pod中才能在Kubernetes上运行。
```

我们将很快介绍Pods，但现在，将其视为一个薄薄的包装，用于将不同类型的任务抽象为可以在Kubernetes上运行的形式。以下的快递类比可能有所帮助。
快递公司允许您运送图书、衣物、食品、电器等等，只要您使用他们认可的包装和标签。一旦您将货物打包并贴上标签，就将其交给快递公司进行投递。然后，快递公司处理复杂的物流问题，如使用哪种飞机和卡车、将货物安全转交给当地的投递中心以及最终交付给客户。他们还提供跟踪包裹、更改投递细节和确认成功投递的服务。而**您**只需打包和标记货物。
在Kubernetes上运行应用程序类似。Kubernetes可以运行容器、虚拟机、Wasm应用程序等等，只要将它们包装在Pod中。一旦在Pod中包装好，您就可以将应用程序交给Kubernetes运行。这包括选择适当的节点、加入网络、附加卷等复杂的物流问题。Kubernetes甚至允许您查询应用程序并进行更改。
考虑一个快速的示例。
您使用自己喜欢的编程语言编写一个应用程序，将其容器化，推送到一个注册表，并将其包装在一个Pod中。此时，您可以将Pod交给Kubernetes，Kubernetes将运行它。然而，大多数情况下，您将使用更高级的控制器来部署和管理Pod。为此，您将Pod包装在一个控制器对象中，例如一个_Deployment_。

2：Kubernetes的工作原理 19

暂时不用担心细节，我们将在本书的后面以更深入的方式和大量示例来详细讨论所有内容。现在，您只需要知道两件事：

```
1.应用程序需要被包装在Pod中才能在Kubernetes上运行
2.Pod通常被包装在高级控制器中以获取高级功能
```

让我们快速回到快递类比，以帮助解释控制器的作用。

大多数快递公司提供额外的服务，如货物保险、交付的签名和照片证明、快速交付服务等等。所有这些增值服务都提升了服务的价值。
再次强调，Kubernetes是相似的。它实现了一些增加价值的控制器，比如确保应用程序的健康状况，在需求增加时自动扩展等功能。

图2.7显示了一个被Pod包装的容器，而Pod本身则被Deployment包装。暂时不要担心YAML配置，它只是为了引入这个概念。

```
图2.7-对象嵌套
```

重要的是要理解，每个包装层都添加了一些东西：

- 容器包装了应用程序并提供依赖项
- Pod包装了容器，使其能够在Kubernetes上运行
- Deployment包装了Pod，并添加了自愈、扩展等功能

您将Deployment（YAML文件）作为应用程序的“期望状态”发布到API服务器，Kubernetes将其实现。

说到期望状态...

2：Kubernetes操作原则20

###声明性模型和期望状态

声明性模型和期望状态是Kubernetes运行方式的核心。它们遵循三个基本原则：

- 观察状态
- 期望状态
- 协调

“观察状态”是您拥有的状态，“期望状态”是您想要的状态，“协调”是保持观察状态与期望状态同步的过程。

```
术语：我们使用实际状态、当前状态和观察状态这些术语指的是同一件事情——集群的最新视图。
```

在Kubernetes中，声明性模型的工作方式如下：

```
1.您在YAML清单文件中描述应用程序的期望状态
2.您将YAML文件发布到API服务器
3.它作为意图记录在集群存储中
4.控制器注意到集群的观察状态与新的期望状态不匹配
5.控制器进行必要的更改以协调差异
6.控制器在后台持续运行，确保观察状态与期望状态匹配
```

让我们仔细看一下。

您使用YAML编写清单文件，告诉Kubernetes应用程序应该是什么样子。我们称之为期望状态，通常包括使用哪些镜像，有多少副本以及哪些网络端口等信息。

创建清单文件后，您将其发布到API服务器进行身份验证和授权。将YAML文件发布到Kubernetes的最常见方式是使用kubectl命令行实用程序。

进行身份验证和授权后，配置将作为意图记录持久化到集群存储中。

此时，集群的观察状态与您的新期望状态不匹配。控制器会注意到这一点，并开始协调的过程。这将涉及执行YAML文件中描述的所有更改，可能包括调度新的Pod、拉取镜像、启动容器、将其连接到网络并启动应用程序进程。

完成协调后，观察状态将与期望状态匹配，一切都会正常。然而，控制器会继续在后台运行，准备协调任何未来的差异。

重要的是要理解，我们所描述的与传统的“命令模型”非常不同：

- “命令模型”需要复杂的平台特定命令脚本来实现最终状态
- “声明性模型”是一种简单的与平台无关的描述最终状态的方式

Kubernetes支持两种模型，但更偏好采用“声明性模型”。这是因为声明性模型与版本控制系统集成，实现了自愈、自动缩放和滚动更新等功能。

让我们看几个简单的声明性示例。

假设您从一个YAML文件部署了一个应用程序，请求十个副本。如果运行两个副本的节点失败，观察状态将下降到8个副本，并不再与期望状态的10个副本匹配。没关系，控制器会注意到这一差异，并安排2个新副本，将总数恢复到10个。

对于应用程序的更新也是如此。例如，如果您更新YAML文件，告诉应用程序使用更新的镜像并将更改发布到Kubernetes，相关的控制器将注意到差异，并用运行新版本的新副本替换运行旧版本的副本。

如果您试图以命令式的方式执行此类更新，您将需要编写复杂的脚本来管理、监视和健康检查整个更新过程。而以声明性方式，您只需要更改一行YAML代码，Kubernetes会完成其他所有工作。

这非常强大，是Kubernetes工作方式的基础。

###Pods。
VMware世界中调度的原子单位是虚拟机（VM）。在Docker世界中，它是容器。在Kubernetes中，它是Pod。

是的，Kubernetes可以运行容器、虚拟机、Wasm应用程序等等。但它们都需要包装在Pod中。

2：Kubernetes的运行原则 22

**Pod和容器**

最简单的配置是每个Pod运行一个容器，这就是为什么我们有时会将Pod和容器这两个术语交替使用。然而，多容器Pod也有强大的用例，包括：

- 服务网格
- 初始化应用程序环境的辅助服务
- 具有紧密耦合的辅助功能的应用程序，例如日志收集器

图2.8显示了一个带有主应用程序容器和服务网格辅助容器的多容器Pod。Sidecar是同一Pod中与主应用程序容器一起运行的辅助容器的行话。在图2.8中，服务网格辅助容器加密进出主应用程序容器的网络流量并提供遥测服务。

```
图2.8-多容器服务网格Pod
```

多容器Pod还帮助我们实现"单一职责原则"，即每个容器执行一个简单的任务。在图2.8中，主应用程序容器可能提供消息队列或其他核心应用程序功能。我们将加密和遥测逻辑添加到主应用程序中，而是将其保持简单，并在同一Pod中运行的服务网格容器中实现。

**Pod的结构**

每个Pod是一个共享的执行环境，用于一个或多个容器。执行环境包括网络堆栈、卷、共享内存等。

单容器Pod中的容器拥有执行环境，而多容器Pod中的容器共享执行环境。

例如，图2.9显示了一个多容器Pod，其中两个容器共享Pod的IP地址。主应用程序容器在Pod外部可访问，地址为10.0.10.15:8080，而辅助容器在10.0.10.15:5005可访问。如果它们需要相互通信，可以使用Pod的localhost接口实现容器对容器的通信。

```
图2.9-共享Pod IP的多容器Pod
```

当您的应用程序具有需要共享内存或存储等资源的紧密耦合组件时，应选择多容器Pod。在大多数其他情况下，应使用单容器Pod，并通过网络进行松散耦合。

**Pod调度**

Pod中的所有容器总是被调度到同一个节点上。这是因为Pod是一个共享的执行环境，很难在节点之间共享内存、网络和卷。

启动Pod也是一个原子操作。这意味着只有在所有容器都启动后，Kubernetes才会将Pod标记为运行中。例如，如果一个Pod有两个容器，但只有一个容器启动了，那么该Pod将不会准备好。

**Pod作为扩展的单位**

Pod是Kubernetes中的最小调度单位。因此，扩展应用程序是通过添加更多的Pods来实现的，缩小应用程序是通过删除Pods来实现的。**不要**通过向现有Pods添加更多的容器来进行扩展。图2.10展示了如何使用Pods作为扩展单位来扩展web-fe微服务。

```
图2.10-使用Pods进行扩展
```

**Pod的生命周期**

Pod是有限的——它们被创建、存在和消亡。每当一个Pod消亡时，Kubernetes会用一个新的Pod来替换它。即使新的Pod看起来、闻起来和感觉起来与旧的Pod相同，它始终是一个全新的Pod，具有新的ID和新的IP。

这迫使您设计应用程序具有松散耦合性，对个别Pod的故障具有免疫性。

**Pod的不可变性**

Pod是不可变的。这意味着一旦Pod运行起来，就不会对其进行更改。

例如，如果您需要更改或更新一个Pod，应该始终用运行更新的新Pod替换它。不应该登录到一个Pod上并对其进行更改。这意味着每当我们谈论"更新Pods"时，我们总是指删除旧的Pod，并用新的Pod来替换它。对于我们中的一些人来说，这可能是一种巨大的心态转变，但它与现代工具和GitOps风格的工作流非常契合。

2：Kubernetes的运行原则 25

### 部署
尽管Kubernetes可以使用Pods，但你几乎总是通过更高级的控制器（如Deployments、StatefulSets和DaemonSets）来部署它们。这些控制器都在控制平面上运行，并作为后台监控循环工作，将观察到的状态与期望的状态进行协调。

Deployments为无状态应用程序添加了自愈、扩展、滚动更新和版本回滚功能。

请参考图2.7，了解Deployments如何包装Pods。

### 服务对象和稳定的网络

在本章前面，我们提到Pods是有生命限制的，它们可以死亡。然而，如果它们由控制器管理，它们将被具有新ID和新IP地址的新Pods替换。同样的事情也会在滚动升级和扩展操作中发生：

- 滚动升级用新的Pods替换旧的Pods，具有新的IP地址
- 扩展操作会添加具有新IP地址的新Pods
- 缩减操作会删除现有的Pods

这些事件会产生IP变动，并使Pods变得不可靠。例如，客户端不能与个别Pods建立可靠的连接，因为不能保证Pods会在那里。

这就是Kubernetes的服务（Services）发挥作用的地方，它为一组Pods提供可靠的网络连接。

图2.11显示了内部和外部客户端通过Kubernetes服务连接到一组Pods。服务（Service）（大写“S”是因为它是Kubernetes API对象）提供可靠的名称、IP和负载均衡请求到其后面的Pods。

你应该将服务（Services）看作有前端和后端。前端有一个DNS名称、IP地址和网络端口。后端使用标签来负载均衡跨一组动态Pods的流量。

由于扩展事件、滚动升级和故障会导致Pods的出现和消失，服务（Services）会保持一个健康Pods的列表。这意味着它们总是均衡地分配流量到活跃的健康Pods上。服务还保证前端的名称、IP和端口不会发生变化。

### 章节总结

本章介绍了一些重要的Kubernetes特性。

控制平面节点承载了实现Kubernetes智能的控制平面服务。它们可以是物理服务器、虚拟机、云实例等。生产集群通常运行三个或五个控制平面节点以实现高可用性。

控制平面服务包括API服务器、调度器、集群存储和控制器。

工作节点用于运行用户应用程序，也可以是物理服务器、虚拟机、云实例等。每个工作节点都运行kubelet服务，该服务监视API服务器上的新工作任务，并报告任务状态。

工作节点还具有一个或多个运行时和kube-proxy服务。运行时执行低级操作，如启动和停止容器和Wasm应用程序。kube-proxy处理节点上的所有网络任务。

你了解到Kubernetes支持声明性和命令式方法来部署和管理应用程序，但更倾向于使用声明性方法。这就是你将所需状态描述在YAML配置文件中，并将其提供给Kubernetes进行部署和管理的地方。控制器在控制平面上运行，并将观察到的状态与期望的状态进行协调。

你还了解到Pods、Deployments和Services。Pods允许容器和其他工作负载在Kubernetes上运行。Deployments添加了自愈、扩展和滚动升级功能。Services添加了可靠的网络连接和基本的负载均衡。

## 第三章：获取Kubernetes

本章介绍了获取Kubernetes集群的几种方式，以便您可以在整本书中进行实际操作的示例。

您将学习以下内容：

```
1. 在您的笔记本电脑上创建一个Kubernetes集群（免费）
2. 在云端创建一个托管的Kubernetes集群（需要花钱）
```

有很多获取Kubernetes的方法，我们无法涵盖所有方法。然而，我精选了两种简单的方法，它们能让您轻松地跟随本书中的大部分示例。您也可以使用其他集群，但是一些实际操作示例可能会有细微差异。

笔记本电脑示例使用Docker Desktop在一个单节点的Kubernetes集群中构建。我推荐大多数读者选择这个选项，因为它是免费的，并且您几乎可以跟随所有的示例。

云端示例在Google Cloud中构建一个生产级的Google Kubernetes Engine（GKE）集群。它易于构建和使用，但**需要花费一些钱！**只有在您愿意花钱的情况下才选择这个选项。

### 在您的笔记本电脑上创建一个Kubernetes集群

本节将指导您使用Docker Desktop构建一个单节点的Kubernetes集群。

您需要完成以下步骤来构建该集群：
- 安装Docker Desktop
- 启用Docker Desktop内置的Kubernetes集群
- 测试您的集群

(^3) Docker Desktop可以个人和教育用途免费使用。如果您在工作中使用它，并且您的公司拥有超过250名员工或年收入超过1000万美元，您需要支付许可费。

```
3: 获取Kubernetes 29
```

**安装Docker Desktop**

```
Docker Desktop是在您的笔记本电脑上获取Docker、Kubernetes和kubectl最简单的方法。您还可以获得一个漂亮的用户界面，方便在kubectl上下文之间切换。
kubectl是Kubernetes的命令行实用工具，您将需要它来执行本书中的所有示例。
kubectl上下文是一组设置，告诉kubectl应向哪个集群发出命令并使用哪些凭据进行身份验证。稍后您将了解更多相关信息。
按照以下简单步骤安装Docker Desktop：
```

```
1. 在网络上搜索Docker Desktop
2. 下载适用于您系统的安装程序（Linux、Mac或Windows）
3. 运行安装程序并按照下一步的指示进行操作
```

Windows用户在提示时应安装WSL 2子系统。

```
安装完成后，您可能需要手动启动应用程序。Mac用户在运行时菜单栏顶部会出现一个鲸鱼图标，而Windows用户在系统托盘底部会出现鲸鱼图标。单击鲸鱼图标会显示一些基本控制选项，并显示Docker Desktop是否正在运行。
打开终端并运行以下命令，以确保Docker和kubectl已安装并正常工作。
```

```
$ docker --version
Docker版本 25.0.2，构建 29cf629
```

```
$ kubectl version --client=true -o yaml
clientVersion:
compiler: gc
gitVersion: v1.29.1
major: "1"
minor: "29"
platform: darwin/arm64
```

**启用Docker Desktop内置的Kubernetes集群**

```
单击菜单栏或系统托盘中的Docker鲸鱼图标，选择“设置”选项。
从左侧导航栏选择Kubernetes，勾选“启用Kubernetes”选项，然后点击“应用并重启”。
```

3: 获取Kubernetes 30

Docker Desktop将花费一两分钟来拉取所需的镜像并启动集群。当集群启动并运行时，Docker Desktop窗口左下角的Kubernetes图标将变为绿色。

**测试您的集群**

运行以下命令以确保集群已启动并运行，并且您的kubectl上下文已设置。

$ kubectl get nodes
名称 状态 角色 年龄 版本
docker-desktop 就绪 控制平面 93天 v1.29.1

恭喜您，在您的笔记本电脑上构建了一个可以用于本书大部分实例的Kubernetes集群。由于一些实例利用了Google Cloud上的高级存储功能，您将无法在某些存储示例中使用该集群。您还将为WebAssembly章节构建一个不同的集群。

### 在云中创建托管的Kubernetes集群。

此选项需要支付费用。在创建此集群之前，请确保您了解相关费用。我还建议您在使用完毕后立即删除它。我通常在每晚删除我的集群，并且只在打开本书并想尝试一些实例时才创建新的集群。

所有主要的云平台都提供托管的Kubernetes服务。这是一种模式，云提供商构建集群并管理高可用性（HA）、性能和更新等事项。

并非所有托管的Kubernetes服务都相同，但它们通常是您能够获得的最轻松的生产级Kubernetes集群。例如，Google Kubernetes Engine（GKE）是一个托管服务，可以创建高性能、高可用性的集群，并默认实现安全最佳实践。只需几个简单的点击和您的信用卡信息。

其他常见的托管Kubernetes服务包括：

- AWS：弹性Kubernetes服务（EKS）
- Azure：Azure Kubernetes服务（AKS）
- Civo Cloud Kubernetes
- DigitalOcean：DigitalOcean Kubernetes（DOKS）

```
3: 获取Kubernetes 31
```

- Google Cloud平台：Google Kubernetes Engine（GKE）
- Linode：Linode Kubernetes Engine（LKE）

我们将创建一个GKE集群，并完成以下所有步骤：

- GKE先决条件
- 创建GKE集群
- 测试您的GKE集群

**GKE先决条件**

```
GKE是Google Cloud Platform（GCP）上的托管Kubernetes服务。与大多数托管Kubernetes服务一样，它提供：
```
- 一个快速简便的方法创建一个生产级别的集群
- 一个受管控的控制平面
- 细项计费
- 与其他服务（如负载均衡器、卷、服务网格等）的集成

创建一个GKE集群，您需要一个已经配置了计费的Google Cloud账户和一个空白项目。这些都很容易设置，本节的其余部分假设您已经拥有它们。
您还需要gcloud命令行界面。访问https://cloud.google.com/sdk/，点击开始按钮，按照您所使用平台的版本进行安装。
安装程序会自动安装kubectl命令行工具。作为安装的一部分，您将被提示运行gcloud auth login命令以授权访问您的Google Cloud项目。这将打开一个浏览器会话，您需要按照提示进行操作并接受授权。

创建一个GKE集群

在你拥有一个新的Google Cloud项目并且安装了gcloud命令行界面后，请按照以下步骤创建一个新的GKE集群。

1. 访问https://console.cloud.google.com/，然后在左侧的导航窗格中选择Kubernetes Engine > 集群。您可能需要点击左上角的三个水平线（汉堡包图标）以显示导航窗格。
2. 选择创建集群的选项，然后选择切换到标准集群的选项。不要创建AutoPilot集群，因为目前这些不适用于所有示例。您将被提示确认您要从AutoPilot切换到标准模式。
3. 给您的集群取一个有意义的名字。本书中的示例将使用gke-tkb。
4. 在位置类型中选择一个区域性集群。本书中的某些示例只适用于区域性集群。
5. 为您的集群选择一个区域。
6. 点击Release channel并从Rapid channel中选择最新版本。
7. 从左侧导航菜单中点击default-pool，并在Size部分将每个区域的节点数设置为1。
8. 可以自由探索其他设置。但请不要更改任何设置，因为它们可能会影响本书后面的示例。
9. 当您对配置和预计的每月费用满意后，点击创建。

创建集群需要几分钟的时间。

测试您的GKE集群

您的Google Cloud控制台的集群页面显示了您项目中Kubernetes集群的高级概览。请随意浏览并熟悉一些设置。
点击您的新集群右侧的三个点以显示连接选项。命令行访问部分提供了一个长的gcloud命令，用于配置kubectl与您的集群通信。将该命令复制到剪贴板，并在终端中运行它。

$ gcloud container clusters get-credentials gke-tkb --region...
正在获取集群终端节点和认证数据。
为gke-tkb生成kubeconfig条目。

当命令完成后，运行以下kubectl get nodes命令以列出集群中的节点。

$ kubectl get nodes
名称 状态 角色 版本
gke-gke-tkb-default...h2gp 准备就绪 <无> v1.29.0-gke.1381000
gke-gke-tkb-default...l29b 准备就绪 <无> v1.29.0-gke.1381000
gke-gke-tkb-default...qzv6 准备就绪 <无> v1.29.0-gke.1381000

节点名称和Kubernetes版本应与您创建的GKE集群相关。

注意，所有节点在ROLES列下都有<无>。这是因为GKE是一个托管平台，只允许您看到工作节点。GKE管理控制平面节点并将其隐藏起来。

如果您收到关于认证插件废弃的警告，请按照链接文章中的说明进行操作。

恭喜您！您拥有一个适用于大多数实践示例的生产级别的Kubernetes集群。在WebAssembly章节中，您将构建一个不同的集群。

警告。务必及时删除集群，以避免不必要的费用。我建议每天删除集群，并在每次需要集群时重新创建一个新的。这样做将显然删除您在删除的集群上创建的任何内容。

使用kubectl工作




**kubectl** 是Kubernetes的命令行工具，在所有的实例中都会用到它。如果你按照安装集群的指示安装了其中之一，那么你已经有了它。

在终端窗口中输入 **kubectl** 来检查是否已安装。如果没有安装，可以在网上搜索“安装kubectl”并按照系统的指示进行安装。

**kubectl** 的版本与你的集群版本相差不能超过一个次要版本。例如，如果你的集群正在运行Kubernetes 1.29.x，那么你的**kubectl**版本应该不低于1.28.x，也不高于1.30.x。

从高层次来看，**kubectl** 将用户友好的命令转换为HTTP REST请求，并将其发送至API服务器。在后台，它会读取一个名为_kubeconfig_的文件，以了解要发送命令的集群和使用哪些凭据。

kubeconfig文件名为**config**，存放在你的主目录的隐藏文件夹**.kube**中。它包含以下定义：

- 集群

- 用户（凭据）

- 上下文

_集群_ 是**kubectl**知道的Kubernetes集群列表，并且允许单个**kubectl**安装管理多个集群。每个集群定义都有一个名称、证书信息和API服务器终点。

_用户_ 是用户凭据的列表。例如，你可能有一个_dev_用户和一个_ops_用户，他们具有不同的权限。这些用户都存在于kubeconfig文件中，并具有友好的名称和一组凭据。如果你使用X.509证书，Kubernetes使用的用户名和组名嵌入在证书中。

_上下文_ 是**kubectl**将集群和用户分组在一个友好名称下的方式。例如，你可能有一个名为**ops-prod**的上下文，它将**ops**用户凭据与**prod**集群组合在一起。使用此上下文，**kubectl**将向**prod**集群的API服务器发送命令，并作为**ops**用户进行身份验证。

下面是一个包含一个名为**shield**的集群、一个名为**coulson**的用户和一个名为**director**的上下文的简单kubeconfig文件。**director**上下文将**coulson**用户和**shield**集群组合在一起，并设置为默认上下文。

apiVersion: v1
kind: Config
clusters: <<==== 此块中的集群定义

- name: shield <<==== 集群的友好名称
  cluster:
  server: https://192.168.1.77:8443 <<==== 集群的API终点
  certificate-authority-data: LS0tLS1CRUdJTiBDRVJ <<==== 集群的证书
  users: <<==== 此块中的用户定义
- name: coulson <<==== Kubernetes未使用的友好名称
  user:
  client-certificate-data: LS0tLS1CRUdJTiBDRV... <<==== 用户证书
  client-key-data: LS0tLS1CRUdJTiBFQyB <<==== 用户私钥
  contexts: <<==== 此块中的上下文
- context:
  name: director <<==== 名为"director"的上下文
  cluster: shield <<==== 发送命令到该集群
  user: coulson <<==== 以该用户进行身份验证
  current-context: director <<==== kubectl将使用该上下文

可以通过运行**kubectl config view**命令来查看你的kubeconfig文件。该命令将隐藏敏感数据。

可以使用**kubectl config current-context**命令查看当前上下文。下面的示例显示了一个系统，**kubectl**配置为使用**docker-desktop**上下文中定义的集群和用户。

$ kubectl config current-context
docker-desktop

可以通过运行**kubectl config use-context**命令来更改当前上下文。下面的命令将当前上下文设置为**hpa-test**。只有当你的kubeconfig文件有一个名为**hpa-test**的有效上下文时，该命令才会生效。

$ kubectl config use-context hpa-test
已切换到上下文 "hpa-test"。

$ kubectl config current-context
hpa-test

如果你安装了Docker Desktop，可以通过点击Docker鲸鱼并选择**KubernetesContext**选项来轻松切换**kubectl**上下文。

### 章节总结

本章向你展示了几种获取Kubernetes集群的方法。当然，还有很多其他选项。

像Docker Desktop、k3d、KinD和minikube这样的选项是在笔记本电脑或其他个人设备上获取本地开发集群的好方法。
Docker Desktop附带了完整的Docker开发工具套件，并自动安装了**kubectl**。它还附带了一个可选的单节点Kubernetes集群。k3d和KinD构建在Docker Desktop之上的多节点Kubernetes集群。

您已经学会了如何在Google Cloud（GKE）中启动托管的Kubernetes集群。然而，这需要花钱，您应该在不使用时始终删除它。

本章还概述了Kubernetes命令行工具**kubectl**。

## 4:与Pods一起工作

```
Kubernetes上的每个应用都在Pod内部运行。
```

- 当您部署一个应用时，您将其部署在一个Pod中
- 当您终止一个应用时，您终止其所在的Pod
- 当您扩展一个应用时，您添加更多的Pods
- 当您缩减一个应用时，您删除Pods
- 当您更新一个应用时，您部署新的Pods

```
这使得Pods变得重要，这也是为什么本章详细讲解的原因。
本章主要分为两个部分：
```

- Pod理论
- Pods的实践操作

```
如果我们即将涵盖的内容中有一些感觉熟悉，那是因为我们正在构建在第2章引入的一些概念之上。
```

我们即将发现Kubernetes使用Pods来运行许多不同的工作负载类型。然而，大多数情况下，Pods运行容器，因此我们将在大多数示例中引用容器。

### Pod理论。

```
Kubernetes出于许多原因使用Pods。它们是一个抽象层，它们使资源共享、添加功能、增强调度等。
让我们更详细地看一下其中一些。
```

```
Pods是一个抽象层
```

```
Pods抽象了不同工作负载类型的细节。这意味着您可以在其中运行容器、虚拟机、无服务器函数和Wasm应用程序，而Kubernetes并不知道区别。
使用Pods作为一个抽象层对Kubernetes以及工作负载都有好处：
```

4: 与Pods一起工作 37

- _Kubernetes_ 可以专注于部署和管理Pods，而不必关心它们内部的内容
- 不同类型的 _工作负载_ 可以在同一个集群上并行运行，利用声明式Kubernetes API的全部功能，并获得Pods的所有其他益处

容器和Wasm应用程序使用标准Pods、标准工作负载控制器和标准运行时。然而，无服务器函数和虚拟机需要一些额外的帮助。

无服务器函数在标准Pods中运行，但需要像Knative^4这样的应用程序来扩展API以使用自定义资源和控制器。虚拟机类似，需要像KubeVirt^5这样的应用程序来扩展API。

图4.1显示了四种不同的工作负载在同一个集群上运行。每个工作负载都包装在一个Pod中，由一个控制器进行管理，并使用标准运行时。虚拟机工作负载在_VirtualMachineInstance (VMI)_中运行，而不是在Pod中，但VMIs与Pods非常类似，并利用了许多Pod的特性。

```
图4.1-包装在Pods中的不同工作负载
```

**Pods增强工作负载**

Pods以许多方式增强工作负载，包括以下所有内容：

- 资源共享
- 高级调度
- 应用程序健康检查
- 重启策略
- 安全策略
- 终止控制

(^4) https://knative.dev/
(^5) https://kubevirt.io/

4: 与Pods一起工作 38

- 卷

以下命令显示了Pod属性的完整列表，并返回超过1000行。

$ kubectl explain pods --recursive
KIND: Pod
VERSION: v1
DESCRIPTION:
Pod是一组可以在主机上运行的容器。此资源由客户端创建并安排到主机上。
FIELDS:
apiVersion <string>
kind <string>
metadata <Object>
annotations <map[string]string>
labels <map[string]string>
name <string>
namespace <string>
<Snip>

您甚至可以深入到特定的Pod属性，并查看其支持的值。以下示例深入到Pod _restartPolicy_ 属性。

$ kubectl explain pod.spec.restartPolicy
KIND: Pod
VERSION: v1
FIELD: restartPolicy <string>
DESCRIPTION:
Pod中所有容器的重启策略。始终、失败时或从不之一。默认为始终。
更多信息：https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/...
可能的枚举值：

- `"始终"`
- `"从不"`
- `"失败时"`

尽管Pods添加了这么多内容，但它们是轻量级的，并且几乎没有额外的开销。

**Pods实现资源共享**
Pods运行一个或多个容器，而同一Pod中的所有容器共享Pod的执行环境。这包括：

- 共享的文件系统和卷（mnt命名空间）
- 共享的网络堆栈（net命名空间）

```
4: 使用Pod 39
```

- 共享的内存（IPC命名空间）
- 共享的进程树（pid命名空间）
- 共享的主机名（uts命名空间）

```
图4.2显示了一个多容器Pod，其中两个容器共享Pod的卷和网络资源。
```

```
图4.2-多容器Pod共享IP和卷
```

其他应用程序和客户端可以通过Pod的10.0.10.15 IP地址访问容器-主应用程序容器可以在端口8080上访问，而边车容器可以在端口5005上访问。如果它们需要在Pod内部相互通信，它们可以使用Pod的本地适配器。两个容器还会挂载Pod的卷，并可以使用它来共享数据。例如，边车容器可以从远程Git仓库同步静态内容，并将其存储在卷中，主应用程序容器可以读取并将其作为Web页面提供。

```
Pod和调度
```

```
Kubernetes保证将同一Pod中的所有容器调度到同一群集节点。尽管如此，只有当容器需要共享内存、卷和网络等资源时，才应将它们放在同一Pod中。如果您的唯一要求是将两个工作负载调度到同一节点上，应将它们放在各自的Pod中，并使用以下选项之一将它们一起调度。
```

```
术语：在进一步进行之前，请记住，节点是可以是物理服务器、虚拟机或云实例的主机服务器。Pod将容器封装并在节点上执行。
```

4: 使用Pod 40

Pod提供了许多高级调度功能，包括以下所有内容：

- nodeSelectors
- 亲和性和反亲和性
- 拓扑分布约束
- 资源请求和资源限制

_nodeSelectors_是在特定节点上运行Pod的最简单方法。您给nodeSelector提供一个标签列表，调度器将仅将Pod分配给具有所有标签的节点。

_亲和性_和_反亲和性_规则类似于更强大的nodeSelector。

从名称上可以看出，它们支持亲和性和反亲和性规则。但它们还支持硬规则和软规则，并且可以选择节点和Pod：

- 亲和性规则_吸引_
- 反亲和性规则_排斥_
- 硬规则必须_遵守_
- 软规则只是_建议_

在**节点**上选择是常见的，并且与nodeSelector类似，您提供一个标签列表，调度器将Pod分配给具有这些标签的节点。

要选择**Pod**，调度器采用类似的标签列表，并将Pod调度到运行其他具有这些标签的**Pods**节点。

考虑一些例子。

指定**project=qsk**标签的_hard node affinity rule_告诉调度器只能在具有**project=qsk**标签的节点上运行Pod。如果找不到具有该标签的节点，它将不会调度Pod。如果是软规则，调度器将_尝试_查找具有该标签的节点，但如果找不到，则仍将调度Pod。如果是反亲和性规则，调度器将寻找**不**具有该标签的节点。对于基于Pod的规则，逻辑相同。

拓扑分布约束是一种灵活的方式，可以智能地将Pods分布在基础设施中，以提高可用性、性能、本地性或其他要求。一个典型的例子是将Pods分布在云端或数据中心的底层可用区，以实现高可用性（HA）。但是，您也可以为几乎任何内容创建自定义域，例如将Pods调度到靠近数据源的位置，靠近客户端以改善网络延迟等等。

资源请求和资源限制非常重要，每个Pod都应该使用它们。它们告诉调度器Pod需要多少CPU和内存，调度器使用它们来选择具有足够资源的节点。如果您不指定它们，调度器无法知道Pod需要什么资源，并可能将其调度到资源不足的节点。

4: 使用Pod 41

**部署Pods**

部署Pod包括以下步骤：
**限制**

请根据英文内容直接翻译，维持原有的格式，不省略任何信息。

**Pod的生命周期**

Pod被设计成具有有限生命周期和不可变性。

有限生命周期意味着你创建一个Pod，它执行一个任务，然后它终止。一旦任务完成，Pod会被删除并且无法重新启动。如果任务失败，也是一样的——它会被删除并且无法重新启动。

不可变性意味着一旦Pod部署，你无法修改它们。如果你来自传统背景，在常规的服务器维护过程中经常修补和更改配置，这可能是一个巨大的思维转变。如果你需要修改一个Pod，你需要创建一个具有修改的新Pod，删除旧的Pod并用新的Pod替换它。如果一个Pod需要存储数据，你应该附加一个卷，并将数据存储在卷中，这样在删除Pod时数据不会丢失。

让我们来看一个典型的Pod生命周期。

你在一个声明性的YAML对象中定义一个Pod，然后将其发布到API服务器。在调度程序找到一个节点来运行它之前，它进入“待定”阶段。假设它找到了一个节点，Pod被调度，本地kubelet指示运行时启动其容器。一旦所有容器都运行起来，Pod进入“运行”阶段。如果Pod是一个长期运行的Pod，比如一个Web服务器，它将无限期地保持在运行阶段。

如果Pod是一个短期的Pod，比如一个批处理作业，只要所有容器完成了它们的任务，它就进入“成功”状态。如图4.3所示。

```
图4.3 - Pod生命周期
```

关于在Kubernetes上运行虚拟机的一点说明。虚拟机被设计为可变的不朽对象。例如，你可以重新启动它们，更改它们的配置，甚至迁移它们。这与Pod的设计目标非常不同，这就是为什么KubeVirt将虚拟机包装在一个名为VirtualMachineInstance（VMI）的修改后的Pod中，并使用自定义工作负载控制器来管理它们。

**重启策略**

在本章的前面，我们说过Pod可以通过重启策略来增强应用程序。然而，这些策略适用于各个容器，而不是实际的Pod。

让我们考虑一些场景。

你使用一个部署控制器将一个Pod调度到一个节点上，然后该节点失败了。当发生这种情况时，部署控制器会注意到节点故障，删除Pod，并在一个存活的节点上用一个新Pod替换它。即使新的Pod基于相同的Pod配置，它具有新的UID、新的IP地址和无状态。当节点在节点维护过程中或由于资源调整而驱逐Pod时，情况也是一样的——被驱逐的Pod被删除，并在另一个节点上用一个新的Pod替换它。

在扩展操作、更新和回滚过程中也是一样的。例如，缩小规模会删除Pod，而扩大规模则会增加新的Pod。

重要的是要记住，每当我们说我们要“更新”或“重启”Pod时，我们实际上是用新的Pod替换它们。

尽管Kubernetes无法重启Pod，但它确实可以重启容器。这始终由本地kubelet完成，并由**spec.restartPolicy**的值控制，该值可以是以下任何一个：

- Always（总是）
- Never（从不）
- OnFailure（失败时）

这些值都是不言自明的：**Always**总是尝试重启容器，**Never**永远不会尝试重启，**OnFailure**仅在容器失败时尝试重启，而不是在容器成功完成时。该策略适用于整个Pod，意味着它适用于Pod中的所有容器，除了“init containers”。稍后我们会详细介绍“init containers”。
你选择的重启策略取决于应用的性质 —— 它是一个长期运行的容器还是一个短期运行的容器。

长期运行的容器托管运行时间无限的应用程序，例如 Web 服务器、数据存储和消息队列。如果它们出现故障，通常希望重新启动它们，因此通常会给予它们“始终”重启策略。

短期运行的容器不同，通常运行批处理类型的工作负载，一直运行到任务完成。大多数情况下，当它们完成时，你会很满意，并且只有在它们出现故障时才想要重新启动它们。因此，你可能会给予它们“出现故障时”重启策略。如果你不关心它们是否失败，可以给予它们“从不”策略。

总之，Kubernetes 永远不会重启 Pod —— 当它们失败、缩放上下、更新时，Kubernetes 总是删除旧的 Pod 并创建新的 Pod。然而，Kubernetes 可以重新启动同一节点上的单个容器。

静态 Pod 与控制器

有两种部署 Pod 的方式：

1. 直接通过 Pod 清单部署（很少见）
2. 间接通过工作负载资源和控制器部署（最常见）

通过 Pod 清单直接部署会创建一个无法自愈、扩展或进行滚动更新的静态 Pod。这是因为它们仅由所在节点上的 kubelet 管理，而 kubelet 仅能重新启动同一节点上的容器。此外，如果节点失败，kubelet 也会失败，无法对 Pod 提供任何帮助。

相反，通过工作负载资源部署的 Pod 获得了由高可用性控制器管理的所有好处，该控制器可以在其他节点上重新启动它们、根据需求进行扩展，并执行高级操作，如滚动更新和版本回滚。本地 kubelet 仍然可以尝试重新启动失败的容器，但如果节点失败或被驱逐，控制器可以在不同节点上重新启动它。关于工作负载资源和控制器的更多信息请参见第6章。

请记住，当我们说“重启 Pod”时，我们指的是用新的 Pod 替换它。

Pod 网络

每个 Kubernetes 集群都运行一个 Pod 网络，并自动连接所有的 Pod 到它。它通常是一个扩展到每个集群节点的平坦的二层覆盖网络，允许每个 Pod 与其他集群节点上的任何其他 Pod 直接通信。

Pod 网络是由第三方插件实现的，该插件通过容器网络接口（CNI）与 Kubernetes 进行交互并配置网络。

你在集群构建时选择一个网络插件，并为整个集群配置 Pod 网络。有很多插件可供选择，每个插件都有其优缺点。然而，在撰写本文时，Cilium 是最受欢迎的插件，并实现了许多高级功能，如安全性和可观察性。

图4.4显示了三个节点运行五个 Pod。所有五个 Pod 都连接到 Pod 网络并可以相互通信。你还可以看到 Pod 网络跨越了所有三个节点。然而，该网络仅用于 Pod，而不是节点。如图所示，你可以将节点连接到多个不同的网络，但 Pod 网络跨越了所有这些网络。

许多集群创建了一个非常开放的 Pod 网络，几乎没有安全性。这使得集群易于使用，避免了与网络安全常见的困扰。然而，你应该使用 Kubernetes 网络策略和其他措施来保护它。

多容器 Pod

多容器 Pod 是一种强大的模式，在实际应用中非常流行。根据微服务设计模式，每个容器应具有单一明确定义的职责。例如，将内容从仓库同步并作为 Web 页面提供的应用程序具有两个不同的职责：

1. 同步内容
2. 提供 Web 页面
你应该使用两个微服务来设计这个应用程序，并为每个微服务提供一个独立的容器 - 一个负责“同步”内容，另一个负责“提供”内容。我们称之为“关注点分离”或“单一职责原则”，它使容器小而简单，鼓励重用，并且使故障排除更容易。大部分情况下，你会将应用程序容器放在它们自己的Pod中，并通过网络进行通信。然而，有时将它们放在同一个Pod中是有益的。以“同步和提供”的示例为例，将容器放在同一个Pod中将允许“同步”容器从远程系统拉取内容并将其存储在共享卷中，而“网页”容器可以读取并提供它。图4.5显示了该架构。

```
图4.5-多容器Pod
```

```
Kubernetes有两种用于多容器Pod的主要模式：初始化容器和旁车容器。让我们快速解释一下每个模式。
```

**多容器Pod：初始化容器**

```
初始化容器是在Kubernetes API中定义的一种特殊类型的容器。你在同一个Pod中运行它们，但Kubernetes保证它们会在主应用程序容器启动之前启动和完成。它也保证它们只运行一次。

初始化容器的目的是准备和初始化环境，使其为应用程序容器准备好。

考虑几个快速示例。

你有一个应用程序，只有在远程API接受连接时才能启动。而不是在主应用程序中添加检查远程API的逻辑，你可以在同一个Pod中的初始化容器中运行该逻辑。当你部署Pod时，初始化容器首先启动，并发送请求到远程API等待其响应。在此过程中，主应用程序容器无法启动。然而，一旦远程API接受请求，初始化容器完成，主应用程序容器将启动。

假设你有另一个应用程序，在启动之前需要一次性克隆远程存储库。同样，而不是在主应用程序中添加克隆和准备内容的代码（远程服务器地址的知识，证书，身份验证，文件同步协议，校验和验证等），你可以在一个初始化容器中实现，它保证在主应用程序容器启动之前完成任务。

初始化容器的一个缺点是它们只能在主应用程序容器启动之前运行任务。对于与主应用程序容器并行运行的内容，你需要一个“旁车容器”。

**多容器Pod：旁车容器**

“旁车容器”是与应用程序容器同时运行的常规容器，其生命周期与Pod相同。

与初始化容器不同，“旁车容器”不是Kubernetes API中的一种资源 - 我们目前使用常规容器来破解旁车模式。正在进行工作以将旁车模式正式化为API的一部分，但在撰写本文时，它仍然是一个早期alpha功能。

旁车容器的作用是在不必在实际应用程序中实现功能的情况下为应用程序添加功能。常见示例包括用于抓取日志、同步远程内容、代理连接和处理数据的旁车容器。它们在服务网格中也被广泛使用，其中旁车容器拦截网络流量并提供流量加密和遥测。

图4.6显示了一个带有主应用程序容器和服务网格旁车容器的多容器Pod。旁车容器拦截所有网络流量并提供加密和解密。它还将遥测数据发送到服务网格控制平面。

```
图4.6-服务网格旁车
```

**Pod理论总结**

Pod是Kubernetes上调度的原子单位，抽象了其中的工作负载细节。它们还支持高级调度和许多其他功能。

许多Pod运行单个容器，但多容器Pod更强大。你可以使用多容器Pod来紧密耦合需要共享资源（如内存和卷）的工作负载。你还可以使用多容器Pod来增强应用程序（旁车模式）和初始化环境（初始化模式）。

你可以在声明性YAML对象中定义Pods，但通常会通过更高级别的工作负载控制器部署它们，这些控制器会增加超能力，如自愈、自动扩展等。

是时候看一些示例了。

### 与Pod一起动手实践。

如果你正在跟随操作，请克隆本书的GitHub仓库，并从**pods**文件夹运行所有命令。
$ git clone https://github.com/nigelpoulton/TheK8sBook.git
正在克隆到 'TheK8sBook'...

$ cd TheK8sBook/pods

**Pod清单文件**

让我们先看看我们的第一个Pod清单文件。这是来自pods文件夹的pod.yml文件。

4：使用Pod 48

kind: Pod
apiVersion: v1
metadata:
name: hello-pod
labels:
zone: prod
version: v1
spec:
containers:

- name: hello-ctr
  image: nigelpoulton/k8sbook:1.0
  ports:
    - containerPort: 8080
      resources:
      limits:
      memory: 128Mi
      cpu: 0.5

这是一个简单的示例，但你可以立即看到四个顶级字段：

- kind
- apiVersion
- metadata
- spec

**kind**字段告诉Kubernetes你所定义的对象的类型。这个示例定义了一个Pod，但如果你正在定义一个Deployment，**kind**字段将会是**Deployment**。

**apiVersion**告诉Kubernetes在创建对象时使用的API版本。

到目前为止，这个清单描述了一个Pod，并告诉Kubernetes使用API的**v1**版本来构建它。

**metadata**部分为Pod命名为**hello-pod**，并给它两个标签。在未来的章节中，您将使用这些标签将Pod连接到一个用于网络连接的服务。

大部分操作发生在**spec**部分。这个示例定义了一个带有名为**hello-ctr**的应用容器的单容器Pod。该容器基于**nigelpoulton/k8sbook:1.0**镜像，监听端口8080，并告诉调度器它需要最大256MB的内存和半个CPU。

您只需要在**spec.containers**部分下面添加更多的容器，就可以将其变成一个多容器Pod。

**清单文件：代码中的同理心**

快速的侧步。

4：使用Pod 49

Kubernetes的YAML文件是优秀的文档来源，您可以使用它们快速了解新团队成员的基本功能和要求，以及帮助弥合开发人员和运维人员之间的差距。

例如，新团队成员可以阅读您的YAML文件，快速了解应用程序的基本功能和要求。运维团队也可以使用它们来了解应用程序的要求，如网络端口、CPU和内存要求等等。

Nirmal Mehta在他2017年的DockerCon演讲中将这些附加好处描述为一种名为“代码中的同理心”的形式。

**从清单文件部署Pod**

运行以下**kubectl apply**命令来部署Pod。该命令将**pod.yml**文件发送到当前_kubeconfig_文件定义的API服务器，并附加来自kubeconfig文件的凭据。

$ kubectl apply -f pod.yml
pod/hello-pod已创建

尽管输出显示Pod已创建，但它可能仍在拉取镜像和启动容器。

运行**kubectl get pods**来检查状态。

$ kubectl get pods
名称 已就绪 状态 重启时长
hello-pod 0/1 正在创建容器 0 9秒

该示例中的Pod尚未完全创建完成-**READY**列显示为零个容器就绪，**STATUS**列显示原因。

这是一个好时机来提到，Kubernetes会自动从Docker Hub拉取（下载）镜像。要使用其他注册表，只需在YAML文件中的图像名称之前添加注册表的URL。

一旦**READY**列显示为**1/1**并且**STATUS**列显示为**Running**，您的Pod将在一个健康的集群和节点上运行，并由节点的kubelet进行监视。

您将在未来的章节中了解如何连接到应用程序并进行测试。

**审查Pod**

让我们看一下您将使用**kubectl**监视和检查Pod的主要方式。

4：使用Pod 50

**kubectl get**

您已经运行过**kubectl get pods**命令，并且看到它返回了一行基本信息。然而，以下标志可以获得更多的信息：

- **-o wide**提供更多的列，但仍然是单行输出
- **-o yaml**获取Kubernetes对对象的所有了解
下面的示例展示了使用`kubectl get pods`命令和`-o yaml`选项的输出结果。由于篇幅限制，输出结果被截断，但请注意它被分为两个主要部分：

- spec（规范）
- status（状态）

`spec`部分显示对象的“期望状态”，而`status`部分显示对象的“观察状态”。

```
$ kubectl get pods hello-pod -o yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      <Snip>
  name: hello-pod
  namespace: default
spec: <<==== 期望状态在此块中
  containers:
  - image: nigelpoulton/k8sbook:1.0
    imagePullPolicy: IfNotPresent
    name: hello-ctr
    ports:
      <Snip>
status: <<==== 观察状态在此块中
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-01-03T18:21:51Z"
    status: "True"
    type: Initialized
    <Snip>
```

完整的输出结果比您用于创建Pod的17行YAML文件要多得多。那么Kubernetes从哪里获取所有这些额外的详细信息呢？

主要有两个来源：

- Pod具有许多属性，如果在YAML文件中没有明确定义，这些属性将使用默认值填充
- `status`部分显示Pod的当前状态

另一个很好的命令是`kubectl describe`。该命令会以格式良好的方式提供对象的概述，包括生命周期事件。

```
$ kubectl describe pod hello-pod
Name: hello-pod
Namespace: default
Labels: version=v1
        zone=prod
Status: Running
IP: 10.1.0.103
Containers:
  hello-ctr:
    Container ID: containerd://ec0c3e...
    Image: nigelpoulton/k8sbook:1.0
    Port: 8080/TCP
    <Snip>
Conditions:
  Type    Status
  ----    ------
  Initialized     True
  Ready   True
  ContainersReady     True
    <Snip>
Events:
  Type    Reason     Age   Message
  ----    ------     ----  -------
  Normal  Scheduled  5m30s Successfully assigned ...
  Normal  Pulling    5m30s Pulling image "nigelpoulton/k8sbook:1.0"
  Normal  Pulled     5m8s  Successfully pulled image ...
  Normal  Created    5m8s  Created container hello-ctr
  Normal  Started    5m8s  Started container hello-ctr
```

您可以使用`kubectl logs`命令从Pod中的任何容器获取日志。命令的基本格式是`kubectl logs <pod>`。

如果您对多容器的Pod运行该命令，将自动获取来自Pod中的第一个容器的日志。但是，您可以使用`--container`标志来覆盖此行为，并指定您想要获取日志的容器的名称。如果您不确定多容器Pod中容器的名称或它们的顺序，只需运行`kubectl describe pod <pod>`命令即可。您还可以从Pod的YAML文件中获取相同的信息。

下面的YAML示例显示了一个具有两个容器的多容器Pod。第一个容器名称为`app`，第二个容器名称为`syncer`。如果不指定`--container`标志对该Pod运行`kubectl logs`命令，将获取来自`app`容器的日志。

```
kind: Pod
apiVersion: v1
metadata:
  name: logtest
spec:
  containers:
  - name: app <<==== 第一个容器（默认）
    image: nginx
    ports:
    - containerPort: 8080
  - name: syncer <<==== 第二个容器
    image: k8s.gcr.io/git-sync:v3.1.6
    volumeMounts:
    - name: html
      <Snip>
```

如果您想要获取`syncer`容器的日志，可以运行以下命令。请不要运行此命令，因为您尚未部署此Pod。

```
$ kubectl logs logtest --container syncer
```

`kubectl exec`命令是在正在运行的容器内执行命令的好方法。

您可以使用`kubectl exec`有两种方式：

```
1. 远程命令执行
2. 执行会话
```

远程命令执行允许您从本地shell发送命令到容器。容器执行命令并将输出返回到您的shell中。
一个_exec session_将您的本地shell连接到容器的shell，与登录到容器相同。

让我们先看看远程命令执行。

从您的本地shell运行以下命令。它要求**hello-pod** Pod中的第一个容器运行**ps**命令。

4: 使用Pods 53

$ kubectl exec hello-pod -- ps
PID USER TIME COMMAND
1 root 0:00 node ./app.js
17 root 0:00 ps aux

容器执行了**ps**命令，并在您的本地终端显示了结果。

命令的格式是**kubectl exec <pod> -- <command>**，您可以在容器中执行任何安装的命令。默认情况下，命令在Pod中的第一个容器中执行，但是您可以使用**--container**标志覆盖此设置。

尝试运行以下命令。

$ kubectl exec hello-pod -- curl localhost:8080
OCI runtime exec failed:...... "curl": executable file not found in $PATH

此命令失败，因为容器中没有安装**curl**命令。

让我们使用**kubectl exec**获取与相同容器的交互式exec会话。通过连接您的终端和容器的终端，它感觉就像您登录到了容器。

运行以下命令，为**hello-pod** Pod中的第一个容器创建一个exec会话。您的shell提示符将更改，表示您已连接到容器的shell。

$ kubectl exec -it hello-pod -- sh

#

**-it**标志告诉**kubectl exec**通过将您的shell的STDIN和STDOUT流连接到Pod中第一个容器的STDIN和STDOUT来使会话交互。**sh**命令在会话中启动一个新的shell进程，您的提示符将更改，表示您现在在容器内部。

从exec会话中运行以下命令，安装**curl**二进制文件，然后执行**curl**命令。

# apk add curl

<Snip>

# curl localhost:8080

<html><head><title>K8s rocks!</title><link rel="stylesheet" href="http://netdna....

对于正在运行的Pod来说，像这样对其进行更改是一个反模式，因为Pod被设计为不可变对象。然而，对于演示目的来说是可以的。

4: 使用Pods 54

**Pod主机名**

Pod的名称来自其YAML文件的**metadata.name**字段，Kubernetes将其用作Pod中每个容器的主机名。

如果您正在跟随操作，您将部署一个名为**hello-pod**的单个Pod。您从以下YAML文件部署它，并将Pod名称设置为**hello-pod**。

kind: Pod
apiVersion: v1
metadata:
name: hello-pod <<==== Pod主机名。被所有容器继承。
labels:
<Snip>

从现有的exec会话中运行以下命令，以检查容器的主机名。该命令区分大小写。

$ env | grep HOSTNAME
HOSTNAME=hello-pod

如您所见，容器的主机名与Pod的名称匹配。如果是多容器Pod，所有容器将具有相同的主机名。

因此，您应确保Pod名称是有效的DNS名称（a-z，0-9，减号和点号）。

输入**exit**退出exec会话，返回到本地终端。

**检查Pod的不可变性**

Pod被设计为不可变对象，这意味着您不应在部署后对其进行更改。

不可变性应用于两个级别：

- 对象不可变性（Pod）
- 应用不可变性（容器）

Kubernetes通过阻止对正在运行的Pod的配置进行更改来处理_对象不可变性_。然而，Kubernetes不能总是阻止您更改容器中的应用程序和文件系统。您负责确保容器及其应用程序是无状态且不可变的。

以下示例使用**kubectl edit**编辑一个正在运行的Pod对象。尝试更改以下任何属性：

```
4: 使用Pods 55
```

- Pod名称
- 容器名称
- 容器端口
- 资源请求和限制

您需要从本地终端运行此命令，它将在您的默认编辑器中打开文件。对于Mac和Linux用户，它通常会在**vi**中打开文件，而对于Windows用户，则通常是**notepad.exe**。

```
$ kubectl edit pod hello-pod
```
```
＃请编辑下面的对象。以“＃”开头的行将被忽略...
apiVersion：v1
kind：Pod
metadata：
<Snip>
labels：
version：v1
zone：prod
name：hello-pod <<====尝试更改此处
namespace：default
resourceVersion：“432621”
uid：a131fb37-ceb4-4484-9e23-26c0b9e7b4f4
spec：
containers：
```

- image：nigelpoulton / k8sbook：1.0
  imagePullPolicy：IfNotPresent
  name：hello-ctr <<====尝试更改此处
  ports：
    - containerPort：8080 <<====尝试更改此处
      protocol：TCP
      resources：
      limits：
      cpu：500m <<====尝试更改此处
      memory：256Mi <<====尝试更改此处
      requests：
      cpu：500m <<====尝试更改此处
      memory：256Mi <<====尝试更改此处

```
编辑文件，保存更改，然后关闭编辑器。您将收到一条消息，告诉您更改被禁止，因为属性是不可变的。
如果您在kubectl编辑会话中被卡住，可以通过键入以下组合键退出：：q，然后按回车键。
```

```
4：处理Pod 56
```

**资源请求和资源限制**

```
Kubernetes允许您为Pod中的每个容器指定资源请求和资源限制。
```

- _请求_是最小值
- _限制_是最大值

```
考虑以下来自Pod YAML的片段：
```

```
resources：
requests：<<====调度的最低要求
cpu：0.5
memory：256Mi
limits：<<====kubelet的最大限制
cpu：1.0
memory：512Mi
```

```
此容器需要至少256Mi内存和半个CPU。调度程序读取此信息并将其分配给具有足够资源的节点。如果无法找到合适的节点，则将Pod标记为待处理状态，然后集群自动缩放器将尝试提供新的集群节点。
假设调度程序找到了一个合适的节点，它会将Pod分配给该节点，然后kubelet会下载Pod规范并要求本地运行时启动它。作为该过程的一部分，kubelet会保留所请求的CPU和内存，确保资源在需要时可用。它还基于每个容器的资源限制设置资源使用上限。在此示例中，它将CPU限制设置为一个，内存限制设置为512Mi。大多数运行时还会执行资源限制，但每个运行时如何实现这一点可能会有所不同。
```

容器在执行时，其最低要求（_请求_）是有保证的。但是，如果节点有额外可用资源，它可以使用更多，但是不能超过您在其_限制_中指定的值。
对于多容器的Pod，调度程序将所有容器的请求组合起来，并找到具有足够资源以满足整个Pod的节点。
如果您一直密切关注示例，您可能已经注意到您用于部署** hello-pod**的**pod.yml**文件仅指定了资源限制 - 它没有指定资源请求。但是，一些命令输出显示了限制 **和** 请求。这是因为如果您只指定了限制，Kubernetes会自动将请求设置为与限制相匹配。

**多容器Pod示例 - 初始化容器**

```
以下YAML定义了一个带有初始化容器和主应用容器的多容器Pod。它来自于本书GitHub存储库的pods文件夹中的initpod.yml文件。
```

4：处理Pod 57

apiVersion：v1
kind：Pod
metadata：
name：initpod
labels：
app：initializer
spec：
initContainers：

- name：init-ctr
  image：busybox：1.28.4
  command：['sh'，'-c'，'until nslookup k8sbook; do echo waiting for k8sbook service;\
  sleep 1; done; echo Service found!']
  containers：
    - name：web-ctr
      image：nigelpoulton/web-app：1.0
      ports：
      - containerPort：8080

在**spec.initContainers**块下定义一个容器会将其变为初始化容器，Kubernetes保证它会在常规容器之前运行并完成。

常规应用容器在**spec.containers**块下定义，并且只有在**所有**初始化容器成功完成后才会启动。
这个例子有一个称为**init-ctr**的单个初始化容器和一个称为**web-ctr**的单个应用容器。初始化容器运行一个循环，寻找一个名为**k8sBook**的Kubernetes服务。一旦你创建了这个服务，初始化容器将收到响应并退出。这样主容器就可以开始运行了。在后续的章节中，您将了解有关"服务"的内容。

使用以下命令部署多容器Pod，然后使用**--watch**标志运行**kubectl get pods**命令，查看它是否启动。

$ kubectl apply -f initpod.yml
pod/initpod created

$ kubectl get pods --watch
NAME READY STATUS RESTARTS AGE
initpod 0/1 Init:0/1 0 6s

**Init:0/1**状态告诉您初始化容器仍在运行，这意味着主容器尚未启动。如果运行**kubectl describe**命令，您将看到整个Pod的状态是**Pending**。

4: 使用Pod 58

$ kubectl describe pod initpod
Name: initpod
Namespace: default
Priority: 0
Service Account: default
Node: docker-desktop/192.168.65.3
Labels: app=initializer
Annotations: <none>
Status: Pending <<==== Pod状态
<Snip>

Pod将保持在此阶段，直到您创建一个名为**k8sbook**的服务。

运行以下命令创建服务并重新检查Pod状态。

$ kubectl apply -f initsvc.yml
service/k8sbook created

$ kubectl get pods --watch
NAME READY STATUS RESTARTS AGE
initpod 0/1 Init:0/1 0 15s
initpod 0/1 PodInitializing 0 3m39s
initpod 1/1 Running 0 3m57s

初始化容器在服务出现后立即完成，并启动主应用容器。请给它几秒钟来完全启动。

如果再次对**initpod** Pod运行**kubectl describe**，您将看到初始化容器处于"终止"状态，因为它成功完成了（退出代码为0）。

**多容器Pod示例-边车容器**

```
注意：在写作时，Kubernetes尚未提供用于边车容器的API支持。然而，Kubernetes 1.28引入了一种潜在的解决方案的alpha支持。如果这一功能成熟并受到关注，我将在本书中进行更新。
```

边车容器与主应用容器同时运行整个Pod的生命周期。我们目前将它们定义为Pod YAML的**spec.containers**部分中的常规容器，它们的工作是增强主应用容器或提供辅助支持服务。

以下的YAML文件定义了一个多容器Pod，其中两个容器都挂载了相同的共享卷。将主应用容器放在第一个容器，然后是边车容器是常见的做法。

```
4: 使用Pod 59
```

```
apiVersion: v1
kind: Pod
metadata:
name: git-sync
labels:
app: sidecar
spec:
containers:
```

- name: ctr-web <<==== 第一个容器（主应用）
  image: nginx
  volumeMounts:
    - name: html <<==== 挂载共享卷
      mountPath: /usr/share/nginx/
- name: ctr-sync <<==== 第二个容器（边车）
  image: k8s.gcr.io/git-sync:v3.1.6
  volumeMounts:
    - name: html <<==== 挂载共享卷
      mountPath: /tmp/git
      env:
    - name: GIT_SYNC_REPO
      value: https://github.com/nigelpoulton/ps-sidecar.git
    - name: GIT_SYNC_BRANCH
      value: master
    - name: GIT_SYNC_DEPTH
      value: "1"
    - name: GIT_SYNC_DEST
      value: "html"
      volumes:
- name: html <<==== 共享卷
  emptyDir: {}

```
主应用容器的名称为ctr-web。它基于一个NGINX镜像，并从共享的html卷加载一个静态网页。
第二个容器的名称为ctr-sync，它是边车容器。它监视一个GitHub仓库，并将更改同步到相同的共享html卷中。
```

当GitHub仓库的内容发生变化时，边车容器将更新复制到共享卷中，应用容器会注意到并提供更新后的网页。

我们将按照以下步骤进行演示：
```
1. 分叉GitHub存储库
2. 使用您分叉的存储库的URL更新YAML文件
3. 部署应用程序
4. 连接到应用程序并查看显示“这是版本1.0”
5. 对GitHub存储库的分叉进行更改

4：使用Pod 60

```
6. 确认您的更改出现在网页上
```

转到GitHub并“分叉”以下存储库。您需要一个GitHub账户来执行此操作。

https://github.com/nigelpoulton/ps-sidecar

返回本地计算机并编辑**sidecarpod.yml**文件。更改**GIT_SYNC_REPO**的值以匹配您分叉的存储库的URL，并保存更改。

运行以下命令来部署应用程序。它将部署Pod以及您将用于连接到应用程序的Service。

$ kubectl apply -f sidecarpod.yml
pod/git-sync created
service/svc-sidecar created

使用**kubectl get pods**命令检查Pod的状态。

一旦Pod进入“running”状态，运行**kubectl get svc**命令并复制**EXTERNAL-IP**列中的值。如果您正在运行Docker Desktop集群或其他本地选项，则可能显示为“localhost”。

将该值粘贴到新的浏览器选项卡中，以查看网页。它将显示**This is version 1.0**。

确保对您的分叉存储库执行以下步骤。

转到您的分叉存储库并编辑**index.html**文件。将**<h1>**行更改为其他内容，并保存更改。

刷新应用程序的网页以查看您的更新。

恭喜您。附属容器成功地监视了远程Git存储库，将更改同步到共享卷，并且主应用程序容器更新了网页。

随意运行**kubectl get pods**和**kubectl describe pod**命令，以查看多容器Pod在输出中的显示方式。

### 清理

如果您一直在跟进，您的集群上将有以下对象。

```
Pods Services
hello-pod
initpod k8sbook
git-sync svc-sidecar
```

使用以下命令删除它们。

$ kubectl delete pod hello-pod initpod git-sync
pod "hello-pod" deleted
pod "initpod" deleted
pod "git-sync" deleted

$ kubectl delete svc k8sbook svc-sidecar
service "k8sbook" deleted
service "svc-sidecar" deleted

您还可以使用它们的YAML文件删除对象。

$ kubectl delete -f sidecarpod.yml -f initpod.yml -f pod.yml -f initsvc.yml
pod "git-sync" deleted
service "svc-sidecar" deleted
pod "initpod" deleted
pod "hello-pod" deleted
service "k8sbook" deleted

您可能还想删除您对GitHub存储库的分叉。

### 章节总结

在本章中，您了解到Kubernetes将所有应用程序部署在Pod中。这些应用程序可以是容器、无服务器函数、Wasm应用程序和虚拟机。但是，它们通常是容器，因此我们通常根据执行的容器来称呼Pod。

除了抽象不同类型的应用程序外，Pod还提供共享的执行环境、高级调度、应用程序健康探测等等。

Pod可以是单容器或多容器的，多容器Pod中的所有容器共享Pod的网络、卷和内存。

您通常会通过更高级别的工作负载控制器（例如Deployments、Jobs和DaemonSets）部署Pods。第三方工具（如Knative和KubeVirt）使用自定义资源和自定义工作负载控制器扩展Kubernetes API，从而允许Kubernetes运行无服务器和虚拟机工作负载。

您可以在声明性的YAML文件中定义Pods，然后将它们发布到API服务器，控制平面将它们调度到集群中。大多数时候，您将使用**kubectl apply**将YAML清单发布到API服务器，调度器将部署它们。

＃5：使用命名空间创建虚拟集群

命名空间是将Kubernetes集群划分为多个“虚拟集群”的一种方式。

本章为命名空间奠定了基础，让您熟悉创建和管理命名空间，并介绍了一些用例。您将在后续章节中看到它们的实际应用。

本章分为以下部分：

- 命名空间简介
- 命名空间用例
- 默认命名空间
- 创建和管理命名空间
- 部署到命名空间

### 命名空间简介。
```
首先要知道的是，Kubernetes的命名空间（Namespaces）与内核命名空间（kernel namespaces）不同。

- 内核命名空间将操作系统划分为称为容器（containers）的虚拟操作系统
- Kubernetes的命名空间将Kubernetes集群划分为称为命名空间（Namespaces）的虚拟集群

```
注：在指称Kubernetes的命名空间时，我们将会大写命名空间（Namespace）。
这是为了遵循Kubernetes API对象大写的模式，并澄清我们指的是Kubernetes的命名空间，而不是内核命名空间。
```

另外，需要知道命名空间是一种软隔离（soft isolation）的形式，可以实现软多租户（soft multi-tenancy）。例如，您可以为您的开发（dev）、测试（test）和质量保证（qa）环境创建命名空间，并对每个环境应用不同的配额和策略。然而，它们无法阻止一个命名空间中被攻击的工作负载对其他命名空间中的工作负载造成影响。

以下命令显示对象是否有命名空间。正如您所看到的，大多数对象有命名空间，这意味着您可以将它们部署到特定的命名空间并应用自定义策略和配额。而不具有命名空间的对象（如节点和持久化卷）是集群范围的（cluster-scoped），无法隔离到命名空间。

5: 使用命名空间的虚拟集群 63

$ kubectl api-resources
名称 缩写 ... 是否有命名空间 类型
节点 nodes 否 Node
持久化卷索赔 persistentvolumeclaims 是 PersistentVolumeClaim
持久化卷 persistentvolumes 否 PersistentVolume
Pod pods 是 Pod
Pod模板 podtemplates 是 PodTemplate
副本控制器 replicationcontrollers 是 ReplicationController
资源配额 resourcequotas 是 ResourceQuota
密钥 secrets 是 Secret
服务账户 serviceaccounts 是 ServiceAccount
服务 services 是 Service
<剪辑>

除非您另行指定，否则Kubernetes会将对象部署到默认命名空间（default Namespace）。

### 命名空间的使用场景。

命名空间是多个租户共享同一集群的一种方式。

“租户”是一个宽泛的术语，可以指个别应用程序、不同团队或部门，甚至是外部客户。如何实施命名空间以及您认为什么是“租户”由您决定，但最常见的做法是使用命名空间将集群划分为同一组织内部的租户使用。例如，您可以将生产集群划分为以下三个命名空间，以匹配您的组织结构：

- finance
- hr
- corporate-ops

您可以将财务应用部署到“finance”命名空间，将人力资源应用部署到“hr”命名空间，将企业应用部署到“corporate-ops”命名空间。每个命名空间都可以有自己的用户、权限、资源配额和策略。

使用命名空间将集群划分给外部租户的情况较少见。这是因为它们只提供了软隔离，无法防止受损的工作负载逃逸到其他命名空间中影响工作负载。在撰写本文时，唯一强隔离租户的方法是在自己的集群和自己的硬件上运行它们。

图5.1显示了左侧使用命名空间实现软多租户的集群。该集群上的所有应用程序共享相同的节点和控制平面，受损的工作负载可以影响两个命名空间。右侧的两个集群通过实施两个独立的集群，在专用硬件上提供了强隔离。

5: 使用命名空间的虚拟集群 64

```
图5.1-软隔离和硬隔离
```

命名空间轻量且易于管理，但只提供软隔离。运行多个集群成本更高且带来了更多的管理开销，但它提供了强隔离。

### 默认命名空间。

每个Kubernetes集群都有一组预先创建的命名空间。

运行以下命令以列出您的命名空间。

$ kubectl get namespaces
名称 状态 年龄
default 已激活 2天
kube-system 已激活 2天
kube-public 已激活 2天
kube-node-lease 已激活 2天

如果在创建对象时未指定命名空间，则新对象将进入“default”命名空间。 “kube-system”命名空间用于运行控制平面组件，如内部DNS服务和度量服务器。 “kube-public”用于需要任何人都能读取的对象。最后， “kube-node-lease”用于节点心跳和管理节点租约。

运行“kubectl describe”命令来检查集群上的一个命名空间。在使用“kubectl”时，可以将“namespace”替换为“ns”。

5: 使用命名空间的虚拟集群 65

$ kubectl describe ns default
名称：default
标签：kubernetes.io/metadata.name=default
注释：无
状态：已激活
无资源配额。
无LimitRange资源。

您还可以在“kubectl”命令中添加“-n”或“--namespace”以针对特定命名空间过滤结果。
运行以下命令以列出**kube-system**命名空间中的所有Service对象。您的输出可能会有所不同。

```
$ kubectl get svc --namespace kube-system
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S)
kube-dns ClusterIP 10.43.0.10 <none> 53/UDP,53/TCP,9153...
metrics-server ClusterIP 10.43.4.203 <none> 443/TCP
traefik-prometheus ClusterIP 10.43.49.213 <none> 9100/TCP
traefik LoadBalancer 10.43.222.75 <pending> 80:31716/TCP,443:31...
```

您还可以使用**--all-namespaces**标志返回所有命名空间中的对象。

### 创建和管理命名空间

在本节中，您将了解如何创建、检查和删除命名空间。

如果您想跟随操作，您需要克隆本书的GitHub仓库。

```
$ git clone https://github.com/nigelpoulton/TheK8sBook.git
<Snip>
```

您还需要从**TheK8sBook/namespaces**目录运行所有命令。

命名空间是**core v1** API组中的一级资源。这意味着它们是稳定、易于理解的，并且已经存在很长时间。这也意味着您可以以命令式和声明式的方式与它们一起工作。我们将两者都做。

运行以下命令以命令式地创建一个名为**hydra**的新命名空间。

```
$ kubectl create ns hydra
namespace/hydra created
```

现在，从**shield-ns.yml** YAML文件中以声明方式创建一个命名空间。这是一个简单的文件，定义了一个名为**shield**的单个命名空间。

```
5: 通过命名空间创建虚拟集群 66
```

```
kind: Namespace
apiVersion: v1
metadata:
name: shield
labels:
env: marvel
```

```
使用以下命令创建它。
```

```
$ kubectl apply -f shield-ns.yml
namespace/shield created
```

```
列出所有命名空间，查看您创建的两个新命名空间。
```

```
$ kubectl get ns
NAME STATUS AGE
<Snip>
hydra Active 49s
shield Active 3s
```

```
如果您对漫威电影宇宙有所了解，您将知道Shield和Hydra是势不两立的敌人，它们不应该只通过命名空间来分隔而共享同一集群。
删除hydra命名空间。
```

```
$ kubectl delete ns hydra
namespace "hydra" deleted
```

**为特定命名空间配置kubectl**

在使用命名空间时，您很快就会意识到在所有**kubectl**命令中添加**-n**或**--namespace**标志是很麻烦的。更好的方法是将您的_kubeconfig_配置为自动针对特定命名空间运行命令。
运行以下命令将您的kubeconfig配置为将来的所有**kubectl**命令针对**shield**命名空间运行。

```
$ kubectl config set-context --current --namespace shield
Context "tkb" modified.
```

```
运行几个简单的kubectl get命令来测试它是否有效。shield命名空间是空的，所以您的命令不会返回任何对象。
```

5: 通过命名空间创建虚拟集群 67

### 将对象部署到命名空间

如前所述，大多数对象都是命名空间的。Kubernetes将新对象部署到**default**命名空间，除非您另外指定。

有两种将对象部署到特定命名空间的方法：

- 命令式地
- 声明式地

要以命令式方式执行此操作，将**-n**或**--namespace**标志添加到命令中。要以声明式方式执行此操作，您需要在对象的YAML清单中指定命名空间。

让我们使用声明式方法将一个应用程序部署到**shield**命名空间。

应用程序在本书的GitHub仓库的**namespaces**文件夹中的**app.yml**文件中定义。它定义了三个对象：ServiceAccount、Service和Pod。以下YAML片段显示了所有三个对象都针对**shield**命名空间。

如果您不理解YAML文件中的所有内容，不要担心，您只需要知道它定义了三个对象，并将每个对象都定位到**shield**命名空间。

apiVersion: v1
kind: ServiceAccount
metadata:
namespace: shield <<==== 命名空间
name: default
---

apiVersion: v1
kind: Service
metadata:
namespace: shield <<==== 命名空间
name: the-bus
spec:
type: LoadBalancer
ports:
- 端口：8080
  目标端口：8080
  选择器：
  env：marvel

---

apiVersion：v1
kind：Pod
metadata：
namespace：shield <<==== 命名空间
name：triskelion
<Snip>

5：带有命名空间的虚拟集群 68

使用以下命令部署它。如果出现关于ServiceAccount缺少注释的警告，请不必担心。

$ kubectl apply -f app.yml
serviceaccount/default已配置
service/the-bus已配置
pod/triskelion已创建

运行几个命令来验证所有三个对象是否在**shield**命名空间中。如果您已经配置**kubectl**自动定位到shield命名空间，则不需要添加**-n shield**标志。

$ kubectl get pods -n shield
NAME READY STATUS RESTARTS AGE
triskelion 1/1 Running 0 48s

$ kubectl get svc -n shield
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
the-bus LoadBalancer 10.43.30.174 localhost 8080:31112/TCP 52s

现在应用程序已部署，使用**curl**或您的浏览器连接到它。只需将浏览器或**curl**命令指向**EXTERNAL-IP**列中的值，端口为8080。如果您的情况与书中的示例相似，您将连接到localhost:8080。

$ curl localhost:8080

<!DOCTYPE html>
<html>
<head>
<title>AOS</title>
<Snip>

恭喜您。您已创建了一个命名空间并将应用程序部署到其中。连接到应用程序与连接到默认命名空间中的应用程序没有任何区别。

### 清理

以下命令将清理您的集群并将kubeconfig恢复为使用**default**命名空间。

删除shield命名空间。这将自动删除Pod、Service和ServiceAccount，并可能需要几秒钟才能完成。

5：带有命名空间的虚拟集群 69

$ kubectl delete ns shield
namespace "shield" deleted

重置kubeconfig，使其使用**default**命名空间。如果不这样做，将来的命令将针对已删除的shield命名空间运行，并返回无结果。

$ kubectl config set-context --current --namespace default
Context "tkb" modified.

### 章节总结

在本章中，您了解了Kubernetes使用命名空间来划分集群以进行资源和会计目的。每个命名空间可以拥有自己的用户、RBAC规则和资源配额，并且可以选择性地应用策略到命名空间。然而，它们不是强大的工作负载隔离边界，因此您不能将它们用于强制多租户。

如果在部署时不指定命名空间，Kubernetes将部署对象到**default**命名空间。

## 6：Kubernetes部署

本章将向您展示如何使用_Deployments_在Kubernetes上为无状态应用程序添加云原生特性，例如自愈、扩展、滚动更新和版本回滚。

本章分为以下几个部分：

- 部署理论
- 创建部署
- 手动扩展应用程序
- 执行滚动更新
- 执行回滚操作

### 部署理论

在Kubernetes上运行无状态应用程序的最流行方式是使用部署（Deployments）。它们可以添加自愈、扩展、滚动更新和回滚功能。

考虑一个快速的例子。

假设您需要一个需要具备弹性、按需扩展并经常更新的Web应用程序。您编写了该应用程序，对其进行了容器化，并在Pod的YAML中定义了它，以便在Kubernetes上运行。然后，您将Pod包装在一个部署中，并将其提交给Kubernetes，部署控制器将Pod部署。此时，您的集群正在运行一个管理单个Pod的单个部署。

如果Pod失败，部署控制器会将其替换为新的Pod。如果需求增加，部署控制器可以部署更多相同的Pod。当您更新应用程序时，部署控制器会删除旧的Pod，并替换为新的Pod。

假设该应用程序还有另一个无状态的微服务，比如购物车。您会将其容器化，将其包装在自己的Pod中，将Pod包装在自己的部署中，并将其部署到集群中。

此时，您将有两个部署来管理两个不同的微服务。

图6.1显示了部署控制器监视和管理这两个部署的设置。**web**部署管理四个相同的Web服务器Pod，**cart**部署管理两个相同的购物车Pod。

6：Kubernetes部署 71

```
Figure6.1-部署
```

在幕后，部署遵循标准的Kubernetes架构，包括：
```
在最高层次上，资源定义了对象，控制器管理这些对象。

Deployment资源存在于apps/v1 API中，定义了所有支持的属性和功能。

Deployment控制器在控制平面上运行，监视Deployments，并将观察到的状态与期望的状态进行调和。

Deployments和Pods

每个Deployment管理一个或多个相同的Pods。

例如，一个由Web服务和购物车服务组成的应用程序将需要两个Deployments - 一个用于管理Web Pods，另一个用于管理购物车Pods。图6.1显示了web Deployment管理四个相同的web Pods和cart Deployment管理两个相同的购物车Pods。

图6.2显示了一个Deployment YAML文件，请求四个副本的单个Pod。如果将副本数增加到六个，它将部署和管理两个额外的相同Pods。

注意Pod规范是如何在嵌入在Deployment YAML中的模板中定义的。

Deployments和ReplicaSets

我们一再强调，Deployments添加了自愈、扩展、滚动和回滚的功能。然而，在幕后，实际上是一种不同的资源，称为ReplicaSet，它提供了自愈和扩展的功能。图6.3显示了容器、Pods、ReplicaSets和Deployments的整体架构，以及它们如何映射到Deployment YAML中。

将此Deployment YAML发布到集群将创建一个Deployment、一个ReplicaSet和两个运行相同容器的相同Pods。Pods由ReplicaSet管理，而ReplicaSet又由Deployment管理。因此，您应该通过Deployment执行所有管理操作，而不是直接管理ReplicaSet或Pods。

关于扩展的一点说明

您可以手动扩展您的应用程序，我们将很快看到如何做到这一点。然而，Kubernetes有几个自动扩展器，可以自动扩展您的应用程序和基础设施。其中一些包括：

- 水平Pod自动缩放器
- 垂直Pod自动缩放器
- 集群自动缩放器

水平Pod自动缩放器（HPA）根据当前需求增加和删除Pods。大多数集群默认安装它，并广泛使用。

集群自动缩放器（CA）根据需要增加和删除集群节点，以确保始终有足够的节点运行所有计划的Pods。这也是默认安装并广泛使用的。

垂直Pod自动缩放器（VPA）根据当前需求增加和减少分配给运行Pods的CPU和内存。它不是默认安装的，有一些已知的限制，并且使用较少。目前的实现在扩展Pods资源时会删除现有Pod并用新Pod替换它。这是具有破坏性的，甚至可能导致Kubernetes将新Pod调度到不同的节点。然而，正在进行工作以实现对活动Pod的原地更新。

像karmada这样的社区项目进一步允许您跨多个集群扩展应用程序。

让我们通过使用HPA和CA快速了解一个示例。

您将一个应用程序部署到您的集群，并配置一个HPA来自动调整应用程序Pods的数量在2到10之间。需求增加，HPA要求调度器将Pods数量从2增加到4。这可以实现，但需求继续上升，HPA要求调度器再增加两个Pods。然而，这次调度器找不到具有足够资源的节点，并将两个新的Pods标记为待定。CA注意到待定的Pods并动态添加一个新的集群节点。一旦节点加入集群，调度器将待定的Pods分配给它。

缩小规模的过程是相同的。例如，当需求减少时，HPA会减少Pods的数量。这可能会触发CA减少集群节点的数量。在删除集群节点时，Kubernetes必须将节点上的所有Pods驱逐，并在存活节点上用新Pods替换它们。

有时您会听到人们提到"多维度自动缩放"。这是将多个扩展方法结合起来的术语——扩展Pods和节点，或者水平扩展应用程序（添加更多Pods）和垂直扩展（向现有Pods添加更多资源）。

这一切都是关于状态。
```
在继续之前，了解以下概念非常重要。如果您已经了解这些概念，请直接跳转到“使用部署进行滚动更新”部分。

- 期望状态（Desired state）
- 观察状态（有时称为“实际状态”或“当前状态”）
- 协调（Reconciliation）

“期望状态”是您想要的状态，“观察状态”是您实际拥有的状态，目标是让它们始终匹配。当它们不匹配时，控制器会启动一个“协调”过程，将观察状态与期望状态重新同步。

“声明式模型”是我们在不告诉Kubernetes“如何”实现它的情况下向Kubernetes声明期望状态的方式。您把“如何”的工作交给Kubernetes。

(^8) https://karmada.io/

6：Kubernetes部署 75

**声明式与命令式**

声明式模型描述了一个最终目标-您告诉Kubernetes您想要什么。命令式模型需要长列表的命令，告诉Kubernetes如何达到最终目标。

以下类比将有所帮助：

- 声明式：给我一块巧克力蛋糕，可以喂十个人。
- 命令式：开车去商店。买鸡蛋、牛奶、面粉、可可粉...开车回家。
  预热烤箱。将材料混合。放入蛋糕模中。如果是通风烤箱，
  将蛋糕放入烤箱烘烤30分钟。如果不是通风烤箱，将蛋糕
  放入烤箱烘烤40分钟。设置一个定时器。定时器到期后，
  从烤箱中取出蛋糕并关闭烤箱。待凉后再加上糖霜。

声明式模型更简单，将“如何”工作交给了Kubernetes。命令式模型更加复杂，因为您需要提供所有步骤和命令，希望能够实现最终目标-在本例中为了给十个人做一块巧克力蛋糕。

让我们看一个更具体的例子。

假设您有一个包含两个微服务（前端和后端）的应用程序。您预计需要五个前端副本和两个后端副本。

采用声明式方法，您编写一个简单的YAML文件，请求五个前端Pod，监听外部端口80，并请求两个后端Pod，监听内部端口27017。然后，将文件交给Kubernetes，然后坐等Kubernetes完成。这是一件美妙的事情。

相反，命令式模型是一份通常很长的复杂指令列表，没有期望状态的概念。更糟糕的是，命令式指令可能有无穷无尽的变化。例如，拉取和启动_containerd_容器的命令与拉取和启动_CRI-O_容器的命令是不同的。这导致了更多的工作，并容易出错，而且因为它没有声明期望状态，所以没有自我修复能力。这是非常糟糕的。

Kubernetes支持这两种模型，但强烈推荐使用声明式模型。

```
注意：containerd和CRI-O是在Kubernetes工作节点上运行的CRI运行时，负责执行诸如启动和停止容器之类的底层任务。
```

**控制器与协调**

协调对于期望状态至关重要。

```
例如，ReplicaSet是作为后台控制器实现的，运行在一个协调循环中，确保始终存在正确数量的Pod副本。如果没有足够的Pod副本，它会添加更多。如果有太多，它会终止一些。假设期望状态是十个副本，但只有八个存在。无论这是由于故障还是由于自动伸缩器请求增加副本，都没有关系。ReplicaSet控制器会创建两个新副本，将观察状态与期望状态同步。最好的部分是，它不需要您的帮助！完全相同的协调过程还实现了自我修复、扩展、滚动更新和回滚功能。让我们更仔细地看看滚动更新和回滚。
```

**使用部署进行滚动更新**

```
部署在零停机滚动更新（rollout）方面表现出色。但是，如果您将应用程序设计得满足以下条件，它们将发挥最佳效果：
```

```
1.通过API松耦合
2.向前向后兼容
```
这两点都是现代云原生微服务应用的特点，并且工作原理如下。

您的微服务应该始终松散耦合，并且只通过定义良好的API进行通信。这样做意味着您可以更新和修补任何微服务，而不必担心对其他微服务产生影响 - 所有连接都是通过规范化的API进行的，这些API公开了文档化的接口并隐藏了具体细节。

确保发布是向后和向前兼容的意味着您可以执行独立的更新，而不必关心客户端使用的版本。一个简单的非技术类比是一辆汽车。汽车暴露了一个标准的驾驶“API”，包括方向盘和脚踏板。只要您不改变这个“API”，您可以重新映射发动机，更换排气管，并获得更大的刹车，而驾驶员不需要学习任何新技能。

有了这些要点，零停机时间的发布工作原理如下。

假设您正在运行五个无状态微服务的副本。只要所有客户端通过向后和向前兼容的API连接，客户端可以连接到这五个副本中的任何一个。为了进行发布，Kubernetes创建一个运行新版本的副本，并终止运行旧版本的副本。此时，您有四个副本运行旧版本，一个副本运行新版本。此过程重复进行，直到所有五个副本都运行新版本。由于应用程序是无状态的，且有多个副本正在运行，因此客户端不会经历任何停机或服务中断。

背后发生了更多的事情，让我们仔细看看。

每个微服务都构建为一个容器，并包装在一个Pod中。然后，您将每个Pod包装在自己的Deployment中，以实现自我修复、扩展和滚动更新。每个Deployment都描述了以下所有内容：

- Pod副本的数量
- 要使用的容器镜像
- 网络端口
- 如何执行滚动更新

您将Deployment YAML文件发布到API服务器，ReplicaSet控制器确保正确的Pod数量被调度。它还监视集群，确保观察到的状态与期望的状态相匹配。Deployment位于ReplicaSet之上，管理其配置并添加滚动发布和回滚机制。

到目前为止都很好。

现在，假设您面临已知的漏洞，并且需要发布一个带有修复的更新。为此，您更新**相同的DeploymentYAML文件**，使用新的Pod规范重新发布到API服务器。这将使用一个新的期望状态更新现有的Deployment对象，该状态请求相同数量的Pod，但都运行包含修复的新版本。

此时，观察到的状态不再匹配期望的状态 - 您有五个旧的Pod，但您希望有五个新的Pod。

为了协调，Deployment控制器创建一个新的ReplicaSet，定义相同数量的Pod，但运行较新的版本。现在，您有两个ReplicaSet - 一个用于旧版本的Pod，一个用于新版本的Pod。随着新ReplicaSet中Pod数量的递增，旧ReplicaSet中的Pod数量递减。最终结果是一个平滑的增量发布，没有停机时间。

将来的更新也会发生同样的过程 - 您继续更新相同的Deployment清单，并将其存储在版本控制系统中。

图6.4显示了一个已经更新一次的Deployment。最初的发布在左侧创建了一个ReplicaSet，更新在右侧创建了一个ReplicaSet。更新完成后，左侧的ReplicaSet不再管理任何Pod，而右侧的ReplicaSet正在管理三个活动的Pod。

在接下来的部分中，您将看到保留旧的ReplicaSet及其配置的重要性。

**回滚**

如图6.4所示，旧的ReplicaSet被关闭，并且不再管理任何Pod。但是，它们的配置仍然存在，可以用于轻松回滚到先前的版本。

回滚过程是发布过程的相反过程 - 在当前的ReplicaSet减少的同时，将旧的ReplicaSet增加。

图6.5显示了相同的应用回滚到先前的配置，并由先前的ReplicaSet管理。

但这还不是结束。Kubernetes允许您对发布和回滚进行精细控制。例如，您可以插入延迟，控制发布的速度和节奏，甚至可以探测更新后的副本的健康状况和状态。

但是，空谈是没有用的。让我们看看Deployments的实际操作。

### 创建一个Deployment
您如果想跟着进行操作，需要从书籍的GitHub存储库获取实验文件。如果您还没有获取到文件，请运行以下命令来克隆存储库。

```
$ git clone https://github.com/nigelpoulton/TheK8sBook.git
正在克隆到 'TheK8sBook'...
```

切换到deployments文件夹并从那里运行所有后续命令。

```
$ cd TheK8sBook/deployments
```

我们将使用deploy.yml文件，如下面的代码片段所示。它定义了一个包含在Deployment中的单容器Pod。我们将关注和强调其中的部分。

```
kind: Deployment
apiVersion: apps/v1
metadata:
name: hello-deploy <<==== Deployment名称（必须是有效的DNS名称）
spec:
replicas: 10 <<==== 部署和管理Pod副本的数量
selector:
matchLabels:
app: hello-world
revisionHistoryLimit: 5
progressDeadlineSeconds: 300
minReadySeconds: 10
strategy: <<==== 此块定义滚动更新设置
type: RollingUpdate
rollingUpdate:
maxUnavailable: 1
maxSurge: 1
template: <<==== 以下是Pod模板
metadata:
labels:
app: hello-world
spec:
containers:
```

- name: hello-pod
  image: nigelpoulton/k8sbook:1.0
  ports:
    - containerPort: 8080

文件中有很多内容，让我们解释其中最重要的部分。

前两行告诉Kubernetes基于**apps/v1** API中定义的Deployment资源版本创建一个Deployment对象。

**metadata**部分将Deployment命名为**hello-deploy**。您应该始终为对象提供有效的DNS名称。这意味着在对象名称中只能使用字母数字、点和破折号。

**spec**部分是大部分操作发生的地方。

**spec.replicas**要求创建十个Pod副本。在这种情况下，ReplicaSet控制器将创建十个在**spec.template**部分定义的Pod副本。

**spec.selector**是Pod需要具有的标签列表，以便Deployment和ReplicaSet控制器对其进行管理。此标签选择器必须与Pod模板块（**spec.template.metadata.labels**）中的Pod标签匹配。在此示例中，它们都指定了**app=hello-world**标签。

**spec.revisionHistoryLimit**告诉Kubernetes保留前五个ReplicaSets，以便您可以回滚到最近的五个版本。保留更多版本可以提供更多回滚选项，但在具有大量发布的大型集群上保留过多版本可能会导致对象膨胀并引起问题。

**spec.progressDeadlineSeconds**告诉Kubernetes在报告更新停滞之前，给每个新副本一个五分钟的启动窗口。计数器针对每个副本重新设置，这意味着每个副本都有自己的五分钟窗口来正确启动（进度）。

**spec.strategy**告诉Deployment控制器在进行升级时如何更新Pod。在本章的后面部分，当您执行升级时，我们将解释这些设置。

最后，**spec.template**下面的所有内容定义了此Deployment将要管理的Pod。此示例定义了一个使用**nigelpoulton/k8sbook:1.0**镜像的单容器Pod。

运行以下命令在集群上创建Deployment。

```
注意：所有的kubectl命令都包含了从kubeconfig文件中获取的必要身份验证令牌。
```

$ kubectl apply -f deploy.yml
deployment.apps/hello-deploy已创建

此时，Deployment配置已经持久化到集群存储中作为意向记录，并且Kubernetes已经将十个副本调度到健康的工作节点上。Deployment和ReplicaSet控制器也正在后台运行，监视状态并渴望执行它们的协调魔法。

随时运行**kubectl get pods**命令来查看这十个Pod。

**检查Deployments**

您可以使用正常的**kubectl get**和**kubectl describe**命令来查看Deployments和ReplicaSets的详细信息。

$ kubectl get deploy hello-deploy
名称 已就绪 最新更新 可用 年龄
hello-deploy 10/10 10 10 105秒
$ kubectl describe deploy hello-deploy
名称: hello-deploy
命名空间: 默认
注解: deployment.kubernetes.io/revision: 1
选择器: app=hello-world
副本: 10 期望的 | 10 已更新 | 10 总数 | 10 可用 | 0 不可用
策略类型: 滚动更新
最小可运行时间: 10 秒
滚动更新策略: 最多1个不可用，最多1个增加
Pod模板:
标签: app=hello-world
容器:
hello-pod:
镜像: nigelpoulton/k8sbook:1.0
端口: 8080/TCP
<SNIP>
旧的副本集: 无
新的副本集: hello-deploy-54f5d46964 (已创建10个副本/总共10个副本)
<Snip>

输出已被裁剪以提高可读性，但请花一点时间仔细阅读，因为它们包含了很多信息，将加强您所学到的知识。

正如前面提到的，部署自动创建关联的副本集。使用以下命令验证这一点。

$ kubectl get rs
名称 期望的 当前的 可用的 年龄
hello-deploy-54f5d46964 10 10 10 3m45s

由于您只进行了初始部署，因此只有一个副本集。然而，您可以看到副本集的名称与部署的名称相匹配，并在末尾添加了一个哈希值。这是部署清单中Pod模板部分的密码哈希值（**spec.template** 下面的所有内容）。很快您就会看到这一点，但对Pod模板部分进行更改会启动一个部署，并使用更新后的Pod模板创建一个新的副本集。

您可以使用 **kubectl describe** 命令获取有关副本集的更详细信息。您的副本集将有一个不同的名称。

6: Kubernetes部署 82

$ kubectl describe rs hello-deploy-54f5d46964
名称: hello-deploy-54f5d46964
命名空间: 默认
选择器: app=hello-world,pod-template-hash=54f5d46964
标签: app=hello-world
pod-template-hash=54f5d46964
注解: deployment.kubernetes.io/desired-replicas: 10
deployment.kubernetes.io/max-replicas: 11
deployment.kubernetes.io/revision: 1
受控于: 部署/hello-deploy
副本: 10 当前的/ 10 期望的
Pod状态: 10 运行中/ 0 等待/ 0 成功/ 0 失败
Pod模板:
标签: app=hello-world
pod-template-hash=54f5d46964
容器:
hello-pod:
镜像: nigelpoulton/k8sbook:1.0
端口: 8080/TCP
<Snip>

请注意输出与部署输出的相似之处。这是因为部署决定了副本集的配置，并将副本集信息汇总到部署中。副本集的状态（观察到的状态）也会汇总到部署的状态中。

**访问应用**

部署正在运行，您有十个副本。然而，您需要一个Kubernetes服务对象才能连接到应用程序。我们将在下一章讲解服务，但现在只需知道它们提供对Pod的网络访问即可。

以下的YAML来自**deployments**文件夹中的**lb.yml**文件。它定义了一个与您刚刚部署的Pod配合使用的服务。

apiVersion: v1
kind: Service
metadata:
name: lb-svc
labels:
app: hello-world
spec:
type: LoadBalancer
ports:

- port: 8080
  protocol: TCP
  selector:
  app: hello-world <<==== 将流量发送到具有此标签的Pod

```
使用以下命令进行部署。
```

```
$ kubectl apply -f lb.yml
service/lb-svc已创建
```

```
验证服务配置，并复制EXTERNAL-IP列中的值。
```

```
$ kubectl get svc lb-svc
名称 类型 CLUSTER-IP EXTERNAL-IP 端口
lb-svc LoadBalancer 10.100.247.251 localhost 8080:31086/TCP
```

```
在新的浏览器标签页中，使用EXTERNAL-IP字段上的值以8080端口连接。如果您在本地Docker Desktop集群上，则为localhost:8080。如果您的集群在云中，则为公共IP或DNS名称。
图6.6显示了浏览器访问localhost:8080上的应用程序。
```
### 手动调整应用程序的规模。

```
您可以通过两种方式手动调整部署的规模：
```

- 命令式调整
- 声明式调整

命令式方法使用 **kubectl scale** 命令，而声明式方法则需要您更新部署的 YAML 文件并重新提交到集群。我们会展示两种方法，但是声明式方法是首选。

```
验证当前是否有十个副本。
```

6: Kubernetes 部署 84

$ kubectl get deploy hello-deploy
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
hello-deploy   10/10   10           10          28m

运行以下命令以命令式地将副本数减少到五个，并验证操作是否成功。

$ kubectl scale deploy hello-deploy --replicas 5
deployment.apps/hello-deploy scaled

$ kubectl get deploy hello-deploy
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
hello-deploy   5/5       5               5             29m

恭喜，您已成功将部署的副本数缩减至5个。然而，这可能会带来一个潜在问题......

您环境的当前状态与您声明式的清单不再匹配——集群中只有五个副本，但部署的 YAML 文件仍定义了10个。这可能在使用 YAML 文件执行未来的更新时会导致问题。例如，更新 YAML 文件中的镜像版本并重新提交到集群将会把副本数更改回10个，而您可能并不想要这样。因此，您应始终确保您的 YAML 清单与您的实际环境保持同步，而最简单的方法是通过 YAML 清单以声明方式进行所有更改。

让我们重新提交 YAML 文件，并将副本数恢复为10个。

$ kubectl apply -f deploy.yml
deployment.apps/hello-deploy configured

$ kubectl get deploy hello-deploy
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
hello-deploy   10/10   10           10          38m

您可能已经注意到，缩放操作几乎是即时的。而接下来您将看到的滚动更新则不是这样。

Kubernetes 还具有自动缩放器，它可以根据当前需求自动调整 Pod 和基础架构的规模。

### 执行滚动更新

让我们执行滚动更新。

6: Kubernetes 部署 85

```
注意：滚动更新、发布、零停机更新和滚动更新这些术语的含义相同，我们会互换使用。
```

应用程序的新版本已经创建、测试并上传到 Docker Hub，并使用 **nigelpoulton/k8sbook:2.0** 标签。现在只剩下您执行滚动更新的任务了。为了简化过程并将重点放在 Kubernetes 上，我们忽略了现实世界中的 CI/CD 工作流程和版本控制工具。

在继续之前，重要的是您理解所有的“更新”操作实际上都是**“替换”**操作。当您更新一个 Pod 时，实际上是删除它并用一个新的 Pod 替换它。Pod 是**不可变对象**，所以在部署后您永远不会更改或更新它们。

第一步是在 **deploy.yml** 文件中更新镜像版本。使用您喜欢的编辑器将镜像版本更新为 **nigelpoulton/k8sbook:2.0** 并保存更改。

下面缩略的输出显示了要更新的文件中的哪一行。

apiVersion: apps/v1
kind: Deployment
metadata:
name: hello-deploy
spec:
replicas: 10
<Snip>
template:
<Snip>
spec:
containers:

- name: hello-pod
  image: nigelpoulton/k8sbook:2.0 <<==== 更新此行为 2.0
  ports:
    - containerPort: 8080

下次将文件提交到 Kubernetes 时，运行版本为 **1.0** 的每个 Pod 将被删除并替换为运行版本为 **2.0** 的新 Pod。然而，在这之前，让我们查看控制滚动更新工作方式的设置。

YAML 文件的 **spec** 部分包含了告诉 Kubernetes 如何执行更新的所有设置。

6: Kubernetes 部署 86

<Snip>
revisionHistoryLimit: 5
progressDeadlineSeconds: 300
minReadySeconds: 10
strategy:
type: RollingUpdate
rollingUpdate:
maxUnavailable: 1
maxSurge: 1
<Snip>

**revisionHistoryLimit** 告诉 Kubernetes 保留前五个发布的配置，以便进行轻松回滚操作。
**progressDeadlineSeconds** 告诉Kubernetes在假设Pod副本失败之前，给每个新的Pod副本一个五分钟的启动时间窗口。

**spec.minReadySeconds** 限制了Kubernetes替换副本的速率。此配置告诉Kubernetes在每个副本之间等待10秒钟。较长的等待时间可以更好地捕捉问题，并防止替换所有副本为损坏的副本的情况发生。在实际应用中，您需要将此值设置得足够大，以捕捉常见的故障。

还有一个嵌套的**spec.strategy**映射，告诉Kubernetes：

- 使用**RollingUpdate**策略进行更新
- 永远不要有超过期望状态的一个Pod（**maxUnavailable: 1**）
- 永远不要有超过期望状态的一个Pod（**maxSurge: 1**）

该应用的期望状态是十个副本。因此，**maxSurge: 1**表示在部署过程中Kubernetes可以增加到11个副本，而**maxUnavailable**允许其减少到9个。最终的结果是每次更新两个Pod副本（9和11之间的差值为2）。

这一切都很好，但是Kubernetes如何知道要删除和替换哪些Pod呢？

通过标签！

如果您仔细查看**deploy.yml**文件，您会看到Deployment规范具有一个选择器块。这是一个标签列表，Deployment控制器在进行部署期间查找要更新的Pod时会查找这些标签。在这个例子中，控制器将查找具有**app=hello-world**标签的Pod。如果您看一下文件底部的Pod模板，您会注意到它创建具有相同标签的Pod。最终结果：此部署将创建具有**app=hello-world**标签的Pod，并在执行更新等操作时选择具有相同标签的Pod。

Pod和Deployment都是不可变的，这意味着您在创建Deployment之后无法更改选择器或标签。

运行以下命令将更新的清单文件发布到集群，并开始部署。

$ kubectl apply -f deploy.yml
deployment.apps/hello-deploy已配置

部署将每次替换两个Pod，并在每个Pod之间等待十秒钟。这意味着完成部署可能需要一到两分钟的时间。

您可以使用**kubectl rollout status**命令来监视部署的进度。

$ kubectl rollout status deployment hello-deploy
正在等待部署"hello-deploy"的部署... 4个新副本中的4个...
正在等待部署"hello-deploy"的部署... 4个新副本中的4个...
正在等待部署"hello-deploy"的部署... 10个新副本中的6个...
^C

如果您在部署仍在进行时退出监视进度，可以运行**kubectl get deploy**命令并查看更新相关设置的效果。例如，以下命令显示已经更新了六个副本，当前可用副本为九个。九个副本比期望状态的十个少一个，这是清单中**maxUnavailable=1**值的结果。

$ kubectl get deploy hello-deploy
名称 就绪 上次更新 可用 年龄
hello-deploy 9/10 6 9 63分钟

**暂停和恢复部署**

您可以使用**kubectl**暂停和恢复部署。

如果您的部署仍在进行中，请使用以下命令暂停它。

$ kubectl rollout pause deploy hello-deploy
deployment.apps/hello-deploy已暂停

在暂停的部署期间运行**kubectl describe**命令可以提供一些有趣的信息。

$ kubectl describe deploy hello-deploy
名称: hello-deploy
命名空间: default
注解: deployment.kubernetes.io/revision: 2
选择器: app=hello-world
副本数: 10个期望 | 6个已更新 | 11个总共 | 9个可用 | 2个不可用
策略类型: RollingUpdate
MinReadySeconds: 10
RollingUpdateStrategy: 1个最大不可用，1个最大增长
（删除部分）
**注解**行显示对象正在修订2版（修订1版是初始推出，当前更新是修订2版）。**副本**显示推出尚未完成。倒数第三行显示部署条件正在进行但已暂停。最后两行中，您可以看到初始发布的副本集正在管理3个副本，而新发布的副本集正在管理6个副本。

如果在推出过程中发生了扩容事件，Kubernetes将平衡额外的副本集。在这个示例中，如果部署通过添加10个新副本扩展到20个，Kubernetes将分配3个新副本给旧副本集，6个新副本给新副本集。

运行以下命令继续推出。

$ kubectl rollout resume deploy hello-deploy
deployment.apps/hello-deploy resumed

推出完成后，您可以使用**kubectl get deploy**命令检查状态。

6: Kubernetes部署 89

$ kubectl get deploy hello-deploy
名称   已准备好   最新   可用   年龄
hello-deploy   10/10   10   10   71分钟

输出显示推出已完成 - 10个Pod已经是最新且可用。

如果您一直在跟随，刷新浏览器并查看更新的应用程序。之前的版本显示**Kubernetesrocks！**，这个版本显示**WebAssemblyiscoming！**

```
图6.7
```

### 执行回滚操作

如前所述，Kubernetes将旧副本集保留为记录的修订历史和回滚的简单方式。以下命令显示具有两个修订版本的部署历史记录。

$ kubectl rollout history deployment hello-deploy
deployment.apps/hello-deploy
修订   更改原因
1    <none>
2    <none>

修订1是基于**1.0**镜像的初始发布。修订2是刚刚将Pod更新为运行**2.0**镜像的推出。

以下命令显示与每个修订版本相关联的两个副本集。

6: Kubernetes部署 90

$ kubectl get rs
名称   期望   当前   已准备好   年龄
hello-deploy-5f84c5b7b7   10   10   10   27分钟
hello-deploy-54f5d46964   0   0   0   93分钟

下一个**kubectl describe**命令针对旧副本集运行，并证明其配置仍然引用旧版本的镜像。输出已经过修剪以适应本书，并且您的副本集名称将不同。

$ kubectl describe rs hello-deploy-54f5d46964
名称：hello-deploy-54f5d46964
命名空间：default
选择器：app=hello-world，pod-template-hash=54f5d46964
标签：app=hello-world
pod-template-hash=54f5d46964
注解：deployment.kubernetes.io/desired-replicas: 10
deployment.kubernetes.io/max-replicas: 11
deployment.kubernetes.io/revision: 1
受控于：Deployment/hello-deploy
副本：0 当前 / 0 期望
Pod状态：0 正在运行 / 0 等待 / 0 成功 / 0 失败
Pod模板：
容器：
hello-pod：
镜像：nigelpoulton/k8sbook:1.0 <<==== 仍配置为旧版本
端口：8080/TCP
<Snip>

您感兴趣的是在本书中倒数第二行显示的内容，列出了旧版本的镜像。这意味着将部署切换回这个副本集将自动用运行**1.0**镜像的新副本替换所有Pod。

```
注意：如果您听到将回滚称为更新，不要感到困惑。它们确实是一样的。它们遵循与更新/推出相同的逻辑和规则 - 终止当前镜像的Pod并用运行新镜像的Pod替换它们。在回滚的情况下，新镜像实际上是旧版本。
```

以下示例使用**kubectl rollout**将应用程序还原到修订1。这是一条命令，不推荐使用。然而，对于快速回滚很方便，只需记住更新您的源YAML文件以反映更改。

6: Kubernetes部署 91

$ kubectl rollout undo deployment hello-deploy --to-revision=1
deployment.apps "hello-deploy" rolled back
尽管看起来操作是瞬间完成的，但实际上并非如此。就像之前提到的那样，在部署的**策略**块中，回滚遵循相同的规则，用于定义扩展的规则。你可以通过以下的**kubectl get deploy**和**kubectl rollout**命令来验证和跟踪进度。

$ kubectl get deploy hello-deploy
名称   就绪   最新   可用   年龄
hello-deploy 9/10 6 9 96m

$ kubectl rollout status deployment hello-deploy
等待部署 "hello-deploy"... 已更新 6 个新副本中的 10 个...
等待部署 "hello-deploy"... 已更新 7 个新副本中的 10 个...
等待部署 "hello-deploy"... 已更新 8 个新副本中的 10 个...
等待部署 "hello-deploy"... 1 个旧副本正在等待终止...
等待部署 "hello-deploy"... 10 个已更新的副本中的 9 个可用...
^C

与扩展相同，回滚每次替换两个Pod，并在每次替换后等待十秒。

恭喜！你已经完成了一个滚动更新和一个成功的回滚。

**扩展和标签**

你已经看到了，部署和副本集使用标签和选择器来确定它们拥有和管理的Pods。

在早期版本的Kubernetes中，如果静态Pod的标签与部署的标签选择器匹配，部署会夺取静态Pod的所有权。然而，Kubernetes的最新版本通过为控制器创建的Pods添加一个系统生成的**pod-template-hash**标签来防止这种情况发生。

来看一个快速的例子。你的集群有五个具有**app=front-end**标签的静态Pods。你添加了一个新的部署，请求创建具有相同标签的十个Pods。在较旧的Kubernetes版本中，会看到已存在的五个具有相同标签的静态Pods，部署会夺取它们的所有权，并只创建五个新的Pods。结果就是，有十个具有**app=front-end**标签的Pods，全部由部署所拥有。然而，原来的五个静态Pods可能正在运行不同的应用程序，而你可能不希望部署管理它们。

幸运的是，现代的Kubernetes版本会为部署（副本集）创建的所有Pods打上**pod-template-hash**标签。这样，高级控制器就无法夺取现有静态Pods的所有权。

仔细看以下的输出片段，可以看到**pod-template-hash**标签如何连接部署与副本集，以及副本集与Pods。

$ kubectl describe deploy hello-deploy
名称: hello-deploy
<片段>
新副本集: hello-deploy-54f5d46964

$ kubectl describe rs hello-deploy-54f5d46964
名称: hello-deploy-54f5d46964
<片段>
选择器: app=hello-world,pod-template-hash=54f5d46964

$ kubectl get pods --show-labels
名称   就绪   状态 标签
hello-deploy-54f5d46964.. 1/1 Running app=hello-world,pod-template-hash=54f5d46964
hello-deploy-54f5d46964.. 1/1 Running app=hello-world,pod-template-hash=54f5d46964
hello-deploy-54f5d46964.. 1/1 Running app=hello-world,pod-template-hash=54f5d46964
hello-deploy-54f5d46964.. 1/1 Running app=hello-world,pod-template-hash=54f5d46964
<片段>

副本集在其标签选择器中包含**pod-template-hash**标签，而部署则没有。这是合理的，因为实际上是副本集管理Pods，而不是部署。

你不应尝试修改**pod-template-hash**标签。

### 清理

使用**kubectl delete -f deploy.yml**和**kubectl delete -f lb.yml**命令来删除示例中创建的部署和服务。

### 章节总结

在本章中，你了解了部署是在Kubernetes上管理无状态应用程序的好方法。它们通过为Pods提供自我修复、可扩展性、滚动更新和回滚等功能增强了其功能。

与Pods一样，部署是Kubernetes API中的对象，应该以声明性的方式与之交互。它们在**apps/v1** API中定义，并作为控制平面上的一个调和循环运行的控制器。

在幕后，部署使用副本集来完成与Pods的大部分工作。例如，实际上是副本集创建、终止和管理Pods的数量。


Pod副本的数量是有限制的。然而，您不应直接创建或编辑ReplicaSets，而是应该通过Deployment进行配置。

您可以通过编辑Deployment的YAML文件并重新提交到集群来手动缩放部署。然而，Kubernetes有自动缩放程序，根据需求自动调整部署规模。

滚动更新是以有序、有组织的方式删除旧的Pod并用新的Pod替换它们。

## 7：Kubernetes Services

Pods不可靠，您不应直接连接到它们。您应该**始终**通过Service连接到它们。

本章内容如下：

- Service理论
- 与Service的实际操作

### Service理论

Kubernetes将Pods视为临时对象，并在发生以下任何事件时删除它们：

- 缩减操作
- 滚动更新
- 回滚
- 失败

这意味着它们是不可靠的，应用程序不能依赖它们来响应请求。幸运的是，Kubernetes有一个解决方案——**Service对象**位于一个或多个相同的Pod之前，并通过**可靠的**DNS名称、IP地址和端口向外界公开它们。

图7.1显示了一个客户端通过名为**app1**的Service连接到一个应用程序。客户端连接到Service的名称或IP，而Service将请求转发到其后面的应用程序Pods。

7: Kubernetes Services 95

```
图7.1-客户端通过Service访问Pods
```

```
注意：Service是Kubernetes API中的资源，因此我们将“S”大写，以避免与其他用法混淆。
```

每个Service都有一个前端和一个后端。前端包括一个DNS名称、IP地址和网络端口，Kubernetes保证它们永远不会改变。后端是一个标签选择器，用于将流量发送到具有匹配标签的健康Pods。回顾图7.1，客户端将流量发送到Service的**app1:8080**或**10.99.11.23:8080**，Kubernetes保证它将到达具有**project=tkb**标签的Pod。

Service还具有足够智能的功能，可以维护一个包含具有匹配标签的健康Pods的列表。这意味着您可以进行缩放、滚动更新和回滚操作，甚至Pods可能会失败，但Service始终会有一个最新的活动健康Pods列表。

**标签和松耦合**

Service使用标签和选择器来确定将流量发送到哪些Pods。这与将Deployments与Pods松耦合的技术相同。

图7.2显示了一个Service选择具有**project=tkb**和**zone=prod**标签的Pods。

7: Kubernetes Services 96

```
图7.2-Services和标签
```

在此示例中，Service将流量发送到Pod A、Pod B和Pod D，因为它们都具有所需的标签。Pod D具有额外的标签并不重要。但是，它不会将流量发送到Pod C，因为它没有这两个标签。以下YAML定义了一个Deployment和一个Service。Deployment将创建具有**project=tkb**和**zone=prod**标签的Pods，而Service将向它们发送流量。

apiVersion：apps/v1
kind：Deployment
metadata：
name：tkb-2024
spec：
replicas：10
<Snip>
template：
metadata：
labels：
project：tkb <<==== 使用这些标签创建Pods
zone：prod <<==== 使用这些标签创建Pods
spec：
containers：
<Snip>
---

apiVersion：v1
kind：Service
metadata：
name：tkb
spec：
ports：

```
7: Kubernetes Services 97
```

- port：8080
  selector：
  project：tkb <<==== 发送到具有这些标签的Pods
  zone：prod <<==== 发送到具有这些标签的Pods

**Services和EndpointSlices**
每当您创建一个服务时，Kubernetes会自动创建一个关联的EndpointSlice来跟踪具有匹配标签的健康Pod。
工作原理如下。
您创建一个服务，然后_ EndpointSlice controller_自动创建一个关联的_EndpointSlice对象_。然后，Kubernetes会监视集群，寻找与服务标签选择器匹配的Pod。任何与选择器匹配的新Pod都会添加到EndpointSlice中，而任何删除的Pod都会被移除。应用程序通过集群DNS将流量发送到服务名称，应用程序的容器使用集群DNS解析名称为IP地址。然后容器将流量发送到服务的IP，服务将其转发到EndpointSlice中列出的一个Pod中。
旧版本的Kubernetes使用_Endpoints_对象而不是EndpointSlices。它们在功能上是相同的，但EndpointSlices在大型繁忙集群上的性能更好。

**服务类型**

```
Kubernetes有几种不同用例和要求的服务类型。主要的有：
```

- ClusterIP
- NodePort
- LoadBalancer

```
ClusterIP是最基本的，为内部Pod网络提供可靠的终端点（名称、IP和端口）。NodePort服务在ClusterIP之上构建，并允许外部客户端通过集群节点上的端口连接。LoadBalancer构建在两者之上，并与云负载均衡器集成，以便从互联网上进行极其简单的访问。这三个都很重要，所以让我们依次看一下每个。
```

```
ClusterIP服务-从集群内部访问应用程序
```

```
ClusterIP是默认的。它有一个名称和IP，被编程到内部网络中，只能从集群内部访问。这意味着：
```

```
7: Kubernetes Services 98
```

- IP只能在内部网络上进行路由
- 名称会自动注册到集群的内部DNS中
- 所有容器都预先设置为使用集群的DNS来解析名称

```
让我们考虑一个例子。
您要部署一个名为skippy的应用程序，并希望集群上的其他应用程序可以通过它的名称访问它。为了满足这些要求，您创建一个新的ClusterIP服务，称为skippy。Kubernetes创建服务，分配一个内部IP，并在集群的内部DNS中创建DNS记录。Kubernetes还配置集群上的所有容器使用集群DNS进行名称解析。这意味着集群上的每个应用都可以使用skippy名称连接到新应用。
然而，这在集群外部是不起作用的，因为ClusterIP不可路由，并且需要访问集群DNS。
```

我们将在服务发现章节中详细介绍。

```
NodePort服务-从集群外部访问应用程序
```

```
NodePort服务在ClusterIP服务的基础上添加了每个集群节点上外部客户端可以使用的专用端口。我们将这个专用端口称为“NodePort”。以下是一个称为skippy的NodePort服务的YAML示例。
```

```
apiVersion: v1
kind: Service
metadata:
name: skippy <<==== 注册到内部集群DNS（ClusterIP）
spec:
type: NodePort <<==== 服务类型
ports:
```

- port: 8080 <<==== ClusterIP端口
  targetPort: 9000 <<==== 容器中的应用程序端口
  nodePort: 30050 <<==== 每个集群节点上的外部端口（NodePort）
  selector:
  app: hello-world

```
将此内容发布到集群将创建一个具有通常可路由的内部IP和DNS名称的ClusterIP服务。它还会在每个集群节点上创建端口30050，并将其映射回ClusterIP。这意味着外部客户端可以将流量发送到任何集群节点的30050端口，并到达服务。
图7.3显示了一个NodePort服务在每个集群节点上暴露了三个Pod的30050端口。步骤1显示一个外部客户端访问NodePort上的节点。步骤2显示节点将请求转发到集群内部服务的ClusterIP。
```

7: Kubernetes Services 99

服务在步骤3中从EndpointSlice的始终最新的列表中选择一个Pod，并在步骤4中将其转发给所选的Pod。

```
图7.3-NodePortService
```

外部客户端可以将请求发送到任何集群节点，并且服务可以将请求发送到三个健康Pod中的任何一个。实际上，未来的请求可能会发送到其他Pod，因为服务执行基本的轮询负载均衡。

然而，NodePort服务有两个重要的限制：
- 他们使用30000-32767之间的高端口
- 客户端需要知道节点的名称或IP，以及节点是否健康

这就是为什么大多数人使用“负载均衡服务”。

**负载均衡服务 - 通过负载均衡器访问应用程序**

“负载均衡服务”是将服务暴露给外部客户端的最简单的方法。它通过在它们之前放置一个云负载均衡器来简化NodePort服务。

图7.4显示了一个负载均衡服务。正如您所见，它基本上是一个由高可用性负载均衡器前端的NodePort服务，具有公开可解析的DNS名称和低端口号。

7：Kubernetes服务100

```
图7.4-负载均衡服务
```

客户端通过可靠友好的DNS名称和低端口连接到负载均衡器，负载均衡器将请求转发到健康的集群节点上的NodePort。从那里开始，它与NodePort服务相同-将请求发送到内部ClusterIP服务，从EndpointSlice中选择一个Pod，并将请求发送到Pod。

以下YAML创建一个在端口8080上监听的负载均衡服务，并将其映射到具有“project=tkb”标签的Pod上的端口9000。它会在后台自动创建所需的NodePort和ClusterIP结构。

apiVersion: v1
kind: Service
metadata:
  name: lb <<==== 在集群DNS中注册
spec:
  type: LoadBalancer
  ports:
    - port: 8080 <<==== 负载均衡器端口
      targetPort: 9000 <<==== 容器内的应用程序端口
  selector:
    project: tkb

您将在后面的实践部分中创建和使用负载均衡服务。

7：Kubernetes服务101

**服务理论摘要**

服务位于Pod之前，使它们通过可靠的网络端点访问。

服务的前端提供一个IP、DNS名称和一个端口，保证在整个服务的生命周期内保持稳定。后端通过标签选择器将流量负载均衡到一组动态的与之匹配的Pod上。

ClusterIP服务是默认的，提供内部集群网络上的可靠端点。NodePort和LoadBalancer提供外部端点。

负载均衡服务在底层云平台上创建一个负载均衡器，以及将流量从负载均衡器转发到Pod的所有结构和映射。

### 与服务的实践

本节将向您展示如何以声明性和命令式的方式处理服务。正如Kubernetes总是更喜欢使用YAML文件以声明性方式部署和管理所有内容。但是，了解命令式命令也很有帮助。

如果您要跟随操作，请确保具备以下所有条件：

- Kubernetes集群
- 书籍的GitHub仓库的克隆

您将创建和使用负载均衡服务，并可以使用我们在第3章中向您展示如何创建的任何集群。如果您的集群位于云中，您将为云中的一个面向互联网的负载均衡器提供资源，并将使用公共IP或公共DNS名称。如果您使用本地集群，例如Docker Desktop，体验将是相同的，但您将使用本地构造，如localhost。

如果您尚未拥有书籍的GitHub仓库的副本，请使用以下命令进行克隆，然后切换到“services”目录。

$ git clone https://github.com/nigelpoulton/TheK8sBook.git
正在克隆到 'TheK8sBook'...

$ cd TheK8sBook/services

运行以下命令部署一个示例应用程序。它是一个部署，创建十个运行在端口8080上并带有“chapter=services”标签的Web应用程序的Pod。

7：Kubernetes服务102

$ kubectl apply -f deploy.yml
deployment.apps/svc-test已创建

确保Pod已成功部署，然后继续下一节。

**命令式处理服务**

**kubectl expose**命令为现有的部署创建一个服务。它足够智能，可以检查正在运行的部署，并创建所需的所有构造，如IP地址、DNS记录和正确的端口映射。

运行以下命令为“svc-test”部署创建一个新的负载均衡器服务。

$ kubectl expose deployment svc-test --type=LoadBalancer
service/svc-test已暴露

运行**kubectl get**查看其基本配置。可能需要一分钟才能填充**EXTERNAL-IP**列。
$ kubectl get svc -o wide
名称 类型 集群 IP 外部 IP 端口（S） 选择器
kubernetes ClusterIP 10.96.0.1 <none> 443/TCP <none>
svc-test LoadBalancer 10.10.19.33 212.2.245.220 8080:31755/TCP chapter=services

第一行是一个系统服务，它在集群上暴露了Kubernetes API。

您的服务位于第二行，有很多信息，让我们逐步了解一下。

首先，它被分配了与其前面的部署相同的名称 - svc-test。

类型列显示该服务是一个负载均衡器服务，示例中的服务分配了EXTERNAL-IP 212.2.245.220。如果您使用的是本地集群，例如Docker Desktop，EXTERNAL-IP 可能显示localhost。

CLUSTER-IP 列列出了服务的内部 IP，该 IP 只能在内部集群网络上进行路由。

PORT(S) 列显示了负载均衡器端口（8080）和节点端口（31755）。默认情况下，负载均衡器端口与应用程序侦听的端口相匹配，但您可以覆盖此设置。节点端口是从30000到32767之间随机分配的。

SELECTOR 列与 Pod 上的标签匹配。

有几个值得注意的事情。

首先，该命令检查了正在运行的部署，并创建了正确的端口映射和标签选择器 - 应用程序侦听端口为8080，并且所有10个 Pod 都具有chapter=services标签。

其次，即使它是一个负载均衡器服务，它也创建了所有的 ClusterIP 和 NodePort 构造。这是因为负载均衡器服务是在 NodePort 服务的基础上构建的，而 NodePort 服务又是在 ClusterIP 服务的基础上构建的，如图7.5所示。

```
图7.5-服务堆叠
```

kubectl describe 命令提供了更多详细信息。

$ kubectl describe svc svc-test
名称：svc-test
命名空间：default
标签：<none>
注释：<none>
选择器：chapter=services
类型：负载均衡器
IP Family Policy：SingleStack
IP Family：IPv4
IP：10.10.19.33
IPs：10.10.19.33
LoadBalancer Ingress：212.2.245.220
端口：<未设置> 8080/TCP <<==== 负载均衡器端口
TargetPort：8080/TCP <<==== 容器中的应用程序端口
NodePort：<未设置> 31755/TCP <<==== 集群节点上的 NodePort

端点：10.1.0.200:8080，10.1.0.201:8080，10.1.0.202:8080 + 7 个...
会话亲和性：None
外部流量策略：Cluster
事件：<none>

输出重复了很多您已经看过的内容，并对一些行添加了注释，以澄清与端口相关的不同值。

还有一些额外的行值得关注。

端点是服务的 EndpointSlice 对象中健康匹配 Pod 的列表。

会话亲和性允许您控制会话的稳定性 - 同一客户端的连接是否始终转到同一个 Pod。默认值为 None，允许将来自同一客户端的连接转发到任何 Pod。如果您的客户端和 Pod 将状态存储在 Pod 中并且需要会话稳定性，则可以尝试 ClientIP 选项。然而，这是一种反模式，因为微服务应用程序应该设计为具有处理可丢弃性的状态，其中客户端可以连接到服务的任何实例。

External Traffic Policy 决定命中服务的流量是否在所有集群节点上的 Pod 之间进行负载均衡，还是仅在流量到达的节点上的 Pod 之间进行负载均衡。默认值为 Cluster，它将流量发送到所有集群节点上的 Pod，但隐藏源 IP 地址。另一个选项是 Local，它仅将流量发送到流量到达的节点上的 Pod，但保留源 IP。

如果您的集群运行双栈网络，输出中还可能列出 IPv6 地址。

通过将浏览器指向 EXTERNAL-IP 列中的值的端口8080，测试服务是否正常工作。

```
图7.7
```
它能够正常工作。该应用程序运行在一个容器中，并监听8080端口。您创建了一个负载均衡服务，它监听8080端口，并将流量转发到每个集群节点上的一个NodePort服务的31755端口，然后该服务再将流量转发到8080端口的ClusterIP服务。从那里，流量会被发送到托管应用程序副本的Pod上的8080端口。

接下来，您将以声明性的方式再次执行相同的操作。不过，在此之前，您需要进行清理。

$ kubectl delete svc svc-test
service "svc-test" 已删除

**声明性的方式**

现在，是时候按照正确的方式——Kubernetes的方式来进行了。

**一个服务清单文件**

以下YAML代码来自于**lb.yml**文件，您将使用它来以声明性的方式部署一个负载均衡服务。

kind: Service
apiVersion: v1
metadata:
name: cloud-lb
spec:
type: LoadBalancer
ports:

- port: 9000 <<==== 负载均衡器端口
  targetPort: 8080 <<==== 容器内应用程序端口
  selector:
  chapter: services

让我们逐步解释一下。

前两行告诉Kubernetes，这个YAML文件定义了一个基于**core/v1** API组规范的服务对象。**core**组是特殊的，所以**apiVersion**字段省略了**core**。

**metadata**块指定了Kubernetes将在集群DNS中注册的服务的名称。您还可以在这里定义标签和注解。

**spec**部分定义了所有的前端和后端细节。这个示例告诉Kubernetes以声明性的方式部署一个**LoadBalancer**服务，在前端监听9000端口，并在后端将流量发送到具有**chapter=services**标签的Pod上的8080端口。

使用以下命令部署它。

7: Kubernetes Services 106

$ kubectl apply -f lb.yml
service/cloud-lb 已创建

**检查服务**

服务是常规的API资源，您可以使用通常的**kubectl get**和**kubectl describe**命令来检查它们。

$ kubectl get svc cloud-lb
名称 类型 集群IP 外部IP 端口
cloud-lb LoadBalancer 10.43.191.202 212.2.247.202 9000:30202/TCP

输出中的**EXTERNAL-IP**列显示为**<pending>**，这意味着云平台正在配置负载均衡器并分配IP地址。请不断刷新命令，直到出现一个地址。

该示例中的服务通过云负载均衡器在212.2.245.220上向互联网公开。如果您在本地集群上运行，比如Docker Desktop，它可能会在您笔记本电脑的localhost接口上进行公开。无论如何，您都可以在**EXTERNAL-IP**列中的值和9000端口上连接到它。

**EndpointSlice对象**

在本章的早些时候，您学到每个服务都有一个或多个专属的EndpointSlice对象。这些对象维护了一个与标签选择器匹配的Pod列表，并且您可以使用通常的**kubectl**命令来检查它们。

这些示例来自于一个运行双栈网络的集群。请注意，存在两个EndpointSlice——一个用于IPv4映射，另一个用于IPv6。您的集群可能只有IPv4映射。

$ kubectl get endpointslices
名称 地址类型 端口 Endpoints 年龄
lb-cloud-n7jg4 IPv4 8080 10.42.1.16,10.42.1.17,10.42.0.19 + 7 more... 2m1s
lb-cloud-9s6sq IPv6 8080 fd00:10:244:1::c,fd00:10:244:1::9 + 7 more... 2m1s

$ kubectl describe endpointslice lb-cloud-n7jg4
名称: lb-cloud-n7jg4
命名空间: default
标签: chapter=services
endpointslice.kubernetes.io/managed-by=endpointslice-controller.k8s.io
kubernetes.io/service-name=svc-test
注释: endpoints.kubernetes.io/last-change-trigger-time: 2024-01-01T18:13:40Z
地址类型: IPv4
端口:

7: Kubernetes Services 107

名称 端口 协议

---- ---- --------

<unset> 8080 TCP
Endpoints:
- 地址：10.42.1.16
  条件：
  准备就绪：是
  主机名：未设置
  目标引用：Pod/lb-cloud-9d7b4cf9d-hnvbf
  节点名称：k3d-tkb-agent-2
  区域：未设置
- 地址：10.42.1.17
  <剪辑>
  事件：无

**kubectl describe** 命令的完整输出中包含每个健康的 Pod 的块，其中包含有用的信息。如果一个 Service 匹配超过 100 个 Pod，它将具有多个 EndpointSlice。

### 清理。

运行以下命令以删除示例中创建的 Deployment 和 Services。当删除相关联的 Service 时，Kubernetes 会自动删除 Endpoints 和 EndpointSlices。

$ kubectl delete -f deploy.yml -f lb.yml
deployment.apps "svc-test" 已删除
service "cloud-lb" 已删除

### 章节总结。

在本章中，您了解到 Services 为 Pod 提供了稳定可靠的网络连接。它们有一个前端，具有稳定的 DNS 名称、IP 地址和端口，Kubernetes 保证这些信息永远不会改变。它们还有一个后端，将流量发送到与标签选择器匹配的健康 Pod。

ClusterIP Services 在 Kubernetes 内部网络上提供可靠的网络连接，NodePort Services 在每个集群节点上公开一个端口，LoadBalancer Services 与云平台集成，创建高可用的面向互联网的负载均衡器。

最后，Services 是 Kubernetes API 中的一级对象，应该通过版本控制的 YAML 文件进行声明性管理。

## 8:Ingress

```
Ingress 是通过单个 LoadBalancer Service 访问多个 Web 应用程序的全部内容。
在阅读本章之前，您需要对 Kubernetes Services 有一定的了解。如果您还没有这方面的知识，请考虑返回并先阅读前一章节。本章分为以下几个部分：
```

- 为 Ingress 做准备
- Ingress 架构
- 使用 Ingress 进行实践操作

我们将 _Ingress_ 大写，因为它是 Kubernetes API 中的一种资源。我们还将使用 _LoadBalancer_ 和 _load balancer_ 这两个术语，如下所示：

- _LoadBalancer_ 指的是 **type=LoadBalancer** 的 Kubernetes Service 对象
- _load balancer_ 指的是您云平台上的面向互联网的负载均衡器之一

```
例如，当您创建一个 Kubernetes LoadBalancer Service 时，Kubernetes 会与您的云平台进行通信，并提供一个云负载均衡器。
Ingress 在 Kubernetes 版本 1.19 中已经晋升为一般可用（GA）状态，在此之前经过了 15 个版本的测试。在它处于 alpha 和 beta 阶段的 3+ 年时间里，服务网格变得越来越受欢迎，并且功能上存在一些重叠。因此，如果您计划部署服务网格，可能就不需要 Ingress。
```

### 为 Ingress 做准备。

```
前一章节向您展示了如何使用 NodePort 和 LoadBalancer Services 将应用程序暴露给外部客户端。然而，这两者都有局限性。
NodePort Services 只能在高端口号上工作，客户端需要跟踪节点 IP 地址。LoadBalancer Services 解决了这个问题，但需要在内部 Services 和云负载均衡器之间建立一对一的映射关系。这意味着具有 25 个面向互联网的应用程序的集群将需要 25 个云负载均衡器，而且云负载均衡器是需要花钱的！您的云平台还可能限制您可以创建的负载均衡器的数量。
```

Ingress 通过让您通过单个云负载均衡器来暴露多个 Services 来解决这个问题。

它通过创建一个在集群上的端口 80 或 443 上的单个云负载均衡器，并使用 _基于主机_ 和 _基于路径_ 的路由将连接映射到集群上的不同 Services。

### Ingress 架构。

Ingress 在 **networking.k8s.io/v1** API 子组中定义，并且它需要常规的两个构造：

```
1. 一个资源
2. 一个控制器
```

资源 _定义_ 路由规则，控制器 _实现_ 这些规则。

然而，Kubernetes 没有内置的 Ingress 控制器，这意味着您需要安装一个。这与 Deployments、ReplicaSets、Services 和大多数其他资源不同，它们都有内置的预配置控制器。然而，一些云平台通过允许您在构建集群时安装一个来简化此过程。我们将在实践操作章节中向您展示如何安装流行的 NGINX Ingress 控制器。

一旦您拥有了一个 _Ingress 控制器_，您就可以使用规则部署 _Ingress 资源_，告诉控制器如何路由请求。
关于"路由"的话题，Ingress在OSI模型的第7层操作，也被称为"应用层"。这意味着它可以检查HTTP头部信息，并根据主机名和路径转发流量。

```
注意：OSI模型是TCP/IP网络的行业标准参考模型，共有七层，编号为1-7。较低的层级涉及信号和电子学，中间层处理通过确认和重试确保可靠性，较高的层级则增加了用于HTTP等服务的功能。Ingress在第7层操作，也被称为应用层，并实现了HTTP智能。
```

下表展示了如何根据主机名和路径将流量路由到后端的ClusterIP服务。

```
主机名示例    路径示例        后端K8s服务
shield.mcu.com   mcu.com/shield   shield
hydra.mcu.com   mcu.com/hydra   hydra
```

图8.1展示了两个请求同时命中同一个云负载均衡器。在幕后，DNS名称解析将两个主机名映射到同一个负载均衡器IP。Ingress控制器监视负载均衡器，并根据HTTP头部中的主机名路由请求。在这个例子中，shield.mcu.com被路由到**shield** ClusterIP服务，而hydra.mcu.com被路由到**hydra**服务。基于路径的路由逻辑也是相同的，在实践部分我们将会看到这两种路由方式。

```
图8.1基于主机名的路由
```

总结一下，一个Ingress可以通过一个云负载均衡器暴露多个ClusterIP服务。您可以创建和部署Ingress资源，告诉Ingress控制器如何根据请求头中的主机名和路径路由请求。您可能需要手动安装Ingress控制器。

让我们亲自来看看它是如何工作的。

### 使用Ingress实践。

如果您跟随本文进行操作，您将需要以下两样东西：

- 一个Kubernetes集群
- 本书GitHub仓库的克隆版本

如果您的集群在云上，示例将创建一个面向互联网的负载均衡器，并且您将使用公共IP地址或公共DNS名称。如果您有一个本地集群，比如Docker Desktop，您将使用localhost和私有IP地址。

如果您还没有克隆本书的GitHub仓库，请使用以下命令进行克隆。

```
8: Ingress 111
```

```
$ git clone https://github.com/nigelpoulton/TheK8sBook.git
正在克隆...
```

```
切换到TheK8sBook/ingress目录，并从该目录运行所有命令。您将完成以下所有步骤：
```

```
1.安装NGINX Ingress控制器
2.配置一个Ingress类
3.部署一个示例应用
4.配置一个Ingress对象
5.检查Ingress对象
6.配置DNS名称解析
7.测试Ingress
```

**安装NGINX Ingress控制器**

```
您将从Kubernetes GitHub仓库的一个YAML文件中安装NGINX控制器。它会安装一系列的Kubernetes构建，包括命名空间、服务账户、配置映射、角色、角色绑定等等。
使用以下命令进行安装。由于URL太长，我将命令分成了两行，您需要在一行上运行它。
```

```
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.9.4/deploy/static/provider/cloud/deploy.yaml
```

```
创建命名空间/ingress-nginx
创建服务账户/ingress-nginx
<省略>
```

运行以下命令检查**ingress-nginx**命名空间，并确保_controller_ Pod正在运行。进入运行阶段可能需要几秒钟时间，Windows用户需要将第一行末尾的反斜杠(\)替换为重音符(')。

```
8: Ingress 112
```

```
$ kubectl get pods -n ingress-nginx \
-l app.kubernetes.io/name=ingress-nginx
```

```
名称  就绪状态  状态    重启次数  年龄
ingress-nginx-admission-create-789md 0/1 已完成 0 25s
ingress-nginx-admission-patch-tc4cl 0/1 已完成 0 25s
ingress-nginx-controller-7445ddc6c4-csf98 0/1 运行中 0 26s
```
**Ingress类别**

```
Ingress类别允许您在单个集群上运行多个Ingress控制器。该过程很简单：
```

```
1.将每个Ingress控制器分配给一个Ingress类别
2.创建Ingress对象时，将它们分配给一个Ingress类别
```

```
如果您按照这样做，您将至少有一个名为nginx的Ingress类别。当您安装NGINX控制器时，就会创建它。
```

```
$ kubectl get ingressclass
名称       控制器                    参数       年龄
nginx   k8s.io/ingress-nginx      <none>    2m25s
```

```
如果您的集群已经有一个Ingress控制器，那么您将有多个类别。使用以下命令更详细地查看nginx Ingress类别。对于Ingress类别对象没有简称。
```

```
$ kubectl describe ingressclass nginx
名称: nginx
标签: app.kubernetes.io/component=controller
app.kubernetes.io/instance=ingress-nginx
app.kubernetes.io/name=ingress-nginx
app.kubernetes.io/part-of=ingress-nginx
app.kubernetes.io/version=1.9.4
注释: <none>
控制器: k8s.io/ingress-nginx
事件: <none>
```

当Ingress控制器和Ingress类别就位后，您就可以部署相同的环境并配置一个Ingress对象了。

8: Ingress 113

**配置基于主机和路径的路由**

本节将部署两个应用程序和一个单独的Ingress对象。Ingress将通过一个负载均衡器将流量路由到两个应用程序。负载均衡器可以是基于云的负载均衡器，也可以是本地集群上的localhost。

您将完成以下所有步骤：

```
1.部署名为shield的应用程序，并将其前置为名为svc-shield的ClusterIP服务（后端）
2.部署名为hydra的应用程序，并将其前置为名为svc-hydra的ClusterIP服务（后端）
3.部署一个Ingress对象，为以下主机名和路径创建一个单独的负载均衡器和路由规则
```

- 基于主机：shield.mcu.com >> svc-shield
- 基于主机：hydra.mcu.com >> svc-hydra
- 基于路径：mcu.com/shield >> svc-shield
- 基于路径：mcu.com/hydra >> svc-hydra
  4.配置DNS名称解析，使shield.mcu.com、hydra.mcu.com和mcu.com指向负载均衡器

图8.2显示使用基于主机的路由的整体架构。

```
图8.2基于主机的路由
```

流量流向shield Pods的步骤如下：

```
1.客户端将流量发送到shield.mcu.com或mcu.com/shield
2.DNS名称解析确保流量传递到负载均衡器
```

8: Ingress 114

```
3.Ingress控制器读取HTTP标头并查找主机名（shield.mcu.com）或路径（mcu.com/shield）
4.Ingress规则触发并将流量路由到svc-shield ClusterIP后端服务
5.ClusterIP服务确保流量到达shield Pod
```

**部署示例环境**

本节将部署两个应用程序和Ingress将路由流量到的ClusterIP服务。

实验在ingress文件夹的app.yml文件中定义，并包括以下内容。

- 名为shield的应用程序，监听端口8080，并由名为svc-shield的ClusterIP服务前置
- 另一个名为hydra的应用程序，也监听端口8080，并由名为svc-hydra的ClusterIP服务前置

使用以下命令部署它。

$ kubectl apply -f app.yml
service/svc-shield已创建
service/svc-hydra已创建
pod/shield已创建
pod/hydra已创建

一旦Pods和Services正常运行，就可以继续下一节创建Ingress。

**创建Ingress对象**

您将部署在ig-all.yml文件中定义的Ingress对象。它描述了一个名为mcu-all的Ingress对象，带有四个规则。

8: Ingress 115
1 apiVersion: networking.k8s.io/v1
2 kind: Ingress
3 metadata:
4 name: mcu-all
5 annotations:
6 nginx.ingress.kubernetes.io/rewrite-target: /
7 spec:
8 ingressClassName: nginx
9 rules:
10 - host: shield.mcu.com <<==== shield应用的主机规则
11 [http:](http:)
12 paths:
13 - path: /
14 pathType: Prefix
15 backend:
16 service:
17 name: svc-shield
18 port:
19 number: 8080
20 - host: hydra.mcu.com <<==== hydra应用的主机规则
21 [http:](http:)
22 paths:
23 - path: /
24 pathType: Prefix
25 backend:
26 service:
27 name: svc-hydra
28 port:
29 number: 8080
30 - host: mcu.com
31 [http:](http:)
32 paths:
33 - path: /shield <<==== shield应用的路径规则
34 pathType: Prefix
35 backend:
36 service:
37 name: svc-shield
38 port:
39 number: 8080
40 - path: /hydra <<==== hydra应用的路径规则
41 pathType: Prefix
42 backend:
43 service:
44 name: svc-hydra
45 port:
46 number: 8080

让我们一步一步来。

前两行指示Kubernetes根据**networking.k8s.io/v1** API中的方案部署一个Ingress对象。

```
8: Ingress 116
```

```
第四行为Ingress对象指定一个名称。
第六行的注释告诉控制器尽力重写路径，使其与应用期望的路径相匹配。此示例将传入的路径重写为“/”。例如，命中mcu.com/shield路径的流量将被重写为mcu.com/。您将很快看到一个示例。此注释专用于NGINX Ingress控制器，如果您使用其他控制器，您需要将其注释掉。
第八行的spec.ingressClassName字段告诉Kubernetes此Ingress对象专用于您之前安装的NGINX Ingress控制器。如果您使用其他Ingress控制器，您需要更改此行或将其注释掉。
该文件包含四条规则：
```

- 第10-19行定义了一个基于主机的规则，用于处理到达shield.mcu.com的流量
- 第20-29行定义了一个基于主机的规则，用于处理到达hydra.mcu.com的流量
- 第30-39行定义了一个基于路径的规则，用于处理到达mcu.com/shield的流量
- 第40-49行定义了一个基于路径的规则，用于处理到达mcu.com/hydra的流量

让我们先看一个基于主机的规则的示例，然后再看一个基于路径的规则。
以下是一个基于主机的规则示例，当流量通过shield.mcu.com到达根路径“/”时，它将被转发到监听8080端口的名为svc-shield的ClusterIP后端Service。

- host: shield.mcu.com <<==== 流量通过此主机名到达
  [http:](http:)
  paths:
  - path: / <<==== 到达根路径（未指定子路径）
  pathType: Prefix
  backend: <<==== 下面的五行引用了一个
  service: <<==== 现有的"backend" ClusterIP Service
  name: svc-shield <<==== 名为"svc-shield"的Service
  port: <<==== 该Service监听
  number: 8080 <<==== 的8080端口

```
以下是一个基于路径的规则示例，当流量到达mcu.com/shield时触发，它将被路由到同样端口上的svc-shield后端Service。
```

```
8: Ingress 117
```

- host: mcu.com <<==== 流量通过此主机名到达
  [http:](http:)
  paths:
  - path: /shield <<==== 到达此子路径
  pathType: Prefix
  backend: <<==== 下面的五行引用了一个
  service: <<==== 现有的"backend" ClusterIP Service
  name: svc-shield <<==== 名为"svc-shield"的Service
  port: <<==== 该Service监听
  number: 8080 <<==== 的8080端口

```
使用以下命令部署Ingress对象。
```

```
$ kubectl apply -f ig-all.yml
ingress.networking.k8s.io/mcu-all created
```

```
检查Ingress对象
```
“列出默认命名空间中的所有Ingress对象。如果您的集群在云上，获取地址可能需要一分钟左右的时间，因为云平台需要配置负载均衡器。”
```
$ kubectl get ing
名称 类别 主机 地址 端口
mcu-all nginx shield.mcu.com,hydra.mcu.com,mcu.com 212.2.246.150 80
```
“**类别**字段显示处理此规则集的Ingress类是哪个。如果您只有一个Ingress控制器并且没有配置类别，它可能显示为**<None>**。**主机**字段是Ingress将处理流量的主机名列表。**地址**字段是负载均衡器的端点。如果您在云上，它将是一个公共IP或公共DNS名称。如果您在本地集群上，它可能是localhost。**端口**字段可以是80或443。关于端口，Ingress仅支持HTTP和HTTPS。描述Ingress。输出被截断以适应页面。”
```
$ kubectl describe ing mcu-all
名称: mcu-all
命名空间: default
地址: 212.2.246.150
Ingress类: nginx
默认后端: <default>
规则:
主机 路径 后端

---- ---- --------

shield.mcu.com / svc-shield:8080 (10.36.1.5:8080)
hydra.mcu.com / svc-hydra:8080 (10.36.0.7:8080)
mcu.com /shield svc-shield:8080 (10.36.1.5:8080)
/hydra svc-hydra:8080 (10.36.0.7:8080)
注释: nginx.ingress.kubernetes.io/rewrite-target: /
事件: <none>
类型 原因 年龄 来自 消息

---- ------ ---- ---- -------

正常 同步 27s (x2 over 28s) nginx-ingress-controller 安排同步
```
让我们逐步了解输出内容。 **地址**行是Ingress创建的负载均衡器的IP或DNS名称。在本地集群上可能是localhost。**默认后端**是控制器将到达的主机名或路径上的流量发送到的位置，如果没有路由。并非所有的Ingress控制器都实现了默认后端。规则定义了_hosts_，_路径_和_后端_之间的映射关系。请记住，**后端**通常是发送流量到Pods的ClusterIP服务。您可以使用注释来定义控制器特定的功能和与云后端的集成。此示例告诉控制器将所有路径重写为看起来像它们是在根目录“/”上到达的。这是一种尽力而为的方法，正如您稍后将看到的，它对于所有应用程序都不起作用。此时，负载均衡器已创建。如果您在云平台上，您可能可以通过云控制台查看它。图8.3显示了如果您的集群在Google Kubernetes Engine（GKE）上，它在Google云后端上的外观。
Windows用户需要以管理员身份打开**notepad.exe**，并打开位于**C:\Windows\System32\drivers\etc**的**hosts**文件。确保打开对话窗口设置为打开**所有文件 (*.*)**。

```
$ sudo vi /etc/hosts
```

```
# Host Database
<Snip>
212.2.246.150 shield.mcu.com
212.2.246.150 hydra.mcu.com
212.2.246.150 mcu.com
```

```
记得保存您的更改。
```

完成后，您发送到shield.mcu.com、hydra.mcu.com或mcu.com的任何流量都将发送到入口负载均衡器。

```
测试入口
```

```
打开一个网络浏览器，尝试以下URL：
```

- shield.mcu.com
- hydra.mcu.com
- mcu.com

```
图8.4显示了整体架构和流量流向。流量命中在创建入口时自动创建的负载均衡器上。流量到达端口
80，并且入口根据头部中的主机名将其发送到内部的ClusterIP服务。shield.mcu.com的流量
进入svc-shield服务，hydra.mcu.com的流量
进入svc-hydra服务。
```

8: 入口 121

```
图8.4-基于主机的路由
```

请注意，对mcu.com的请求被路由到了_default backend_。这是因为您没有为mcu.com创建入口规则。根据您的入口控制器，返回的消息将不同，您的入口甚至可能不实现默认后端。GKE内置的入口配置了一个默认的后端，返回一个有用的消息，说“响应404（后端未找到），服务规则为[ / ]不存在”。

现在尝试连接以下任一：

- mcu.com/shield
- mcu.com/hydra

对于这样的基于路径的路由，入口使用了对象注释中指定的_rewrite targets_功能。但是，图片不会显示，因为这样的路径重写对于所有应用程序都不起作用。

恭喜您，成功配置了基于主机和基于路径的入口 - 您有两个应用程序通过两个ClusterIP服务进行前端，但都通过由Kubernetes Ingress创建和管理的单个负载均衡器暴露！

### 清理

如果您按照本章进行操作，您的集群上将拥有以下所有内容：

```
Pods 服务 Ingress控制器 Ingress资源
shield svc-shield ingress-nginx mcu-all
hydra svc-hydra
```

删除Ingress资源。

8: 入口 122

```
$ kubectl delete -f ig-all.yml
ingress.networking.k8s.io "mcu-all" deleted
```

删除Pods和ClusterIP服务。Pods可能需要几秒钟才能正常终止。

```
$ kubectl delete -f app.yml
service "svc-shield" deleted
service "svc-hydra" deleted
pod "shield" deleted
pod "hydra" deleted
```

删除NGINX Ingress控制器。

```
$ kubectl delete -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/
controller-v1.9.4/deploy/static/provider/cloud/deploy.yaml
namespace "ingress-nginx" deleted
serviceaccount "ingress-nginx" deleted
<Snip>
```

最后，**如果您之前添加了手动条目，请不要忘记恢复/etc/hosts文件**。

```
$ sudo vi /etc/hosts

# Host Database

<Snip>
212.2.246.150 shield.mcu.com <<==== 删除此条目
212.2.246.150 hydra.mcu.com <<==== 删除此条目
212.2.246.150 mcu.com <<==== 删除此条目
```

### 章节总结

在本章中，您了解到入口是通过单个云负载均衡器公开多个应用程序（ClusterIP服务）的一种方式。它们是API中的稳定对象，但与许多服务网格的功能重叠。如果您正在运行服务网格，您可能不需要入口。

许多Kubernetes集群要求您安装一个入口控制器，并且存在许多选择。然而，一些托管的Kubernetes服务通过内置的入口控制器使事情变得容易。

安装入口控制器后，您创建和部署入口对象，这是一系列规则，用于管理入站流量如何路由到集群上的应用程序。它支持基于主机和基于路径的HTTP路由。

## 9: 在Kubernetes上使用WebAssembly
WebAssembly（Wasm）正在推动一股新的云计算浪潮，而像Kubernetes和Docker这样的平台正在不断发展以利用它。虚拟机是第一波浪潮，容器是第二波，而Wasm是第三波。每一波浪潮都使得应用程序更小、更快、更便携，能够做到之前的波浪潮所不能做到的事情。

```
图9.1
```

```
本章内容如下：
```

- Wasm入门
- 在Kubernetes上理解Wasm
- 在Kubernetes上进行Wasm实践

```
术语方面的一点说明。
WebAssembly和Wasm这两个术语的意思是一样的，我们会交替使用。实际上，Wasm是WebAssembly的简称，不是首字母缩写。这意味着正确的写法是Wasm而不是WASM。然而，请对人们友善一点，不要因为他们犯了这样的无关紧要的错误而批评他们。
此外，本章重点介绍在Kubernetes上使用WebAssembly。这是“WebAssembly在浏览器之外”、“WebAssembly在服务器上”、“WebAssembly在云端”和“WebAssembly在边缘”的众多用例之一。
```

```
9：WebAssembly在Kubernetes上 124
```

### Wasm入门。

WebAssembly于2017年首次亮相，并立即因加速Web应用而闻名。快进7年，它成为了W3C的官方标准，所有主要浏览器都支持它，它成为了需要高性能而不牺牲安全性和可移植性的Web游戏和Web应用的首选解决方案。因此，对于云计算企业家来说，观察到WebAssembly的崛起并意识到它将成为云应用的重要技术并不令人意外。

```
事实上，WebAssembly非常适合云端，以至于Docker创始人Solomon Hykes曾经在推特上说：“如果Wasm+WASI在2008年就存在，我们就不需要创建Docker了。这就是它的重要性。WebAssembly在服务器上是计算的未来。标准化的系统接口是缺失的环节。让我们希望WASI能够胜任这个任务！”。他随即在另一条推文中表示，他期待着未来Linux容器和Wasm容器可以并存，而Docker可以与它们一起工作。撰写本文时，Solomon预测的未来已经到来。Docker对Wasm支持很好，已经可以在同一个Kubernetes Pod中同时运行Linux容器和Wasm容器。然而，WebAssembly的标准和生态系统仍然非常新，并且对于许多云应用和用例来说，传统的Linux容器仍然是最佳解决方案。
从技术上讲，Wasm是一种二进制指令集架构（ISA），类似于ARM、x86、MIPS和RISC-V。这意味着编程语言可以将源代码编译为Wasm二进制文件，在任何带有Wasm运行时的系统上运行。Wasm应用程序在一个默认拒绝的安全沙箱中执行，不信任应用程序，因此必须显式允许对所有资源的访问。这与容器相反，容器从一切开放开始。
WASI是WebAssembly系统接口，允许沙箱化的Wasm应用程序安全地访问外部服务，如键值存储、网络、主机环境等。WASI对于WebAssembly在浏览器之外的成功至关重要，撰写本文时，WASI预览版2正在开发中，预计将迈出重要一步。
让我们快速了解一下Wasm的安全性、可移植性和性能方面。
```

**Wasm的安全性**

Wasm从一开始就是全面封锁的，而容器则是一开始就是开放的。

```
说到容器的安全性，必须承认社区在保护容器和容器编排平台方面所做的不可思议的工作。现在在托管的Kubernetes平台上运行安全的容器化应用程序比以往更容易。
```

```
然而，基于默认允许模型的广泛访问共享内核将始终对容器的安全性构成挑战。
```

WebAssembly非常不同。Wasm应用程序在一个默认拒绝的沙箱中执行，运行时必须显式允许访问沙箱之外的任何内容。您还应该知道，这个沙箱经过多年在世界上最具敌意的环境之一（即Web）中运行不受信任的应用程序的实践而经受住了考验。

**Wasm的可移植性**

```
有一个普遍的误解，即容器是可移植的。事实并非如此！
```


我们认为容器是可移植的，因为它们比虚拟机更小，更容易在主机和注册表之间复制。然而，这并不是真正的可移植性。实际上，容器是与架构相关的，这意味着它们不可移植。例如，每个容器都是为单个操作系统和架构构建的。
尽管容器构建工具使为多个平台构建变得容易，但仍然会增加负担，许多组织最终会出现图像蔓延的情况。举个简单例子，我为本书中的大多数应用程序维护两个镜像 - 一个用于ARM上的Linux，另一个用于AMD64上的Linux。有时，我会更新一个应用程序并忘记构建Linux/amd64镜像。这会导致在Linux/amd64上运行Kubernetes的读者的示例失败。

WebAssembly解决了这个问题，并实现了“编译一次，到处运行”的承诺！

它通过实现自己的字节码格式来实现这一点，该格式需要运行时来执行。你只需一次将应用程序构建为Wasm二进制文件，然后任何支持Wasm运行时的主机都可以执行它。举个快速的例子，我在基于ARM的Mac上构建了本章的示例应用程序。然而，我将其编译为Wasm，这意味着它可以在任何具有Wasm运行时的主机上运行。在本章的后面部分，我们将在可能是您的笔记本电脑、数据中心或云中的Kubernetes集群上执行它。它还可以在Kubernetes支持的任何架构上运行。甚至为在物联网和边缘设备上找到的异构架构提供了Wasm运行时。
说到物联网设备，Wasm应用程序通常比Linux容器小得多。这意味着它们可以在资源受限的环境中运行，例如边缘计算和物联网，而容器无法做到这一点。

WebAssembly实现了“编译一次，到处运行”的承诺！

Wasm性能

一般而言，虚拟机需要几分钟才能启动，容器需要几秒钟，但Wasm可以实现亚秒级的启动时间。

事实上，Wasm的“冷启动”速度非常快，以至于不像冷启动。例如，Wasm应用程序通常在十毫秒或更短的时间内启动。通过适当的优化，一些应用程序甚至可以在微秒级启动！
这是具有重大影响力的变革，并推动了许多早期用例。例如，Wasm非常适用于事件驱动体系结构，如无服务器函数。它还使真正的零缩放成为可能。

快速回顾

与传统的Linux容器相比，WebAssembly应用程序更小、更快、更可移植和更安全。然而，这仍然是早期阶段，Wasm并不适用于所有情况。目前，Wasm对于事件处理程序和需要超快启动时间的任何应用都是一个很好的选择。它还非常适用于物联网、边缘计算以及构建扩展和插件。然而，在撰写本文时，对于需要网络、大量I/O和连接到其他服务的传统云应用程序来说，容器可能仍然是更好的选择。
尽管如此，Wasm正在快速发展，WASI预览2将加快进程！
既然我们对WebAssembly有了一些了解，我们来看看如何在Kubernetes上使用它。

了解在Kubernetes上的Wasm..

本节介绍在使用containerd的Kubernetes集群上运行Wasm应用程序的主要要求。还有其他运行Wasm应用程序的方法。本节只是一个概述，我们将在实践部分中详细介绍所有内容。
众所周知，Kubernetes是一个高级编排器，使用其他工具执行低级任务，如创建、启动和停止容器。最常见的配置是Kubernetes使用containerd来管理这些低级任务。
图9.2显示了Kubernetes将任务调度给运行containerd的工作节点。containerd指示runc构建容器并启动应用程序。容器创建后，runc退出，shim进程维持正在运行的容器与containerd之间的连接。

在这种架构中，containerd以下的所有内容对于Kubernetes来说都是隐藏的。这使得可以用Wasm运行时和Wasm shim替换runc和标准shim成为可能。图9.3显示了相同的环境，但增加了两个Wasm shim到节点中。


在这个示例中，所有的更改都在containerd下进行——节点上仍然存在一个未改变的containerd实例。这是一个完全支持的配置，我们将在实操部分进行部署。

值得注意的是，Wasm shim架构与runc shim架构不同。如图9.4所示，Wasm shim是一个包括shim代码和Wasm运行时代码的单个二进制文件。

```
Figure9.4
```

与containerd的接口代码始终是_runwasi_^9，但每个shim可以嵌入一个特定的Wasm运行时。例如，Spin shim嵌入了runwasi Rust库和Spin运行时代码。同样，Slight shim嵌入了runwasi和Slight运行时。在每个shim中，嵌入的Wasm运行时创建_Wasm host_并执行Wasm应用，而runwasi使containerd保持在循环中。

(^9) https://github.com/containerd/runwasi

```
9: WebAssembly on Kubernetes 129
```

```
关于shim的最后一件事。containerd规定所有的shim二进制文件的命名方式如下：
```

- 使用**containerd-shim-**前缀
- 指定运行时的名称
- 指定版本

例如，Spin shim的名称必须为**containerd-shim-spin-v1**。图9.5显示了一个运行不同shim的Kubernetes集群的两个节点。一个节点运行WasmEdge shim，另一个节点运行Spin shim。在这样的配置中，Kubernetes需要帮助将工作负载调度到具有正确shim的节点上。实现这一目标的方式是通过_node labels_和_RuntimeClass_对象的组合。图中的Node 2具有**spin=yes**标签，并存在一个RuntimeClass对象，该对象选择该标签并在**handler**属性中指定目标运行时。这确保任何引用此RuntimeClass的Pod将被调度到Node 2并使用Spin运行时。我们将在实操部分详细介绍所有这些内容。

```
Figure9.5
```

```
使用containerd在Kubernetes集群上部署Wasm应用的工作流程如下：
```

```
1.编写应用并将其编译为Wasm二进制文件
2.将Wasm二进制文件打包为OCI镜像并将其存储在OCI registry中
3.在至少一个集群节点上安装Wasm shim
4.创建一个指定Wasm shim的RuntimeClass
```

```
9: WebAssembly on Kubernetes 130
```

```
5.为Wasm应用创建一个Pod（使用步骤2中的Wasm镜像）
6.在Pod中引用RuntimeClass
7.将Pod部署到Kubernetes
```

当部署Pod时，将会发生以下事情：

```
1.Pod将被调度到与RuntimeClass中的节点选择器匹配的节点
2.节点上的kubelet将使用来自RuntimeClass的shim信息将工作传递给containerd
3.containerd将使用RuntimeClass中请求的shim启动应用
```

如果这听起来让人困惑，不要担心。我们即将详细介绍完整的工作流程。

### 在Kubernetes上使用Wasm的实操

```
在本节中，您将编写一个Wasm应用，并经历将其在多节点的Kubernetes集群上运行所需的所有步骤。
在现实世界中，云平台和其他工具将简化这个过程，也有其他运行Wasm应用的方式。然而，本节使您对所有涉及的内容有深入的了解，以便您在现实世界中部署和管理Wasm应用于Kubernetes时做好准备。
```

我们将完成以下所有操作：

```
1.构建一个简单的Web应用
2.将其编译为Wasm二进制文件
3.将其构建为OCI镜像
4.将其推送到OCI registry
5.构建一个多节点的Kubernetes集群
6.为Wasm配置集群
7.将应用部署到Kubernetes
```

```
如果您计划进行以下操作，请确保您拥有以下内容。如果没有，请安装：
```

- Docker Desktop 4.27.1或更高版本

```
9: WebAssembly on Kubernetes 131
```

- k3d 5.6.0或更高版本
- Rust 1.72或更高版本，安装了**wasm32-wasi**目标
- Spin 2.0或更高版本
第三章介绍了如何安装Docker Desktop和k3d。在后续的步骤中，您将构建一个新的k3d集群。
访问https://www.rust-lang.org/tools/install以安装Rust。
安装完Rust后，运行以下命令安装wasm32-wasi目标，以便Rust可以将代码编译为Wasm二进制文件。

```
$ rustup target add wasm32-wasi
info: 正在下载 'wasm32-wasi' 的 'rust-std' 组件
info: 正在为 'wasm32-wasi' 安装 'rust-std' 组件
```

Spin是一个流行的Wasm框架，包括Wasm运行时和用于构建和处理Wasm应用程序的工具。在网络上搜索“安装Fermyon Spin”，然后按照适用于您平台的安装说明进行操作。

我们将按照以下方式分解本章剩余的内容：

- 构建和准备Wasm应用程序
- 构建和配置用于Wasm的Kubernetes
- 部署和测试应用程序

**构建和准备Wasm应用程序**

如果您已经了解如何编译Wasm应用程序并将其打包为OCI镜像，可以跳过本节。然而，如果您对Wasm和容器还不熟悉，本节将教您关于构建Wasm应用程序和将其打包为容器镜像的重要基础知识。
从本书的GitHub存储库的wasm文件夹中运行以下所有命令。
运行以下spin new命令，并按照提示完成操作。这将为一个简单的Spin应用程序生成脚手架，该应用程序在/tkb路径上响应Web请求。TKB是The Kubernetes Book的缩写。

```
$ spin new tkb-wasm -t http-rust
描述 []：我的第一个Wasm应用程序
HTTP路径 [/...]：/tkb
```

您将会有一个名为tkb-wasm的新目录，其中包含构建和运行应用程序所需的一切。
进入tkb-wasm目录并列出其内容。如果您的系统没有安装tree命令，可以尝试运行ls -R或类似的Windows命令。

```
$ cd tkb-wasm
```

```
$ tree
├── Cargo.toml
├── spin.toml
└── src
└── lib.rs
```

```
2个目录，3个文件
```

我们只关注两个文件：

- **spin.toml** 告诉Spin如何执行构建和运行应用程序等操作
- **src/lib.rs** 是应用程序的源代码

编辑src/lib.rs文件，使其返回文本"The Kubernetes Book loves Wasm!"。仅更改片段中注释所示的行的文本。

```
use spin_sdk::http::{IntoResponse, Request};
<Snip>
fn handle_tkb_wasm(req: Request) -> anyhow::Result<impl IntoResponse> {
    println!("Handling request to {:?}", req.header("spin-full-url"));
    Ok(http::Response::builder()
        .status(200)
        .header("content-type", "text/plain")
        .body("The Kubernetes Book loves Wasm!")?) <<==== 仅更改此行
}
```

保存更改并运行spin build命令，将应用程序编译为Wasm二进制文件。在幕后，spin build会从Rust工具链中运行一个更复杂的cargo build命令。

```
$ spin build
正在使用 `cargo build --target wasm32-wasi --release` 构建组件 tkb-wasm
正在更新 crates.io 索引
正在更新 git 仓库 `https://github.com/fermyon/spin`
<Snip>
已完成所有Spin组件的构建
```

恭喜，您刚刚构建和编译了一个Wasm应用程序！应用程序二进制文件位于target/wasm32-wasi/release/文件夹中的tkb_wasm.wasm。它可以在任何具有Wasm运行时的机器上运行。在本章的后面部分，您将在带有spin Wasm运行时的Kubernetes节点上运行它。
现在，您已经编译了应用程序，下一步是创建一个Dockerfile，告诉Docker如何将其打包为OCI镜像，以便您可以将其存储在OCI注册表（如Docker Hub）中。
在tkb-wasm文件夹中创建一个新的Dockerfile，并使用以下内容。


```
FROM scratch
COPY /target/wasm32-wasi/release/tkb_wasm.wasm.
COPY spin.toml.
```
**限制**
请根据英文内容直接翻译，维持原有的格式，不省略任何信息。

**从头开始**
**FROM scratch**这行告诉Docker要以空的_scratch_镜像为基础创建新的镜像，而不是基于典型的Linux基础镜像。这样可以保持镜像的小巧，并在运行时构建一个最小化的容器。尽管Wasm应用程序不需要容器，但容器平台和工具如Docker和Kubernetes使用需要容器的工具。在运行时，Wasm应用程序和Wasm运行时将在一个基本上只包含命名空间和控制组的最小化容器内执行（没有文件系统等）。

第一个**COPY**指令将编译后的Wasm二进制文件复制到容器的根目录中。第二个指令将**spin.toml**文件复制到同样的根目录下。
**spin.toml**文件告诉spin运行时Wasm应用程序的位置和如何执行它。目前，它期望Wasm应用程序位于**target/wasm32-wasi/release**文件夹中，但Dockerfile将把它复制到容器的根目录中。这意味着您需要更新**spin.toml**文件，让它期望在**/**文件夹中找到它。

编辑**spin.toml**文件，并去掉**[component.tkb-wasm]**字段中的前导路径，使其看起来像这样。代码片段中的注释只是为了向您展示需要更改的行，不要将其包含在您的文件中。

9: 在Kubernetes上使用WebAssembly 134

$ vim spin.toml
<Snip>
[component.tkb-wasm]
source = "tkb_wasm.wasm" <<==== 删除该行中的前导路径
<Snip>

此时，您已经准备好了以下内容：

- 一个Wasm应用程序（Wasm二进制文件）
- 一个**spin.toml**文件，告诉spin Wasm运行时如何执行Wasm应用程序
- 一个**Dockerfile**，告诉Docker如何将Wasm应用程序构建为OCI镜像

运行以下命令将Wasm应用程序构建为OCI镜像。如果您计划在以后的步骤中将其推送到注册表中，需要在最后一行使用不同的镜像名称。

$ docker build \
--platform wasi/wasm \
--provenance=false \
-t nigelpoulton/k8sbook:wasm-0.1.

**--platform wasi/wasm**标志设置镜像的操作系统和架构。类似于**docker run**的工具可以在运行时读取这些属性，帮助它们创建容器并运行应用程序。

检查本地机器上是否存在该镜像。随时运行**docker inspect**命令来验证操作系统和架构属性。

$ docker images
REPOSITORY TAG IMAGE ID CREATED SIZE
nigelpoulton/k8sbook wasm-0.1 30ba15a926fe 2 mins ago 620kB

注意镜像的大小是多么小。类似的_hello world_ Linux容器通常有几兆字节的大小。

恭喜您，您已经创建了一个Wasm应用程序，并将其构建为可以推送到注册表的OCI镜像，以便Kubernetes在后续步骤中拉取它。您不必将镜像推送到注册表，因为后面有一个预先创建好的镜像可以使用。但是，如果您决定将其推送到注册表，您需要用前面步骤中创建的镜像标签替换该镜像标签。您还需要一个注册表账户来推送镜像。

```
9: 在Kubernetes上使用WebAssembly 135
```

```
$ docker push nigelpoulton/k8sbook:wasm-0.1
The push refers to repository [docker.io/nigelpoulton/k8sbook]
cdfbd289f3c8: Pushed
86896b1ae048: Pushed
wasm-0.1: digest: sha256:30ba15a926fef07bf9d8...b2608b2033f45ff5 size: 695
```

到目前为止，您已经编写了一个应用程序，将其编译为Wasm，打包为OCI镜像，并将其推送到注册表。接下来，我们将构建和配置一个可以运行Wasm应用程序的Kubernetes集群。

**构建和配置用于Wasm的Kubernetes**
这个部分要求您在笔记本电脑或其他本地机器上构建一个新的k3d Kubernetes集群。它基于一个特殊的k3d镜像，该镜像包含了其他k3d集群所不包含的预安装的Wasm组件。这意味着我们在第三章中展示给您的构建k3d集群对这些示例是无效的，如果您想跟随这些示例，您需要构建我们即将演示的集群。这是因为Wasm还非常新，不是所有的Kubernetes发行版都包含运行Wasm所需的组件。这将在未来发生变化。
您将在本节中完成以下所有步骤：

- 构建一个3节点的Kubernetes集群（一个控制平面节点和两个工作节点）
- 检查一个工作节点上的Wasm配置
- 为一个工作节点打上标签，以便调度程序知道它可以运行Wasm应用程序
- 创建一个RuntimeClass，以便调度程序将Wasm应用程序分配给该节点

运行以下命令来创建一个名为wasm的新k3d集群。这样做也会将您的kubectl上下文更改为新的集群。当您完成本章节后，您需要将其改回来。

```
$ k3d cluster create wasm \
--image ghcr.io/deislabs/containerd-wasm-shims/examples/k3d:v0.10.0 \
-p "5005:80@loadbalancer" --agents 2
```

第一行创建了一个名为wasm的新集群。
--image标志告诉k3d使用哪个镜像来构建控制平面节点和工作节点。这是一个包含containerd Wasm shims的特殊镜像。
-p标志创建一个负载均衡器，连接到集群上的一个入口，并将主机上的端口5005映射到集群内部端口80上的入口。
--agents 2标志创建两个工作节点。
集群运行起来后，您可以使用以下命令测试连接性。您应该会看到三个节点-一个控制平面节点和两个工作节点。

```
$ kubectl get nodes
NAME STATUS ROLES AGE VERSION
k3d-wasm-agent-0 Ready <none> 13s v1.27.4+k3s1
k3d-wasm-server-0 Ready control-plane 17s v1.27.4+k3s1
k3d-wasm-agent-1 Ready <none> 13s v1.27.4+k3s1
```

如果Kubernetes想要运行Wasm工作负载，至少需要一个同时满足以下两个条件的节点：

1. 运行中的containerd
2. 安装并注册了containerd的Wasm shim

在**k3d-wasm-agent-1**工作节点上执行以下命令，检查containerd是否正在运行。

```
$ docker exec -it k3d-wasm-agent-1 ash

$ ps | grep containerd
PID USER COMMAND
98 0 containerd
<Snip>
```

现在检查是否安装了任何Wasm shim。它们应该在/bin目录中，并按照_containerd shim命名约定进行命名，该约定在shim名称前加上containerd-shim-前缀，并在末尾需要一个版本号。以下输出显示了五个shim- runc-v2是执行Linux容器的默认shim，其他四个是Wasm shims。对于我们来说，重要的是名为containerd-shim-spin-v1的Spin shim。

```
$ ls /bin | grep shim
containerd-shim-lunatic-v1
containerd-shim-runc-v2
containerd-shim-slight-v1
containerd-shim-spin-v1
containerd-shim-wws-v1
```

拥有Wasm shim还不够，它们还需要在containerd中注册并作为containerd配置的一部分加载。

检查containerd配置文件（config.toml）中的Wasm shim条目。该文件通常存储在/etc/containerd中，但k3d目前将其存储在不同的位置。输出被修剪，只显示Wasm运行时。

```
$ cat /var/lib/rancher/k3s/agent/etc/containerd/config.toml
```

```
<Snip>
[plugins.cri.containerd.runtimes.spin]
runtime_type = "io.containerd.spin.v1"

[plugins.cri.containerd.runtimes.slight]
runtime_type = "io.containerd.slight.v1"
```
<限制> 
请根据英文内容直接翻译，维持原有的格式，不省略任何信息。

<翻译后的文章> 

```
[plugins.cri.containerd.runtimes.wws]
runtime_type = "io.containerd.wws.v1"
```

```
[plugins.cri.containerd.runtimes.lunatic]
runtime_type = "io.containerd.lunatic.v1"
```

```
您也可以运行以下命令来验证活动的containerd配置。该命令会解析输出以查找对Spin Wasm shim的引用。
```

```
$ containerd --config \
/var/lib/rancher/k3s/agent/etc/containerd/config.toml \
config dump | grep spin
```

```
<Snip>
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.spin]
runtime_type = "io.containerd.spin.v2"
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.spin.options]
```

```
您已确认containerd正在运行并且已经存在和注册了Wasm shims。这意味着该节点可以运行Wasm容器。
在示例k3d集群中，所有节点都运行相同的shims，这意味着每个节点都可以运行Wasm应用程序，无需进一步操作。然而，大多数现实世界的环境中，节点配置是异构的，不同的节点具有不同的shims和运行时。在这些情况下，您需要为节点打上标签并创建RuntimeClasses，以帮助Kubernetes将工作调度到正确的节点。

我们将使用"wasm=yes"标签为agent-1节点打上标签，并创建一个目标为带有该标签的节点的RuntimeClass。
运行以下命令将"wasm=yes"标签添加到agent-1工作节点上。您需要输入"exit"退出exec会话并返回到主机的终端。

```
$ kubectl label nodes k3d-wasm-agent-1 wasm=yes
node/k3d-wasm-agent-1 labeled
```

```
验证操作是否成功。您的输出可能包含更多的标签。
```

9: WebAssembly on Kubernetes 138

$ kubectl get nodes --show-labels | grep wasm=yes
NAME STATUS ROLES LABELS
k3d-wasm-agent-0 Ready <none> beta.kubernetes...,wasm=yes

运行以下命令创建名为"rc-spin"的RuntimeClass。

kubectl apply -f - <<EOF
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
name: rc-spin
scheduling:
nodeSelector:
wasm: "yes"
handler: spin
EOF

"scheduling.nodeSelector"字段确保使用此RuntimeClass的Pod仅在带有"wasm=yes"标签的节点上调度。"handler"字段告诉containerd使用"spin" shim来执行Wasm应用程序。

检查资源是否正确创建。

$ kubectl get runtimeclass
NAME HANDLER AGE
rc-spin spin 1m

此时，Kubernetes集群已具备运行Wasm工作负载所需的一切 - "agent-1"工作节点已经打上标签并安装了四个Wasm shims，并且存在一个RuntimeClass来将Wasm任务调度到该节点。

**部署和测试应用程序**

应用程序在本书的GitHub存储库的"wasm"文件夹中的"app.yml"文件中定义，包括一个Deployment、一个Service和一个Ingress。

https://github.com/nigelpoulton/TheK8sBook/tree/main/wasm/app.yml

Deployment YAML的重要部分是Pod规范中对RuntimeClass的引用。这将确保所有三个副本都被调度到满足RuntimeClass中的"nodeSelector"要求的节点（具有"wasm=yes"标签的节点）。在我们的示例中，所有三个副本都将被调度到agent-1节点。

9: WebAssembly on Kubernetes 139

apiVersion: apps/v1
kind: Deployment
metadata:
name: wasm-spin
spec:
replicas: 3
<Snip>
template:
metadata:
labels:
app: wasm
<Snip>
spec:
runtimeClassName: rc-spin <<==== 引用RuntimeClass
containers:

- name: testwasm
  image: nigelpoulton/k8sbook:wasm-0.1 <<==== 预先创建的镜像
  command: ["/"]

还有一个在YAML片段中未显示的Ingress和Service。Ingress将到达"/"路径的流量定向到名为"wasm-spin"的ClusterIP Service。然后，Service将流量转发到所有具有"app=wasm"标签的Pod的80端口。在Deployment中定义的副本都具有"app=wasm"标签。
交通流量如图9.6所示。

```
Figure9.6
```

下一步将使用书籍的GitHub仓库中的**app.yml**文件部署应用程序。
这个YAML文件使用了书籍的Docker Hub仓库中预先创建的Wasm镜像。如果您想使用之前步骤中创建的镜像，可以编辑本地的**app.yml**文件，更改**image**字段，并在下面的**kubectl apply**命令中引用本地**app.yml**。

```
9: 在Kubernetes上使用WebAssembly 140
```

```
$ kubectl apply \
-f https://raw.githubusercontent.com/nigelpoulton/TheK8sBook/main/wasm/app.yml
deployment.apps/wasm-spin created
service/svc-wasm created
ingress.networking.k8s.io/ing-wasm created
```

```
使用kubectl get deploy wasm-spin命令检查部署的状态。
```

等待三个副本都准备好后，运行以下命令确保它们都被调度到**agent-1**工作节点上。

```
$ kubectl get pods -o wide
NAME READY STATUS ... NODE
wasm-spin-5f6fccc557-5jzx6 1/1 Running ... k3d-wasm-agent-1
wasm-spin-5f6fccc557-c2tq7 1/1 Running ... k3d-wasm-agent-1
wasm-spin-5f6fccc557-ft6nz 1/1 Running ... k3d-wasm-agent-1
```

```
Kubernetes已将所有三个副本调度到了agent-1节点上。这意味着标签和RuntimeClass按预期工作。
使用以下curl命令测试应用程序。您也可以将浏览器指向http://localhost:5005/tkb。
```

```
$ curl http://localhost:5005/tkb
The Kubernetes Book loves Wasm!
```

```
恭喜，Wasm应用程序正在您的Kubernetes集群上运行！
```

**清理**

```
如果您按照操作进行，您将拥有以下所有可能希望清理的工件：
```

- 名为**wasm**的k3d Kubernetes集群
- 存储在OCI注册表中的Wasm OCI镜像
- 本地主机上的Wasm OCI镜像
- 本地机器上的Spin应用程序

```
清理Kubernetes集群的最简单方法是将其删除。如果您为这些练习构建了一个专用的k3d集群，可以使用此命令删除它。
```

```
9: 在Kubernetes上使用WebAssembly 141
```

```
$ k3d cluster delete wasm
```

```
如果您只想保留集群并删除资源，请运行以下两个命令。
```

```
$ kubectl delete \
-f https://raw.githubusercontent.com/nigelpoulton/TheK8sBook/main/wasm/app.yml
deployment.apps "wasm-spin" deleted
service "svc-wasm" deleted
ingress.networking.k8s.io "ing-wasm" deleted
```

```
$ kubectl delete runtimeclass rc-spin
runtimeclass.node.k8s.io "rc-spin" deleted
```

```
您可以使用以下命令删除本地机器上的Wasm镜像。请确保替换为您的镜像名称。
```

```
$ docker rmi nigelpoulton/k8sbook:wasm-0.1
```

当您使用**spin new**和**spin build**创建应用程序时，您会得到一个名为**tkb-wasm**的新目录，其中包含所有应用程序工件。使用您喜欢的工具删除该目录及其中的所有文件。**确保删除正确的目录！**
将您的Kubernetes上下文设置回本书中其他示例所使用的集群。如果您使用Docker Desktop，可以点击Docker鲸鱼图标，并从**Kubernetes上下文**选项中选择上下文。

### 章节总结。

Wasm正在推动云计算的第三波浪潮，而像Docker和Kubernetes这样的平台正在适应它。Docker已经可以将Wasm应用程序构建为容器镜像，使用**docker run**运行它们，并将它们托管在Docker Hub上。像containerd和runwasi这样的项目正在使在Kubernetes上运行Wasm容器变得容易。

Wasm是一种二进制指令集，编程语言将其作为编译目标，而不是编译为类似于_ARM上的Linux_之类的东西，您将其编译为_Wasm_。
编译的Wasm应用程序是可以在任何具有Wasm运行时的地方运行的小型二进制文件。Wasm应用程序比传统的Linux容器更小、更快、更便携和更安全。然而，在撰写本文时，Wasm应用程序还不能做到Linux容器所能做的一切。
高级过程是使用现有语言编写应用程序，将其编译为Wasm二进制文件，然后使用**docker build**和**docker push**等工具将其构建为OCI镜像并推送到OCI注册表中。从那里，它们可以被包装在Kubernetes Pod中，并像普通容器一样在Kubernetes集群上运行。

运行containerd的Kubernetes集群有越来越多的Wasm运行时选择，它们被实现为_containerd shims_。要在运行containerd的Kubernetes集群上运行Wasm应用程序，您需要在至少一个工作节点上安装和注册Wasm shim。然后，您需要为该节点添加标签，并在RuntimeClass中引用该标签，以便调度器可以将Wasm Pod分配给它。

还有其他在Kubernetes上运行Wasm应用程序的方法。一种替代方式是使用_crun_而不是containerd。如果您需要将Wasm支持添加到现有集群中，请参考**_Kwasm_**项目。

## 10：服务发现深入探讨

在本章中，您将学习服务发现的重要性以及在Kubernetes中如何实现它。您还将学习一些故障排除技巧。

如果您了解Kubernetes服务的工作原理，您将从本章中获得最大收益。如果您还不了解这一点，请先阅读第7章。

本章分为以下几个部分：

- 背景设定
- 服务注册表
- 服务注册
- 服务发现
- 服务发现和命名空间
- 服务发现故障排除

```
注意：单词"service"有很多不同的含义。当我们提到Kubernetes API中的Service资源时，我们将首字母大写以提高清晰度。
```

### 背景设定

在繁忙的平台上，如Kubernetes上找到东西很困难，而服务发现使其变得容易。

大多数Kubernetes集群运行着数百个甚至数千个微服务应用程序。每个应用程序都位于其自己的Service后面，以获得可靠的名称和IP。当一个应用程序与另一个应用程序通信时，它实际上是与其前面的Service进行通信。在本章的其余部分，每当我们说一个应用程序需要找到或与另一个应用程序通信时，我们指的是它需要找到或与其前面的**_Service_**通信。

图10.1显示了**app-a**通过其Service对象与**app-b**进行通信。

10:服务发现深入探讨

```
Figure10.1-AppsconnectviaServices
```

应用程序需要两件事才能向其他应用程序发送请求：

```
1.了解其他应用程序的名称（其Service的名称）
2.将名称转换为IP地址的方法
```

开发人员负责第1步-确保应用程序知道它们使用的其他应用程序和微服务的名称。Kubernetes负责第2步-将名称转换为IP地址。

图10.2是整个过程的高级视图，包括四个主要步骤：

- **步骤1：**开发人员配置**app-a**以与**app-b**通信
- **步骤2：app-a**向Kubernetes请求**app-b**的IP地址
- **步骤3：**Kubernetes返回IP地址
- **步骤4：app-a**向**app-b**的IP地址发送请求

10:服务发现深入探讨

```
Figure10.2
```

第1步是唯一的手动步骤。Kubernetes会自动处理步骤2、3和4。

让我们更详细地了解一下。

### 服务注册表

服务注册表的工作是维护一个Service名称及其关联IP地址的列表。应用程序使用它将Service名称转换为IP地址。

每个Kubernetes集群都有一个内置的集群DNS，它作为其服务注册表使用。它是一个运行在每个Kubernetes集群的控制平面上的Kubernetes本机应用程序，由部署管理的两个或更多Pod组成，并由一个Service进行前端。Deployment通常称为**coredns**或**kube-dns**，而Service始终称为**kube-dns**。

图10.3显示了服务注册表的架构。它还显示了一个Service注册其名称和IP以及两个使用它进行服务发现的容器。正如您稍后将了解到的，_服务注册_和_服务发现_都是自动完成的。

10:服务发现深入探讨
```
图10.3-ClusterDNS架构
```

以下命令显示了组成集群DNS（服务注册表）的Pods、Deployment和Service。它们与图10.3中的内容相匹配，您可以在您的集群上运行这些命令。

这个命令列出了运行集群DNS的Pods。通常情况下，每个Pod使用**registry.k8s.io/coredns/coredns**镜像来提供集群DNS服务。GKE和其他一些集群使用不同的镜像，将Pods和Deployment称为**kube-dns**而不是**coredns**。

$ kubectl get pods -n kube-system -l k8s-app=kube-dns
名称  准备状态  运行状态  重启次数  年龄
coredns-76f75df574-d6nn5 1/1 运行中 0 13天
coredns-76f75df574-n7qzk 1/1 运行中 0 13天

下一个命令显示了管理Pods的Deployment。它确保集群DNS Pods的数量始终正确。

$ kubectl get deploy -n kube-system -l k8s-app=kube-dns
名称  准备状态  最新更新  可用状态  年龄
coredns 2/2 2 2 13天

以下命令显示了集群DNS Pods前面的Service。它始终被称为**kube-dns**，但在每个集群上都有不同的IP。正如您后面将了解到的，Kubernetes会自动为每个容器配置使用该IP进行服务发现。

10: 服务发现深入剖析 147

$ kubectl get svc -n kube-system -l k8s-app=kube-dns
名称  类型  集群IP 外部IP 端口  年龄
kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP,9153/TCP 13天

总之，每个Kubernetes集群都运行一个内部集群DNS服务，用作服务注册表。它映射了每个Service的名称和IP，并作为一组由Deployment管理的Pods在控制平面上运行，并由一个Service前端。

让我们将重点转移到服务注册上。

### 服务注册。

在Kubernetes上，关于服务注册最重要的一点是它是自动完成的！

在高层次上，您开发应用程序并将其放置在服务后面，以获得可靠的名称和IP。Kubernetes会自动将这些服务名称和IP注册到服务注册表中。

从现在开始，我们将把服务注册表称为集群DNS。

服务注册有三个步骤：

```
1.为服务分配一个名称
2.为服务分配一个IP
3.将名称和IP注册到集群DNS
```

开发人员负责第一步，而Kubernetes负责第二步和第三步。

考虑一个快速的例子。

您正在开发一个新的Web应用程序，其他应用程序将使用**valkyrie-web**名称连接到它。为了实现这一点，**您**将其放置在一个名为**valkyrie-web**的服务后面，并将其发布到API服务器。**Kubernetes**确保服务名称是唯一的，并自动为其分配一个IP地址（ClusterIP）。它还会自动在集群DNS中注册名称和IP。

注册过程是自动的，因为集群DNS是一个_Kubernetes本地应用程序_，它监视API服务器以获取新的服务。每当它看到一个新的服务时，它会获取其名称和IP，并自动注册它。这意味着应用程序不需要任何服务注册逻辑 - 您将它们放置在服务后面，集群DNS会自动注册它们。

图10.4总结了服务注册流程，并添加了第7章中的一些细节。

10: 服务发现深入剖析 148

```
图10.4-服务注册流程
```

让我们逐步解释这个图。

您将一个新的服务资源清单发布到API服务器，它经过身份验证和授权。Kubernetes为它分配一个ClusterIP，并将其配置持久化到集群存储中。集群DNS观察到新的服务并注册相应的DNS A和SRV记录。相关的EndpointSlice对象被创建用于保存与服务的标签选择器匹配的健康Pod IP列表。每个节点都运行一个kube-proxy，它观察新的对象并创建本地路由规则，以便将对服务的ClusterIP的请求路由到Pods。

总之，每个应用程序都位于一个服务后面，拥有可靠的名称和IP。集群DNS会监视集群中的新服务对象，并自动注册它们的名称和IP。

我们来看一下服务发现。

### 服务发现。。
应用程序使用名称与其他应用程序进行通信。然而，它们需要将这些名称转换为IP地址，这就是服务发现发挥作用的地方。

假设您有一个包含两个名为**enterprise**和**cerritos**的应用程序的集群。**enterprise**应用程序位于名为**ent**的ClusterIP服务后面，而**cerritos**应用程序位于名为**cer**的ClusterIP服务后面。Kubernetes已经为这两个服务分配了ClusterIP，并且集群DNS已经自动注册了它们。目前的情况如下所示。

```
应用程序 服务名称 ClusterIP
Enterprise ent 192.168.201.240
Cerritos cer 192.168.200.217
```

```
10: 服务发现深入研究 149
```

```
图10.5
```

如果其中一个应用程序想要连接到另一个应用程序，它需要知道应用程序的名称以及如何将其转换为IP地址。
开发人员负责编写使用所消耗应用程序的名称的应用程序，但是Kubernetes会自动将名称转换为IP地址。
考虑一个快速示例，图10.5中的enterprise应用程序需要向cerritos应用程序发送请求。为了使其工作，enterprise应用程序开发人员需要将其配置为在cerritos应用程序之前使用服务的名称。假设他们已经这样做了，enterprise应用程序将发送请求给**cer**。然而，它需要一种将**cer**转换为IP地址的方法。幸运的是，Kubernetes会配置每个容器以向集群DNS请求将名称转换为IP地址。这意味着托管enterprise应用程序实例的容器将**cer**名称发送给集群DNS，并且集群DNS将返回ClusterIP。然后应用程序将请求发送给该IP地址。
正如前面提到的，Kubernetes会配置每个容器使用集群DNS进行服务发现。这是通过自动配置每个容器的**/etc/resolv.conf**文件以使用集群DNS服务的IP地址来完成的。它还添加了搜索域以附加到不完整的名称。
一个_不完整的名称_是一个短名称，例如**ent**。附加搜索域将其转换为完整的域名（FQDN），例如ent.default.svc.cluster.local。
以下是从配置为向集群DNS发送服务发现请求（DNS查询）的容器的**/etc/resolv.conf**文件中提取的内容。它还列出了三个搜索域以附加到不完整的名称。

```
$ cat /etc/resolv.conf
search default.svc.cluster.local svc.cluster.local cluster.local
nameserver 10.96.0.10 <<==== 内部集群DNS的ClusterIP
options ndots:5
```

```
10: 服务发现深入研究 150
```

```
下面的命令证明了前面的/etc/resolv.conf文件中的nameserver IP与集群DNS（kube-dns服务）的IP地址匹配。
```

```
$ kubectl get svc -n kube-system -l k8s-app=kube-dns
名称 类型 ClusterIP 端口 年龄
kube-dns ClusterIP 10.96.0.10 53/UDP,53/TCP,9153/TCP 13天
```

```
现在您已经了解了基本知识，让我们看看图10.5中的enterprise应用程序如何向cerritos应用程序发送请求。
首先，enterprise应用程序需要知道cer服务作为cerritos应用程序的前端的名称。这是enterprise应用程序开发人员的工作。假设它知道该名称，它将发送请求给cer。应用程序的容器的网络堆栈会自动将该名称发送给集群DNS，请求关联的IP。集群DNS会响应cer服务的ClusterIP，并且应用程序将请求发送给该IP。然而，ClusterIP是虚拟IP，需要额外的操作确保请求最终到达cerritos的Pods。
```

**ClusterIP路由**

```
ClusterIP位于名为服务网络的特殊网络中，没有路由到它！这意味着每个容器都将ClusterIP流量发送到其默认网关。
```

```
术语：当系统没有路由时，默认网关是系统发送网络流量的位置。默认网关然后将流量转发到另一个设备，希望下一个设备能够将流量路由至目标。
```
容器的默认网关将流量发送到运行它的节点。节点对服务网络也没有路由，因此将流量发送到自己的默认网关。这将导致节点的内核处理流量，这就是魔术发生的地方...
每个Kubernetes节点都运行一个名为kube-proxy的系统服务，该服务通过监视API服务器上的新服务和EndpointSlice对象来实现控制器功能。每当它看到它们时，它会在内核中创建规则，以拦截ClusterIP流量并将其转发到单个Pod IP。
这意味着每当节点的内核处理ClusterIP的流量时，它会将其重定向到与服务的标签选择器匹配的健康Pod的IP。

**服务发现概述**

```
让我们通过图10.6中的流程图快速总结服务发现过程。
```

```
10: 服务发现深入探讨 151
```

```
图10.6
```

企业应用程序将请求发送到cer服务。容器通过将此名称发送到其/etc/resolv.conf文件配置的集群DNS地址来将其转换为IP地址。集群DNS响应IP，容器将流量发送到该IP。然而，ClusterIP位于服务网络上，容器没有到达它的路由。因此，它将其发送到其默认网关，后者将其转发到运行它的节点。节点也没有路由，因此它将其发送到自己的默认网关。这导致节点的内核处理请求并将其重定向到与服务的标签选择器匹配的Pod的IP地址。

### 服务发现和命名空间

每个Kubernetes对象在集群地址空间中都有一个名称，并且您可以使用命名空间来分隔地址空间。集群地址空间是一个我们通常称之为集群域的DNS域。在大多数集群中，它是cluster.local，并且对象名称必须在其中是唯一的。例如，在默认命名空间中只能有一个名为cer的服务，它将被称为cer.default.svc.cluster.local。这样的长名称被称为完全限定域名（FQDN），其格式为<object-name>.<namespace>.svc.cluster.local。

```
您可以使用命名空间将集群域下的地址空间进行分隔。例如，如果您的集群有两个名为dev和prod的命名空间，则地址空间将
```

10: 服务发现深入探讨 152

被分隔如下：

- **dev：** <service-name>.dev.svc.cluster.local
- **prod：** <service-name>.prod.svc.cluster.local

对象名称在命名空间内必须是唯一的，但在命名空间之间不必是唯一的。作为一个快速示例，图10.7显示了一个被划分为两个名为dev和prod的命名空间的单个集群。两个命名空间都有相同的cer服务实例。这使得命名空间成为在同一个集群上运行并行开发和生产配置的好工具。

```
图10.7
```

应用程序可以使用ent和cer等简短的名称连接到本地命名空间中的服务，但是它们需要使用完全限定的域名来连接远程命名空间中的服务。

让我们通过一个快速示例进行演示。

**服务发现示例**

以下YAML来自本书的GitHub存储库中的service-discovery文件夹中的sd-example.yml文件。

10: 服务发现深入探讨 153

该文件定义了两个命名空间、两个部署、两个服务和一个单独的跳板Pod。部署和服务具有相同的名称，因为它们位于不同的命名空间中。跳板Pod仅部署到dev命名空间。书中的示例已经剪裁。

```
图10.8
```

apiVersion: v1
kind: Namespace
metadata:
name: dev
---

apiVersion: v1
kind: Namespace
metadata:
name: prod
---

apiVersion: apps/v1
kind: Deployment
metadata:
name: enterprise
namespace: dev
spec:
replicas: 2
template:
spec:
containers:

- image: nigelpoulton/k8sbook:text-dev
  name: enterprise-ctr
  ports:
    - containerPort: 8080

---

apiVersion: apps/v1

10: 服务发现深入探讨 154

kind: Deployment
metadata:
name: enterprise
namespace: prod
spec:
replicas: 2
template:
spec:
containers:
- image: nigelpoulton/k8sbook:text-prod
  name: enterprise-ctr
  ports:
    - containerPort: 8080

---

apiVersion: v1
kind: Service
metadata:
name: ent
namespace: dev
spec:
selector:
app: enterprise
ports:

- port: 8080
  type: ClusterIP

---

apiVersion: v1
kind: Service
metadata:
name: ent
namespace: prod
spec:
selector:
app: enterprise
ports:

- port: 8080
  type: ClusterIP

---

apiVersion: v1
kind: Pod
metadata:
name: jump
namespace: dev
spec:
terminationGracePeriodSeconds: 5
containers:

- name: jump
  image: ubuntu
  tty: true
  stdin: true

10: Service discovery deep dive 155

运行以下命令来部署所有内容。您需要从**service-discovery**目录中运行该命令。

$ kubectl apply -f sd-example.yml
namespace/dev created
namespace/prod created
deployment.apps/enterprise created
deployment.apps/enterprise created
service/ent created
service/ent created
pod/jump-pod created

检查所有内容是否正确部署。输出已被修剪以适应本书，并未显示所有对象。

$ kubectl get all --namespace dev
NAME READY UP-TO-DATE AVAILABLE AGE
deployment.apps/enterprise 2/2 2 2 51s

NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
service/ent ClusterIP 10.96.138.186 <none> 8080/TCP 51s
<Snip>

$ kubectl get all --namespace prod
NAME READY UP-TO-DATE AVAILABLE AGE
deployment.apps/enterprise 2/2 2 2 1m24s

NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
service/ent ClusterIP 10.96.147.32 <none> 8080/TCP 1m25s
<snip>

您有两个命名空间，一个叫**dev**，另一个叫**prod**，每个命名空间都有一个**enterprise**应用的实例和一个**ent**服务的实例。**dev**命名空间还有一个独立的名为**jump**的Pod。

让我们看看在命名空间内和跨命名空间内服务发现是如何工作的。

您将执行以下所有操作：

```
1.登录到dev命名空间中的jump Pod
2.检查其/etc/resolv.conf文件
3.连接到本地dev命名空间中的ent服务的实例
4.连接到远程prod命名空间中的ent服务的实例
```

每个命名空间中的应用版本返回不同的消息，以确保您已连接到正确的实例。

打开一个交互式的exec会话，连接到jump Pod中的容器。您的终端提示符将更改以指示您已连接到容器。

10: Service discovery deep dive 156

$ kubectl exec -it jump --namespace dev -- bash
root@jump:/#

检查容器的**/etc/resolv.conf**文件的内容。它应该包含您集群的**kube-dns**服务的IP地址，以及**dev**命名空间（dev.svc.cluster.local）的搜索域。

# cat /etc/resolv.conf

search dev.svc.cluster.local svc.cluster.local cluster.local
nameserver 10.96.0.10
options ndots:5

安装**curl**实用程序。

# apt-get update && apt-get install curl -y

<snip>

运行以下**curl**命令来连接本地**dev**命名空间中的**ent**服务的8080端口。

# curl ent:8080

来自DEV命名空间的问候！
主机名：enterprise-76fc64bd9-lvzsn

“来自DEV命名空间的问候！”的响应证明连接已达到**dev**命名空间中的实例。

容器自动将dev.svc.cluster.local附加到名称并将查询发送到**/etc/resolv.conf**文件中指定的集群DNS。集群DNS返回本地**dev**命名空间中**ent**服务的ClusterIP，应用程序将流量发送到该IP地址。在流量到达节点的默认网关之前，该流量在节点的内核中引发一个陷阱，并被重定向到托管应用程序的Pod。

运行另一个**curl**命令，但这次附加**prod**命名空间的域名。这将导致集群DNS返回**prod**命名空间中服务的ClusterIP。
# curl ent.prod.svc.cluster.local:8080

来自PROD命名空间的问候！
主机名：enterprise-5cfcd578d7-nvzlp

这次，响应来自**prod**命名空间中的Pod。

测试证明，Kubernetes会自动将短名称解析到本地命名空间，并且您需要指定FQDN才能跨命名空间连接。

输入**exit**以从跳转Pod分离终端。

```
10: 服务发现深入解析 157
```

### 服务发现故障排除。

```
Kubernetes使服务注册和服务发现自动化。但是，背后发生了很多事情，了解如何检查和重启是有帮助的。
如前所述，Kubernetes使用集群DNS作为其内置服务注册表。这是作为一个或多个托管Pod运行的，Service对象提供了一个稳定的终端。重要组件有：
```

- **Pods**：由**coredns**部署管理
- **Service**：名为**kube-dns**的ClusterIP服务，监听端口53 TCP/UDP
- **EndpointSlice对象**：以**kube-dns**为前缀的名称

```
所有这些对象都位于kube-system命名空间中，并使用k8s-app=kube-dns标签进行标记，以便您更容易找到它们。
检查coredns部署及其Pod是否正在运行。
```

```
$ kubectl get deploy -n kube-system -l k8s-app=kube-dns
名称                READY   UP-TO-DATE   AVAILABLE   AGE
coredns            2/2     2            2           14d
```

```
$ kubectl get pods -n kube-system -l k8s-app=kube-dns
名称                          READY   状态     重启次数   AGE
coredns-76f75df574-6q7k7     1/1     运行中   0          14d
coredns-76f75df574-krnr7     1/1     运行中   0          14d
```

```
检查每个coredns Pod的日志。以下输出是正常工作的DNS Pod的典型输出。您需要使用您环境中的一个Pod的名称。
```

```
$ kubectl logs coredns-76f75df574-n7qzk -n kube-system
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12b...
CoreDNS-1.11.1
linux/arm64, go1.20.7, ae2bbc2
```

现在检查Service和EndpointSlice对象。输出应显示服务正在运行，ClusterIP字段中有一个IP地址，并且正在监听端口53 TCP/UDP。
**kube-dns**服务的ClusterIP地址必须与集群上所有容器的**/etc/resolv.conf**文件中的IP地址匹配。如果不匹配，容器将发送DNS请求到错误的位置。

10: 服务发现深入解析 158

$ kubectl get svc kube-dns -n kube-system
名称      类型        CLUSTER-IP   EXTERNAL-IP   端口(S)                AGE
kube-dns   ClusterIP   10.96.0.10   <none>           53/UDP,53/TCP,9153/TCP   14d

相关的**kube-dns** EndpointSlice对象也应该正常运行，并且具有在端口53上监听的**coredns** Pods的IP地址。

$ kubectl get endpointslice -n kube-system -l k8s-app=kube-dns
名称                地址类型   端口      终端         AGE
kube-dns-jb72g   IPv4      9153,53,53   10.244.1.9,10.244.1.14   14d

一旦您验证了基本的DNS组件正常运行，您就可以进行更详细和深入的故障排除。以下是一些简单的提示。

使用安装了您喜欢的网络工具（ping、traceroute、curl、dig、nslookup等）的故障排除Pod。如果没有自己的自定义镜像，**registry.k8s.io/e2e-test-images/jessie-dnsutils**镜像是一个受欢迎的选择。您可以访问explore.ggcr.dev来浏览registry.k8s.io/e2e-test-images仓库以获取更新的版本。

以下命令启动一个名为**dnsutils**的新的独立Pod，并连接到您的终端。它基于刚提到的镜像，可能需要几秒钟启动。

$ kubectl run -it dnsutils \
--image registry.k8s.io/e2e-test-images/jessie-dnsutils:1.7
检测集群DNS是否工作的常见方法是使用nslookup命令解析kubernetes服务。这个命令在每个集群上运行，并将API服务器暴露给所有Pods。查询应返回一个IP地址和名为kubernetes.default.svc.cluster.local的名称。

# nslookup kubernetes

服务器: 10.96.0.10
地址: 10.96.0.10#53
名称: kubernetes.default.svc.cluster.local
地址: 10.96.0.1

前两行应显示您的集群DNS的IP地址。后两行应显示kubernetes服务和其ClusterIP的完全限定域名(FQDN)。您可以通过运行kubectl get svc kubernetes命令来验证kubernetes服务的ClusterIP。

诸如"nslookup:无法解析kubernetes"的错误表明DNS不工作。可能的解决方法是删除coredns Pods。这将导致coredns Deployment重新创建它们。

以下命令删除DNS Pods。如果您仍然登录到dnsutils Pod，请在运行命令之前键入exit断开连接。

10：服务发现深入研究

$ kubectl delete pod -n kube-system -l k8s-app=kube-dns
pod "coredns-76f75df574-d6nn5"已删除
pod "coredns-76f75df574-n7qzk"已删除

运行kubectl get pods -n kube-system -l k8s-app=kube-dns验证它们是否已重新启动，然后再次测试DNS。

### 清理

运行以下命令进行清理。

$ kubectl delete pod dnsutils

$ kubectl delete -f sd-example.yml

### 章节总结。

在本章中，您了解到Kubernetes使用内部集群DNS进行服务注册和服务发现。它是一个Kubernetes原生应用程序，用于监视新创建的Service对象并自动注册它们的名称和IP。每个节点上的kubelet还会配置所有容器以使用集群DNS进行服务发现。

集群DNS将Service名称解析为ClusterIP。这些是稳定的虚拟IP，在一个名为service network的特殊网络上。对于这个网络没有路由，但kube-proxy会配置所有集群节点将ClusterIP流量重定向到Pod网络上的Pod IP。

11：Kubernetes存储

存储和检索数据对于大多数实际业务应用程序至关重要。幸运的是，Kubernetes的持久卷子系统允许您连接提供高级数据管理服务的企业级存储系统，例如备份和恢复、复制、快照等。

本章分为以下几个部分：

- 整体概述
- 存储提供商
- 容器存储接口(CSI)
- Kubernetes持久卷子系统
- 使用存储类进行动态配置
- 实践操作

Kubernetes支持各种外部存储系统，包括来自EMC、NetApp和所有主要云提供商的企业级存储系统。本章后面的实践示例是针对Google Kubernetes Engine (GKE)上的Kubernetes集群设计的，不适用于其他平台。然而，这些原则和工作流适用于大多数Kubernetes环境。

### 整体概述。

Kubernetes支持许多类型的存储，来自许多不同的提供商。这些包括来自云或您的本地数据中心的各种外部系统的块、文件和对象存储。

图11.1显示了高级架构。

11：Kubernetes存储

```
图11.1
```

左侧是存储提供商。如前所述，这些是提供高级存储服务的外部系统，可以是EMC和NetApp等本地系统，也可以是您的云提供的存储服务。

图表中间是插件层。这是将外部存储系统与Kubernetes连接的接口。现代插件使用容器存储接口(CSI)，这是一个面向容器编排器(如Kubernetes)的行业标准存储接口。如果您是编写存储插件的开发人员，CSI将抽象出Kubernetes内部机制，允许您开发"out-of-tree"存储插件。
<限制>
请根据英文内容直接翻译，维持原有的格式，不省略任何信息。

<翻译后的中文>
CSI之前，我们必须将所有存储插件作为Kubernetes主代码树的一部分进行开发（称为内部插件）。这强制将它们开源，并将所有更新和错误修复与Kubernetes发布周期绑定在一起。这对于插件开发人员和Kubernetes维护人员来说是一个问题。幸运的是，现在我们有了CSI，插件开发人员不再需要开源他们的代码，并且可以根据需要发布更新和错误修复。

图11.1的右侧是Kubernetes持久卷子系统。这是一组标准化的API对象，使在Kubernetes上运行的应用程序能够轻松使用存储。存储相关的API对象越来越多，但核心对象有：

- 持久卷（PV）
- 持久卷声明（PVC）
- 存储类（SC）

在整个章节中，我们将用它们的PascalCase缩写名称来指代它们——_PersistentVolume，PersistentVolumeClaim_和_StorageClass_。我们也会使用它们的简称PV，PVC和SC。

PV映射到外部卷，PVC授予对PV的访问权限，SC使其全部自动化和动态化。

11: Kubernetes存储162

考虑图11.2所示的快速示例和工作流程。

```
Figure11.2-Volumeprovisioningworkflow
```

```
1. Pod需要一个卷，并通过PersistentVolumeClaim请求它
2. PVC请求StorageClass在AWS后端创建一个新的PV和关联的卷
3. SC通过CSI插件调用AWS后端
4. CSI插件在AWS上创建设备（50GB EBS卷）
5. CSI插件将外部卷的创建情况报告给SC
6. SC创建PV并将其映射到AWS后端的EBS卷
7. Pod挂载PV并使用它
```

在深入探讨之前，值得注意的是，Kubernetes有机制防止多个Pod向同一个PV写入。它还强制外部卷和PV之间的1:1映射关系——不能将一个50GB的外部卷映射到2个25GB的PV。

让我们再深入一些。

### 存储提供者

如前所述，Kubernetes允许您使用来自各种外部系统的存储。我们通常称之为提供者或预配器。

每个提供者都提供自己的CSI插件，并具有独特的功能和配置选项。

11: Kubernetes存储163

提供者通常通过Helm图表或YAML安装程序分发插件。一旦安装，插件将作为一组Pod在kube-system命名空间中运行，您有责任阅读插件的文档并正确配置它。

一些明显的限制适用。例如，如果您的集群位于Microsoft Azure上，您不能预配和挂载AWS EBS卷。地域限制也可能适用。例如，Pod通常必须位于与存储后端相同的区域或区域中。

### 容器存储接口（CSI）

CSI是一个开源项目，定义了一个行业标准接口，以便容器编排器可以以统一的方式利用外部存储资源。例如，它为存储提供者提供了一个文档化的接口。这也意味着支持CSI的任何编排平台上应该可以使用CSI插件。

您可以在以下存储库中找到相对最新的CSI插件列表。该存储库将插件称为驱动程序。

- https://kubernetes-csi.github.io/docs/drivers.html

大多数云平台预安装了云原生存储服务的CSI插件。对于第三方存储系统，您将需要手动安装插件，但大多数都可以通过提供者的Helm图表安装或通过YAML文件安装。一旦安装，CSI插件通常作为一组Pod在kube-system命名空间中运行。

### Kubernetes持久卷子系统

持久卷子系统是一组API对象，允许应用程序请求和访问存储。它具有以下我们将查看和使用的资源：

- 持久卷（PV）
- 持久卷声明（PVC）
- 存储类（SC）

如前所述，PV表示Kubernetes上的外部卷。PVC授予应用程序对PV的访问权限。SC允许应用程序动态创建PV。

让我们再走一遍示例。

假设您有一个具有以下存储层级的外部存储系统：

11: Kubernetes存储164

- 快速存储（Flash/SSD）
- 慢速存储（Mechanical）

您希望您的应用程序使用这两种类型，因此为每种类型创建了一个存储类。
```
您需要部署一个需要100GB快速存储的新应用程序。为了实现这一目标，您创建了一个定义了Pod和PVC的YAML文件。Pod通过PVC请求一个卷，而PVC根据**sc-fast** SC定义了一个100GB的卷。

您通过将YAML文件发送到API服务器来部署该应用程序。SC控制器观察到新的PVC，并指示CSI插件在外部存储系统上提供一个100GB SSD卷。外部系统创建卷并向CSI插件报告，然后CSI插件通知SC控制器并将其映射到PV。Pod可以挂载PV并使用它。

如果您现在不理解所有内容也没关系，实践示例将澄清一切。

### 使用存储类进行动态配置...

存储类是**storage.k8s.io/v1** API组中的资源。资源类型是**StorageClass**，您可以在常规的YAML文件中定义它们。在使用**kubectl**时，可以使用**sc**简写。

```
注意：您可以运行kubectl api-resources命令查看API资源和它们的简写的完整列表。它还显示每个资源的API组以及其等效的kind。
```

正如名称所示，存储类让您定义应用程序可以请求的不同类别的存储。您如何定义存储类取决于您拥有的存储类型。例如，如果您有一个具有快速存储和慢速存储以及可选的远程复制的存储系统，您可以定义以下四个类别：

- fast-local
- fast-replicated
- slow-local
- slow-replicated

让我们来看一个例子。

**一个存储类的YAML**

以下YAML对象定义了一个名为**fast-local**的存储类，它将根据爱尔兰AWS区域提供每GB 10 IOPs的加密SSD卷。

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast-local
provisioner: ebs.csi.aws.com <<==== AWS Elastic Block Store CSI插件
parameters:
  encrypted: true <<==== 创建加密卷
  type: io1 <<==== AWS SSD驱动器
  iopsPerGB: "10" <<==== 性能要求
allowedTopologies: <<==== 在哪里提供卷和副本

- matchLabelExpressions:
    - key: topology.ebs.csi.aws.com/zone
      values:
       - eu-west-1a <<==== 爱尔兰AWS区域

与所有Kubernetes YAML文件一样，**kind**和**apiVersion**告诉Kubernetes您正在定义的对象的类型和版本。**metadata.name**是一个任意的字符串，为对象提供了一个友好的名称，**provisioner**字段告诉Kubernetes使用哪个CSI插件。**parameters**块定义要提供的存储类型，**allowedTopologies**属性允许您指定副本的位置。

有几个重要的事项需要注意：

```
1.存储类对象是不可变的-一旦部署，就无法修改它们
2.**metadata.name**应该是有意义的，因为这是**您**和其他对象引用该类别的方式
3.术语_ provisioner_，_plugin_和_driver_有时可以互换使用
4.**parameters**块用于特定于插件的值，并且对于每个插件都不同

大多数存储系统都有自己的功能，您有责任阅读插件的文档并正确配置它。

**使用存储类**

部署和使用存储类的基本工作流程如下：

```
1.安装和配置CSI存储插件
2.创建一个或多个存储类
3.部署引用这些存储类的Pod和PVC
```

该列表假设您有一个连接到Kubernetes集群的外部存储系统。大多数托管的Kubernetes服务预安装了云本地存储后端的CSI驱动程序，这样更容易使用它们。

以下YAML片段定义了一个Pod、一个PVC和一个SC。您可以使用三个破折号（---）将所有三个对象定义在同一个YAML文件中。

apiVersion: v1
kind: Pod <<==== 1. Pod
metadata:
  name: mypod
spec:
  volumes:
```
<限制> 
请根据英文内容直接翻译，维持原有的格式，不省略任何信息。

<翻译后的文本> 
- name: data
  persistentVolumeClaim:
  claimName: mypvc <<==== 2. 通过“mypvc” PVC 请求卷
  containers: ...
  <SNIP>

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: mypvc <<==== 3. 这是“mypvc” PVC
spec:
accessModes:

- ReadWriteOnce
  resources:
  requests:
  storage: 50Gi <<==== 4. 分配一个50Gi的卷...
  storageClassName: fast <<==== 5. ...基于“fast”存储类

---

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: fast <<==== 6. 这是“fast”存储类
provisioner: pd.csi.storage.gke.io <<==== 7. 使用此CSI插件
parameters:
type: pd-ssd <<==== 8. 分配此类型的存储

YAML文件被截断，不包括完整的PodSpec。然而，通过按照编号的步骤，我们可以看到主要的工作流程：

11：Kubernetes存储167

```
1. 一个普通的Pod对象
2. Pod通过mypvc PVC请求卷
3. 文件定义了名为mypvc的PVC
4. PVC分配了一个50Gi的卷
5. 卷将从fast StorageClass中分配
6. 文件定义了fast StorageClass
7. StorageClass通过pd.csi.storage.gke.io CSI插件分配卷
8. CSI插件将从Google Cloud的存储后端分配fast(pd-ssd)存储
```

在继续演示之前，让我们来看一些其他的设置。

**其他卷设置**

StorageClasses提供了许多选项来控制卷的分配和管理。我们将介绍以下内容：

- 访问模式
- 回收策略

**访问模式**

Kubernetes支持三种卷访问模式：

- **ReadWriteOnce**（RWO）
- **ReadWriteMany**（RWM）
- **ReadOnlyMany**（ROM）

**ReadWriteOnce**允许单个PVC以读写（R/W）模式绑定到一个卷。尝试从多个PVC绑定它将失败。

**ReadWriteMany**允许多个PVC以读写（R/W）模式绑定到一个卷。文件和对象存储通常支持该模式，而块存储通常不支持。

**ReadOnlyMany**允许多个PVC以只读（R/O）模式绑定到一个卷。

还有一点需要知道的是，一个PV只能以一种模式打开。例如，不能将一个PV分别以ROM模式和RWM模式绑定到不同的PVC。

11：Kubernetes存储168

**回收策略**

**ReclaimPolicies**告诉Kubernetes当PVC被释放时要如何处理PV和相关的外部存储。目前存在两种策略：

- 删除
- 保留

**删除**是最危险的，也是通过StorageClasses动态创建的PV的默认策略。它在PVC被释放时删除PV和相关的外部存储。这意味着删除PVC将删除PV和外部存储。请谨慎使用。

**保留**在PVC被删除时保留PV和外部存储。这个选项更安全，但您必须手动清理和回收资源。

在进行演示之前，让我们总结一下您已经了解到的StorageClasses。

StorageClasses（SC）允许您定义应用程序可以使用的存储层次结构，以便动态创建卷。您可以在常规的YAML文件中定义它们，引用插件并将它们与特定类型的存储和特定的外部存储系统关联起来。例如，一个SC可以在AWS孟买区域提供高性能的AWS SSD存储，而另一个SC可以从不同的AWS区域提供低速的AWS存储。一旦部署，SC控制器将监视API服务器以获取引用SC的新PVC。每次创建与SC匹配的PVC时，SC都会动态地在外部存储系统上创建所需的卷，并将其映射到应用程序可以挂载和使用的PV。

还有更多细节，但您已经学到了足够的知识来开始使用了。

### 实践操作。

本节将指导您使用StorageClasses在外部系统上动态分配卷。我们将按以下方式进行演示：

- 使用现有的StorageClass
- 创建和使用新的StorageClass
这些演示只适用于像我们在第3章中展示如何构建的**区域GKE集群**。这是因为每个云和每个存储系统都有自己的CSI插件和配置选项，我们无法为它们创建示例。如果您没有区域GKE集群，请不要感到沮丧，通过阅读演示，您仍然能学到很多。

11：Kubernetes存储169

**使用现有的StorageClass**

以下命令列出了典型GKE Autopilot集群上预先创建的SC。输出已经修剪，如果您的集群较少也没有关系。

$ kubectl get sc
RECLAIM
NAME PROVISIONER POLICY VOLUMEBINDINGMODE
enterprise-multi.. filestore.csi.storage.gke.io Delete WaitForFirstConsumer
enterprise-rwx filestore.csi.storage.gke.io Delete WaitForFirstConsumer
premium-rwo pd.csi.storage.gke.io Delete WaitForFirstConsumer
premium-rwx filestore.csi.storage.gke.io Delete WaitForFirstConsumer
standard kubernetes.io/gce-pd Delete Immediate
standard-rwo (def) pd.csi.storage.gke.io Delete WaitForFirstConsumer
standard-rwx filestore.csi.storage.gke.io Delete WaitForFirstConsumer
zonal-rwx filestore.csi.storage.gke.io Delete WaitForFirstConsumer

让我们来看一下输出。

首先，Kubernetes在构建集群时自动创建了这些SC。大多数托管的Kubernetes平台至少预先创建了一个SC。

第六行的**standard-rwo**类是默认类。这意味着它将被未明确指定不同SC的PVC使用。默认的SC在开发环境和没有特定存储要求时才有用。在生产环境中，您应该始终为应用程序的需求指定适当的SC。

**PROVISIONER**列显示每个SC使用的CSI插件。输出中的五个SC使用**filestore.csi.storage.gke.io**插件访问Google Cloud基于NFS的Filestore存储。两个使用**pd.csi.storage.gke.io**插件访问Google Cloud的块存储。**standard** SC使用传统的in-tree **kubernetes.io/gce-pd**插件(非CSI)，您不应该使用它。

每个SC都使用**Delete RECLAIM POLICY**，这意味着Kubernetes在删除PVC时会自动删除和回收所有存储资源。

将**VOLUMEBINDINGMODE**设置为**WaitForFirstConsumer**告诉Kubernetes在Pod尝试挂载它之前不要创建卷。这保证了Kubernetes将在挂载它的Pod所在的同一区域/区域创建外部卷。将其设置为**Immediate**允许您预先创建卷，但不能保证它们将位于与最终挂载它们的Pod相同的区域或区域。

运行以下**kubectl describe**命令以查看详细的SC信息。

11：Kubernetes存储170

$ kubectl describe sc premium-rwo
名称：premium-rwo
IsDefaultClass：No
注释：components.gke.io/component-name=pdcsi,components.gke...
Provisioner：pd.csi.storage.gke.io
参数：type=pd-ssd
AllowVolumeExpansion：True
MountOptions：<none>
ReclaimPolicy：Delete
VolumeBindingMode：WaitForFirstConsumer
事件：<none>

让我们创建一个新的PVC，要求内置的**premium-rwo** SC提供一个新的外部卷。

列出任何现有的PV和PVC，以便您可以轻松识别即将创建的PV和PVC。

$ kubectl get pv
找不到资源
$ kubectl get pvc
默认命名空间中找不到资源。

以下PVC来自书的GitHub存储库中的**storage**文件夹中的**pvc-gke-premium.yml**文件。它描述了一个名为**pvc-prem**的PVC，将通过**premium-rwo** StorageClass提供一个10GB的卷。如果您的GKE集群有一个名为**premium-rwo**的StorageClass，它才有效。

apiVersion：v1
kind：PersistentVolumeClaim
metadata：
  name：pvc-prem
spec：
  accessModes：
  - ReadWriteOnce
  storageClassName：premium-rwo
  resources：
    requests：
      storage：10Gi

运行以下命令创建PVC。确保从**storage**文件夹运行它。

$ kubectl apply -f pvc-gke-premium.yml
persistentvolumeclaim/pvc-prem已创建

11：Kubernetes存储171
以下命令显示PVC已成功创建。然而，它处于“挂起”状态，且没有创建PV。这是因为**premium-rwo** StorageClass使用了**WaitForFirstConsumer**绑定模式。因此，在Pod声明之前，它不会提供卷和PV。

$ kubectl get pv
未找到资源

$ kubectl get pvc
名称 状态  卷 容量 访问模式 存储类 年龄
pvc-prem 挂起 premium-rwo 68秒

以下YAML片段定义了一个将使用**pvc-prem** PVC挂载卷的Pod。

apiVersion: v1
kind: Pod
metadata:
  name: volpod
spec:
  volumes:
  - name: data <<==== 创建名为"data"的新卷
    persistentVolumeClaim:  <<==== 基于PVC
      claimName: pvc-prem  <<==== 使用此名称
  containers:
  - name: ubuntu-ctr
    ...
    volumeMounts:  <<==== 挂载卷
    - name: data  <<==== 名称为"data"的卷(见上文)
      mountPath: /data  <<==== 挂载到此目录

运行以下命令创建Pod。这将触发外部卷和PV的创建。

$ kubectl apply -f prempod.yml
pod/volpod 已创建

等待Pod启动几秒钟，然后重新检查PVC和PV的状态。

11: Kubernetes存储 172

$ kubectl get pvc
名称 状态  卷 容量 访问模式 存储类 年龄
pvc-prem 已绑定 pvc-796afda3... 10Gi RWO premium-rwo 2分30秒

$ kubectl get pv
名称 容量 模式 回收策略 状态 声明 存储类
pvc-796af... 10Gi RWO 删除 已绑定 default/pvc-prem premium-rwo

PVC已绑定，且相关的PV已创建。如果您检查Google Cloud后端，您将看到一个与PV同名的新持久磁盘（请参见Google Cloud控制台> 计算引擎> 磁盘）。您也可以运行**kubectl describe pod volpod**命令验证Pod是否使用了PVC挂载卷。

删除Pod和PVC。Pod将需要几秒钟以正常方式删除。

$ kubectl delete pod volpod
pod "volpod" 已删除

$ kubectl delete pvc pvc-prem
persistentvolumeclaim "pvc-prem" 已删除

删除PVC还将删除Google Cloud后端上的PV和相关卷。这是因为创建它的SC将**ReclaimPolicy**设置为**Delete**。请完成以下步骤以验证此操作。

$ kubectl get pv
未找到资源

转到您的Google Cloud控制台的**Compute Engine> 磁盘**选项卡，并验证后端磁盘是否已删除。

**创建并使用新的StorageClass**

在本节中，您将创建一个新的StorageClass，并使用它来动态分配和使用一个新的卷。

您将创建位于本书GitHub存储库的**storage**文件夹中定义的**sc-gke-fast-repl.yml**文件中定义的SC。它定义了一个名为**sc-fast-repl**的SC，具有以下属性：

- 快速SSD存储（**type: pd-ssd**）
- 复制（**replication-type: regional-pd**）
- 按需创建（**volumeBindingMode: WaitForFirstConsumer**）
- 在删除PVC时保留数据（**reclaimPolicy: Retain**）

11: Kubernetes存储 173

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: sc-fast-repl
provisioner: pd.csi.storage.gke.io
parameters:
  type: pd-ssd
  replication-type: regional-pd
  volumeBindingMode: WaitForFirstConsumer
  reclaimPolicy: Retain

部署SC并验证其存在。

$ kubectl apply -f sc-gke-fast-repl.yml
storageclass.storage.k8s.io/sc-fast-repl 已创建

$ kubectl get sc
RECLAIM ALLOWVOLUME
名称 PROVISIONER POLICY VOLUMEBINDINGMODE EXPANSION
premium-rwo pd.csi.storage.gke.io 删除 WaitForFirstConsumer true
sc-fast-repl pd.csi.storage.gke.io 保留 WaitForFirstConsumer true
<剪辑>

创建SC后，您可以部署在**vol-app.yml**文件中定义的应用程序和PVC。它定义了一个20Gi的名为**pvc2**的PVC，基于新创建的**sc-fast-repl** SC。它还定义了使用PVC来声明和挂载卷的Pod。
apiVersion: v1
kind: PersistentVolumeClaim <<==== 创建一个新的PVC
metadata:
name: pvc2 <<==== 将其命名为pvc2
spec:
accessModes:

- ReadWriteOnce
  storageClassName: sc-fast-repl <<==== 基于此存储类
  resources:
  requests:
  storage: 20Gi <<==== 创建一个20Gi的卷

---

apiVersion: v1 <<==== 此Pod引用它
kind: Pod
metadata:
name: volpod
spec:
volumes:

- name: data
  persistentVolumeClaim: <<==== 通过PVC创建
  claimName: pvc2 <<==== 使用此名称
  <Snip>

11: Kubernetes存储 174

将文件发送到API服务器将创建Pod和PVC。由于Pod挂载了卷，PV和外部存储也将被创建。

使用**kubectl**检查PVC和PV是否存在。Pod启动并启动外部卷和PV创建需要几秒钟时间。

让我们总结一下刚刚发生的事情：

```
1. 您创建了一个名为sc-fast-repl的新存储类，该存储类在Google Cloud上提供区域性持久磁盘
2. SC控制器开始监视API服务器以查找引用sc-fast-repl SC的新PVC
3. 您部署了一个应用程序，该应用程序创建了一个引用SC并请求一个20GB卷的PVC
4. SC控制器观察到了PVC，并与CSI插件合作动态创建了外部卷和PV
```

恭喜您。您已经创建了自己的存储类并使用它来动态提供卷。

### 清理。

使用与创建它们相同的文件删除Pod和PVC。

$ kubectl delete -f vol-app.yml
persistentvolumeclaim "pvc2" deleted
pod "volpod" deleted

即使您删除了Pod和PVC，PV和外部存储仍然存在！

这是因为SC使用**Retain**回收策略创建它们。此策略在删除PVC后保留PV，相关的外部卷和数据。

运行以下命令删除PV。确保使用您环境中的PV名称。

11: Kubernetes存储 175

$ kubectl delete pv pvc-f36b3771-6582-4830-ad43-fb1f1ed3820c
persistentvolume "pvc-f36b3771-6582-4830-ad43-fb1f1ed3820c" deleted

从Google Cloud控制台的**ComputeEngine>Disks**选项卡中删除Google Cloud上的外部磁盘。该磁盘将显示为“未使用”，并且与您在群集上删除的PV具有完全相同的名称。**_如果在GKE后端未删除它，将产生不必要的费用！_**

删除**sc-fast-repl**存储类。

$ kubectl delete sc sc-fast-repl
storageclass.storage.k8s.io "sc-fast-repl" deleted

### 章节总结。

在本章中，您了解了Kubernetes拥有强大的存储子系统，允许应用程序动态从各种外部存储系统中提供和使用存储。

每个外部存储系统都需要其自己的CSI插件，该插件创建外部卷并在Kubernetes内部公开它们。

安装CSI插件后，您可以创建映射到外部系统上的一种类型或层级存储的StorageClass对象。StorageClass控制器在后台运行，监视API服务器以查找新的PVC对象。每次看到一个PVC时，它就在外部系统上创建请求的卷，并将其映射到Kubernetes上的新PV。然后，Pod可以使用PVC挂载PV以供使用。

## 12:ConfigMaps和Secrets

```
大多数商业应用程序都有两个组件：
```

- 应用程序
- 配置

```
简单的例子包括NGINX或httpd（Apache）等Web服务器。如果没有配置，它们都没有太大用处。但是，一旦添加了配置，它们就变得非常有用。
在过去，我们将应用程序和配置打包到一个易于部署的单元中。随着我们进入云原生微服务的早期阶段，我们带来了这种模式。然而，这是一个反模式，现代应用程序应该与其配置解耦。这样做带来以下好处：
```

- 重用
- 更简单的开发和测试
- 更简单和不中断的更改

在本章中，我们将详细解释所有这些以及更多内容。
注意：反模式是指看起来像是一个好主意，但最终证明是一个坏主意的东西。

本章分为以下几个部分：

- 概览
- ConfigMap理论
- 使用ConfigMaps进行实践
- 使用Secrets进行实践

12: ConfigMaps和Secrets 177

### 概览

如前所述，大多数应用程序由应用程序二进制文件和配置组成。Kubernetes允许您将它们构建并存储为不同的对象，并在运行时将它们组合在一起。

考虑一个简单的例子。

想象一下，您在一家有三个环境的公司工作：

- 开发环境
- 测试环境
- 生产环境

您在**开发环境**进行初始测试，在**测试环境**进行更广泛的测试，最后将应用程序升级到**生产环境**。然而，每个环境都有自己的网络策略和安全策略，以及自己独特的凭据和证书。

目前，您将应用程序二进制文件和其配置打包在同一个镜像中，这迫使您为每个应用程序执行以下所有操作：

- 为开发配置、测试配置和生产配置构建三个镜像
- 将这些镜像存储在三个存储库中（一个用于开发镜像、一个用于测试镜像、一个用于生产镜像）
- 在三个环境中的每个应用程序中运行不同版本的应用程序（开发环境中的开发应用程序、测试环境中的测试应用程序、生产环境中的生产应用程序）

每次更改任何应用程序的配置，即使是修正一个拼写错误这样的小更改，都必须构建、测试、存储和重新部署三个镜像-一个用于开发、一个用于测试、一个用于生产。

当每次更新都包含应用程序代码和配置时，要进行故障排除和隔离问题也更加困难。

**在解耦世界中的样子**

设想您仍在同一家公司工作，他们要求您构建一个新的Web应用程序。然而，公司现在将应用程序解耦，这样应用程序代码和配置就可以分别存储。

您决定基于NGINX构建新的应用程序，并创建一个硬化的NGINX镜像，其他团队和应用程序可以通过应用自己的配置来使用。这意味着：

12: ConfigMaps和Secrets 178

- 您只需构建一个可在所有三个环境中使用的镜像
- 您只需将该单个镜像存储和保护在一个存储库中
- 您在所有环境中运行该镜像的相同版本

为使此工作正常，您构建了一个仅包含硬化NGINX的单个镜像，没有嵌入的配置。

然后，您在运行时创建三个**配置**，分别为**开发环境**、**测试环境**和**生产环境**。每个配置将为NGINX容器配置正确环境的策略设置和凭据。其他团队和应用程序可以通过创建自己的配置来**重用**相同的硬化NGINX镜像，用于他们自己的Web应用程序。

在此模型中，您创建和测试单个版本的NGINX，将其构建到单个镜像中，并将其存储在单个存储库中。您可以将所有开发人员授予对存储库的访问权限，因为它不包含敏感数据，并且您可以独立于彼此推送应用程序和其配置的更改。例如，如果主页上有个拼写错误，您可以在配置中修复它，并将其推送到所有三个环境中的现有容器中。您不再需要停止并替换所有三个环境中的每个容器。

让我们看看Kubernetes如何实现这一点。

### ConfigMap理论

Kubernetes具有一个名为ConfigMap（CM）的API资源，它可以让您在Pod之外存储配置数据，并在运行时将其注入。

ConfigMaps是核心API组中的一流对象。它们也是**v1**版本。这告诉我们一些信息：

```
1.它们是稳定的（v1）
2.它们已经存在一段时间（新的东西从不在核心API组中）
3.您可以使用YAML文件定义和部署它们
4.您可以使用kubectl管理它们
```

通常您会使用ConfigMaps存储非敏感的配置数据，例如：

- 环境变量
- 配置文件，如Web服务器配置和数据库配置
- 主机名
- 服务端口

12: ConfigMaps和Secrets 179

- 账户名称

您不应该使用ConfigMaps存储敏感数据，例如证书和密码，因为Kubernetes不会努力保护其内容。对于敏感数据，应使用Kubernetes Secrets和第三方工具的组合。

稍后您将看到如何使用Secrets。


**ConfigMaps的工作原理**
在高层次上，ConfigMap是一个存储配置数据的地方，您可以在运行时轻松地将其注入到容器中。它们对应用程序是透明的，这意味着您不需要修改应用程序以便与它们一起工作。

让我们仔细看一下。

在幕后，ConfigMap是Kubernetes对象，它包含一组键值对：

- **键**是任意名称，可以包括字母数字、破折号、点和下划线
- **值**可以包含任何内容，包括具有多行和换行符的完整配置文件
- 您使用冒号（**key:value**）来分隔键和值
- 它们的大小限制为1MiB（1,048,576字节）

这是一个包含三个简单条目的ConfigMap。

kind: ConfigMap
apiVersion: v1
metadata:
name: epl
data:
Competition: epl
Season: 2022-2023
Champions: Manchester City

这是另一个示例，但是_value_这次包含一个完整的配置文件。

12: ConfigMaps and Secrets 180

kind: ConfigMap
apiVersion: v1
metadata:
name: cm2
data:
test.conf: |
env = plex-test
endpoint = 0.0.0.0:31001
char = utf8
vault = PLEX/test
log-size = 512M

存储数据在ConfigMap中后，您可以使用以下任何方法之一在运行时将其注入到容器中：

```
1.环境变量
2.容器启动命令的参数
3.卷中的文件
```

图12.1显示了这些部分如何连接。

```
图12.1
```

这三种方法都适用于现有的应用程序。但是，卷选项最灵活，而启动命令选项最不灵活。接下来，我们将依次介绍每种方法，但在此之前，让我们简要提到_基于Kubernetes的_应用程序。

**ConfigMaps和基于Kubernetes的应用程序**

_Kubernetes原生应用程序_知道它们正在Kubernetes上运行，并可以与API服务器通信。这意味着它们可以直接通过API服务器访问ConfigMap数据，而无需环境变量或卷。这可以简化事情，但应用程序只能在Kubernetes上运行（Kubernetes锁定）。

12: ConfigMaps and Secrets 181

### 使用ConfigMaps的实践。

如果您想跟随示例进行操作，您需要一个Kubernetes集群和本书的GitHub存储库中的实验文件。

$ git clone https://github.com/nigelpoulton/TheK8sBook.git
正在克隆到 'TheK8sBook'...

确保从**configmaps**文件夹中运行以下所有命令。

与大多数Kubernetes资源一样，您可以以命令方式和声明方式创建ConfigMaps。我们首先看一下命令方式。

**以命令方式创建ConfigMaps**

您可以使用**kubectl create configmap**命令以命令方式创建ConfigMaps。但是，您可以将**configmap**缩写为**cm**，而且该命令接受两种数据源：

- 命令行上的字面值（**--from-literal**）
- 文件（**--from-file**）

运行以下命令以使用命令行字面值创建名为**testmap1**的ConfigMap，其中包含两个条目。Windows用户应该将每行末尾的反斜杠替换为重音符号。

$ kubectl create configmap testmap1 \
--from-literal shortname=AOS \
--from-literal longname="Agents of Shield"

运行以下命令以查看Kubernetes如何存储映射条目。

$ kubectl describe cm testmap1
Name: testmap1
Namespace: default
Labels: <none>
Annotations: <none>
Data
====

shortname:
----

AOS
longname:

12: ConfigMaps and Secrets 182

----

Agents of Shield
BinaryData
====

Events: <none>

您可以看到它只是一个装扮成Kubernetes对象的键值对映射。

以下命令使用**--from-file**标志从名为**cmfile.txt**的文件创建ConfigMap。该文件包含一行文本，您需要从本书的GitHub存储库的**configmaps**文件夹中运行该命令。

$ kubectl create cm testmap2 --from-file cmfile.txt
configmap/testmap2 created

您将在下一节中检查这个ConfigMap。

**检查ConfigMaps**

ConfigMaps是一级API对象。这意味着您可以像任何其他API对象一样检查和查询它们。

列出当前命名空间中的所有ConfigMaps。
$ kubectl get cm
名称      数据   年龄
testmap1  2    11分钟
testmap2  1    2分钟23秒

以下的**kubectl describe**命令显示了有关您从本地文件创建的**testmap2**映射的一些有趣信息：

- 此操作创建了一个映射条目
- _键_的名称与输入文件的名称相匹配（**cmfile.txt**）
- _值_存储了文件的内容

12：ConfigMaps和Secrets 183

$ kubectl describe cm testmap2
名称：testmap2
命名空间：default
标签：<无>
注释：<无>
数据
====

cmfile.txt：<<==== 键
----

Kubernetes FTW！<<==== 值
BinaryData
====

事件：<无>

您还可以使用**-o yaml**标志运行**kubectl get**命令以查看整个对象。

$ kubectl get cm testmap1 -o yaml
apiVersion：v1
数据：
longname：Agents of Shield
shortname：AOS
kind：ConfigMap
元数据：
creationTimestamp：“2024-01-09T14:16:03Z”
名称：testmap1
命名空间：default
resourceVersion：“20904”
uid：87b03869-e29d-4744-b43b-cb6178bc61fe

您应该知道，ConfigMaps没有状态的概念（期望状态和实际状态）。这就是为什么它们有一个**数据**块而不是通常的**规范**和**状态**块。

让我们先看看如何在使用它们将配置数据注入容器之前以声明方式创建ConfigMaps。

**以声明方式创建ConfigMaps**

以下YAML来自书籍的GitHub存储库中的**multimap.yml**文件，并定义了两个映射条目：**given**和**family**。它具有通常的**kind，apiVersion**和**metadata**字段。然而，正如之前提到的，它没有**spec**部分。相反，它有一个**data**部分，您在其中定义键值对的映射。

12：ConfigMaps和Secrets 184

kind：ConfigMap
apiVersion：v1
metadata：
name：multimap
data：
given：Nigel
family：Poulton

使用以下命令部署它。

$ kubectl apply -f multimap.yml
configmap/multimap已创建

此下一个YAML对象看起来比前一个更复杂。但实际上更简单，因为**data**块中只有一个条目。它看起来更复杂是因为_value_条目包含一个完整的配置文件。

kind：ConfigMap
apiVersion：v1
metadata：
name：test-config
data：
test.conf：|
env = plex-test
endpoint = 0.0.0.0:31001
char = utf8
vault = PLEX/test
log-size = 512M

如果仔细观察，您会在键属性的名称后面看到管道字符（**|**）。这告诉Kubernetes将管道后面的所有内容视为单个字面值。因此，ConfigMap对象称为**test-config**，并且具有以下单个映射条目：

```
对象名称   键          值
test-config   test.conf   env = plex-test
endpoint = 0.0.0.0:31001
char = utf8
vault = PLEX/test
log-size = 512M
```

使用以下命令部署它。它将创建一个名为**test-config**的新ConfigMap。

$ kubectl apply -f singlemap.yml
configmap/test-config已创建

使用以下命令检查它。

12：ConfigMaps和Secrets 185

$ kubectl describe cm test-config
名称：test-config
命名空间：default
标签：<无>
注释：<无>
数据
====

test.conf：
----

env = plex-test
endpoint = 0.0.0.0:31001
char = utf8
vault = PLEX/test
log-size = 512M
BinaryData
====

事件：<无>

**将ConfigMap数据注入Pod和容器**

有三种将ConfigMap数据注入容器的方法：

- 作为环境变量
- 作为容器启动命令的参数
- 作为卷中的文件

让我们逐个看一下。

**ConfigMaps和环境变量**

您可以将ConfigMap数据作为环境变量注入容器。但是，如果在部署容器后对ConfigMap进行更改，它们不会出现在容器中。
图12.2展示了这个过程。首先，您创建一个ConfigMap。然后，在Pod模板的**容器**部分中将其条目映射为环境变量。最后，当容器启动时，这些环境变量会以标准的Linux或Windows环境变量的形式出现，并且应用程序会使用它们而不知道有ConfigMap的参与。

12：ConfigMaps和Secrets 186

```
图12.2
```

您已经有一个名为**multimap**的ConfigMap，其中包含以下两个条目：

- given=Nigel
- family=Poulton

以下Pod清单部署了一个只有两个环境变量的单个容器，并将其映射到ConfigMap中的相应位置：

- FIRSTNAME：映射到ConfigMap的**given**条目
- LASTNAME：映射到ConfigMap的**family**条目

apiVersion: v1
kind: Pod
<Snip>
spec:
containers:

- name: ctr1
  env:
  - name: FIRSTNAME <<==== 名为FIRSTNAME的环境变量
  valueFrom: <<==== 基于
  configMapKeyRef: <<==== 一个ConfigMap
  name: multimap <<==== 名为"multimap"
  key: given <<==== 并由"given"字段中的值填充
  - name: LASTNAME <<==== 名为LASTNAME的环境变量
  valueFrom: <<==== 基于
  configMapKeyRef: <<==== 一个ConfigMap
  name: multimap <<==== 名为"multimap"
  key: family <<==== 并由"family"字段中的值填充
  <Snip>

```
12：ConfigMaps和Secrets 187
```

```
当Pod被调度并且容器启动时，FIRSTNAME和LASTNAME将被创建为标准的Linux环境变量，应用程序可以在不了解ConfigMaps的情况下使用它们。
运行以下命令以从envpod.yml部署Pod。
```

```
$ kubectl apply -f envpod.yml
pod/envpod已创建
```

运行以下exec命令以列出容器中名称中包含**"NAME"**字符串的环境变量。这将列出**FIRSTNAME**和**LASTNAME**变量，并且您会看到它们由ConfigMap中的值填充。
请确保在执行以下命令之前Pod正在运行。如果您在Windows机器上，请将**grep NAME**参数替换为**Select-String -Pattern 'NAME'**。

```
$ kubectl exec envpod -- env | grep NAME
HOSTNAME=envpod
FIRSTNAME=Nigel
LASTNAME=Poulton
```

```
如前所述，环境变量是静态的。这意味着您对ConfigMap所做的更新不会显示在容器中，这也是不使用环境变量的主要原因。
```

```
ConfigMaps和容器启动命令
```

```
使用ConfigMaps与容器启动命令的概念很简单。您在Pod模板中指定容器的启动命令，然后使用变量进行自定义。
以下Pod模板是从startuppod.yml文件中提取的。它描述了一个基于busybox映像的名为args1的单个容器。然后，它从multimap ConfigMap定义并填充了两个环境变量。最后，它在容器的启动命令中引用了这些环境变量。
```

12：ConfigMaps和Secrets 188

spec:
containers:

- name: args1
  image: busybox
  env:
  - name: FIRSTNAME <<==== 名为FIRSTNAME的环境变量
  valueFrom: <<==== 基于
  configMapKeyRef: <<==== 一个ConfigMap
  name: multimap <<==== 名为"multimap"
  key: given <<==== 并由"given"字段中的值填充
  - name: LASTNAME <<==== 名为LASTNAME的环境变量
  valueFrom: <<==== 基于
  configMapKeyRef: <<==== 一个ConfigMap
  name: multimap <<==== 名为"multimap"
  key: family <<==== 并由"family"字段中的值填充
  command: [ "/bin/sh", "-c", "echo First name $(FIRSTNAME) last name $(LASTNAME)" ]

图12.3总结了如何从ConfigMap中填充环境变量并在启动命令中引用它们。

```
图12.3-将ConfigMap条目映射到启动命令
```

从**startuppod.yml**文件启动一个新的Pod。Pod将启动，将**Firstname NigellastnamePoulton**打印到容器的日志中，然后退出（成功）。Pod启动和执行可能需要几秒钟的时间。
$ kubectl apply -f startuppod.yml
pod/startup-pod 创建成功

运行以下命令来检查容器日志，并验证是否打印了 **FirstnameNigellastnamePoulton**。

12: 配置映射和密钥 189

$ kubectl logs startup-pod -c args1
名字 Nigel 姓氏 Poulton

描述 Pod 将显示有关环境变量的以下数据。

$ kubectl describe pod startup-pod
<省略>
环境变量:
FIRSTNAME: <设置为配置映射 'multimap' 的键 'given'>
LASTNAME: <设置为配置映射 'multimap' 的键 'family'>
<省略>

正如您所见，使用配置映射与容器启动命令是环境变量的扩展。因此，它们存在相同的限制 - 对映射的更新不会反映在运行的容器中。

如果您运行了 **startup-pod**，它应该处于已完成状态。这是因为其启动命令已完成，导致 Pod 成功。使用 **kubectl delete pod startup-pod** 删除它。

**配置映射和卷**

使用配置映射与卷是最灵活的选择。您可以引用整个配置文件，并且更新会反映在运行的容器中。更新可能需要一分钟左右才会出现在容器中。

使用卷将配置映射数据注入容器的高级过程如下：

```
1. 创建配置映射
2. 在 Pod 模板中定义配置映射卷
3. 将配置映射卷挂载到容器
4. 配置映射条目将出现在容器内部作为文件
```

图 12.4 显示了此过程。

```
图12.4 - 通过卷映射配置映射条目
```

您已经部署了名为 **multimap** 的配置映射，并且它具有以下值：

```
12: 配置映射和密钥 190
```

- given=Nigel
- family=Poulton

以下 YAML 定义了一个名为 cmvol 的 Pod，具有以下配置：

- **spec.volumes** 创建名为 **volmap** 的卷，基于配置映射 **multimap**
- **spec.containers.volumeMounts** 将 **volmap** 卷挂载到 **/etc/name**

```
apiVersion: v1
kind: Pod
metadata:
  name: cmvol
spec:
  volumes:
  - name: volmap <<==== 创建名为 "volmap" 的卷
    configMap: <<==== 基于配置映射
      name: multimap <<==== 名为 "multimap" 的配置映射
  containers:
  - name: ctr
    image: nginx
    volumeMounts: <<==== 这些行将 "volmap" 卷挂载到容器中
    - name: volmap
      mountPath: /etc/name <<==== 在容器的 "/etc/name" 处挂载

```
运行以下命令来部署先前 YAML 中描述的 cmvol Pod。

```
$ kubectl apply -f cmpod.yml
pod/cmvol 创建成功
```

等待 Pod 进入运行阶段，然后运行以下 **kubectl exec** 命令来列出容器的 **/etc/name/** 目录中的文件。

```
$ kubectl exec cmvol -- ls /etc/name
family
given
```

您可以看到容器中有两个与配置映射条目相匹配的文件。随时运行其他 kubectl exec 命令来查看文件的内容，并确保它们与配置映射中的值匹配。

现在，让我们证明对映射的更改会反映在容器中。
使用 kubectl edit 命令编辑配置映射，并更改数据块中的任何值。该命令将在您的默认编辑器中打开 YAML 对象，通常在 Mac 和 Linux 上为 vi，在 Windows 上为 notepad.exe。如果您不熟悉使用 vi，可以在其他编辑器中手动编辑 YAML 文件，并使用 kubectl apply 将其重新提交到 API 服务器。

以下代码块被注释，以显示需要更改的行。

$ kubectl edit cm multimap

# 请编辑下面的对象。以 '#' 开头的行将被忽略，

# 空文件将中止编辑。如果保存时发生错误，

# 将重新打开此文件，并显示相关的错误。

#

apiVersion: v1
data:
  City: Macclesfield <<==== 已更改
  Country: UK <<==== 已更改
kind: ConfigMap
metadata:
  <省略>

保存更改并检查更新是否出现在容器中。更改可能需要一分钟才会出现。

$ kubectl exec cmvol -- ls /etc/name
City
Country
恭喜您，通过_ConfigMap volume_，**multimap** ConfigMap的内容已经在容器的文件系统中显示出来，并且您已经测试了更新操作。

### 与Secrets一起实践

Secrets与ConfigMaps几乎相同 - 它们保存Kubernetes在运行时注入到容器中的应用程序配置数据。然而，Secrets旨在保存敏感数据，如密码、证书和OAuth令牌。

**Kubernetes Secrets安全吗？**

对于这个问题，快速答案是**_不_**。但是这里稍长一点的答案是...

```
12: ConfigMaps and Secrets 192
```

```
尽管设计用于敏感数据，但Kubernetes并没有在集群存储中对Secrets进行加密。它只将它们转换为Base64编码的值，任何人都可以在没有密钥的情况下解码它们。幸运的是，大多数服务网格会加密网络流量，并且您可以使用EncryptionConfiguration对象配置加密在休息时。然而，许多人使用类似HashiCorp的Vault^10的工具来实现更完整和安全的秘密管理解决方案。
```

我们将重点介绍Kubernetes提供的基本秘密管理功能，即使结合第三方工具，它仍然非常有用。
一个典型的秘密工作流程如下：

```
1. 创建秘密，并将其作为未加密对象持久化到集群存储中
2. 调度使用该秘密的Pod
3. Kubernetes通过网络将未加密的秘密传输到运行Pod的节点
4. 节点上的kubelet启动Pod及其容器
5. 容器运行时通过内存中的tmpfs文件系统将秘密挂载到容器中，并将其从Base64解码为明文
6. 应用程序使用该秘密
7. 当您删除Pod时，Kubernetes会删除节点上的秘密副本（但保留集群存储中的副本）
```

```
即使您在集群存储中加密了秘密，并且有一个服务网格在网络传输过程中加密了它，Kubernetes仍然会将其作为明文挂载到容器中，以便应用程序在不需要解密或解码的情况下使用它。
另外，使用内存中的tmpfs文件系统意味着秘密永远不会持久化到群集节点的磁盘上。
简而言之，Secrets并不是非常安全。然而，您可以采取额外的措施来使其更安全。
Secrets的一个明显用例是用于在各个环境中使用的TLS终止代理。图12.5显示了一个配置有三个不同Secrets的单个镜像，用于三个不同的环境。Kubernetes在运行时将适当的Secret加载到每个容器中。
```

(^10) https://www.vaultproject.io/

12: ConfigMaps and Secrets 193

```
Figure12.5-InjectingSecretsatruntime
```

**创建Secrets**

在继续本节之前，请记住Secrets在集群存储中不加密，在网络传输过程中不加密，在容器中显示时也不加密。即使您实施了在集群存储和网络上加密Secrets的解决方案，它们始终以明文形式显示在容器中，以便应用程序可以使用它们。

与所有API资源一样，您可以使用命令式和声明式的方式创建Secrets。

运行以下命令创建一个名为**creds**的新Secret。如果您使用Windows，请记得用反引号替换反斜杠。

$ kubectl create secret generic creds --from-literal user=nigelpoulton \
--from-literal pwd=Password123

您之前学到，Kubernetes通过将Secrets编码为Base64值来隐藏它们。使用以下命令检查：

12: ConfigMaps and Secrets 194

$ kubectl get secret creds -o yaml
apiVersion: v1
kind: Secret
data:
pwd: UGFzc3dvcmQxMjM=
user: bmlnZWxwb3VsdG9u
<Snip>

用户名和密码的值都是Base64编码的。运行以下命令对它们进行解码。您需要在系统上安装**base64**实用程序以使命令生效。如果您没有安装，可以使用在线解码器。

$ echo UGFzc3dvcmQxMjM= | base64 -d
Password123

解码操作成功完成，没有使用密钥，证明Base64编码并不安全。

以下的YAML对象来自**configmaps**文件夹中的**tkb-secret.yml**文件。它描述了一个名为**tkb-secret**的Secret，其中包含两个Base64编码的条目。
apiVersion：v1
kind：Secret
metadata：
  name：tkb-secret
  labels：
    chapter：configmaps
  type：Opaque
data：<<==== 将"data"改为"stringData"以用于纯文本
  username：bmlnZWxwb3VsdG9u
  password：UGFzc3dvcmQxMjM=

如果要添加纯文本条目，请将**data**块重命名为**stringData**。然而，尽管允许您以纯文本形式输入值，但它们仍将以base64格式存储，并且后续的**kubectl**命令将以base64格式检索它们。

将其部署到您的集群中。确保从**configmaps**文件夹运行命令。

$ kubectl apply -f tkb-secret.yml
secret/tkb-secret已创建

运行**kubectl get**和**kubectl describe**命令来检查它。

```
12：ConfigMaps和Secrets 195
```

**在Pod中使用Secrets**

```
Secrets与ConfigMaps类似，这意味着您可以将它们注入到容器中作为环境变量、命令行参数或卷。与ConfigMaps一样，最灵活的方式是使用卷。
以下的YAML描述了一个带有名为secret-vol的Secret卷的单容器Pod，该卷基于您在前一步中创建的tkb-secret。它将secret-vol挂载到容器的/etc/tkb路径下。
```

```
apiVersion: v1
kind: Pod
metadata:
  name: secret-pod
  labels:
    topic: secrets
spec:
  volumes:
    - name: secret-vol <<==== 卷名称
      secret: <<==== 卷类型
        secretName: tkb-secret <<==== 使用此Secret填充卷
  containers:
    - name: secret-ctr
      image: nginx
      volumeMounts:
        - name: secret-vol <<==== 挂载上面定义的卷
          mountPath: "/etc/tkb" <<==== 挂载到此路径

```
Secret卷是Kubernetes API中的资源，Kubernetes会自动将它们以只读方式挂载，以防止容器和应用程序意外地更改它们。
使用以下命令部署Pod。这将导致Kubernetes将未加密的Secret通过网络传输到运行Pod的节点上的kubelet。然后，容器运行时将通过tmpfs挂载将其挂载到容器中。

```
$ kubectl apply -f secretpod.yml
pod/secret-pod已创建
```

以下命令显示Secret作为**/etc/tkb**目录中的两个文件挂载到容器中。

12：ConfigMaps和Secrets 196

$ kubectl exec secret-pod -- ls /etc/tkb
password
username

如果检查任一文件的内容，您会看到它们以纯文本形式挂载，以便应用程序可以轻松使用它们。

$ kubectl exec secret-pod -- cat /etc/tkb/password
Password123

请记住，完整的Secret管理解决方案需要其他工具来对Secret在存储和传输过程中进行加密。

### 清理。

使用**kubectl get**列出在本章中部署的Pods、ConfigMaps和Secrets，并使用**kubectl delete**删除它们。

### 章节总结。

ConfigMaps和Secrets是解耦应用程序和关联配置数据的Kubernetes本地方式。

它们都是Kubernetes API中的一级对象，可以以命令方式和声明方式创建，并且可以使用**kubectl**进行检查。

ConfigMaps设计用于应用程序配置参数甚至整个配置文件，而Secrets设计用于敏感数据。

您可以通过环境变量、容器启动命令参数和卷将两者注入到容器中。卷是首选的方法，因为它们允许您更新映射，并且您的更新会显示在运行的容器中。

Kubernetes不会在集群存储中加密Secret，也不会在网络传输过程中加密Secret。

## 13：StatefulSets

```
在本章中，您将学习如何使用StatefulSets在Kubernetes上部署和管理有状态应用程序。
在本章中，我们将有状态应用程序定义为创建并保存有价值的数据的应用程序。示例包括数据库、键值存储和保存有关客户会话数据并在将来会话中使用的应用程序。
```

我们将按如下方式划分本章：

- StatefulSet理论
- 使用StatefulSets进行实践

```
理论部分介绍了StatefulSets的工作原理以及它们为有状态应用程序提供的功能。但是，如果您一开始不理解全部内容，不用担心，您将在实践部分再次涵盖所有内容。
```

### StatefulSet理论。
在比较StatefulSets和Deployments时会很有帮助。它们都是Kubernetes API中的资源，并遵循标准的Kubernetes控制器架构——控制循环将观察到的状态与期望的状态进行调和。它们都管理Pods并提供自我修复、扩展、滚动等功能。
然而，StatefulSets提供了以下三个特性，Deployments没有：
- 可预测和持久的Pod名称
- 可预测和持久的DNS主机名
- 可预测和持久的卷绑定

这三个属性构成了一个Pod的状态，我们有时将它们称为Pod的sticky ID。StatefulSets确保这三个属性在故障、扩展操作和其他调度事件中持久存在。
举个快速的例子，由StatefulSet管理的失败的Pod将被具有完全相同的Pod名称、完全相同的DNS主机名和完全相同的卷的新Pod所替换。即使Kubernetes在不同的集群节点上启动替换的Pod，这一点也是成立的。

这使得StatefulSets非常适用于需要独特可靠的Pod和卷的应用程序。

以下的YAML定义了一个名为**tkb-sts**的简单StatefulSet，它使用三个副本运行**mongo:latest**镜像。您可以将其提交到API服务器，它将被持久化到集群存储中，调度器将副本分配给工作节点，StatefulSet控制器确保观察到的状态与期望的状态匹配。

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: tkb-sts
spec:
  selector:
    matchLabels:
      app: mongo
  serviceName: "tkb-sts"
  replicas: 3
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: ctr-mongo
        image: mongo:latest
        ...

这就是大致的情况。在进一步深入之前，让我们仔细看看。

**StatefulSet Pod命名**

由StatefulSet创建的每个Pod都有一个可预测的名称。事实上，Pod的名称是StatefulSets启动、自我修复、扩展和删除Pod的核心。它们对于附加卷也非常重要。

StatefulSet Pod名称的格式是**<StatefulSetName>-<Integer>**。整数是从零开始的序数，也就是说从零开始的数字。假设前面的YAML片段，第一个Pod将被称为**tkb-sts-0**，第二个将被称为**tkb-sts-1**，第三个将被称为**tkb-sts-2**。

StatefulSets还应该具有有效的DNS名称，不要使用特殊字符。

**有序的创建和删除**

StatefulSets和Deployments之间的一个重要区别是它们创建Pod的方式。

- StatefulSets一次只创建一个Pod，并等待它运行并准备就绪后再启动下一个Pod
- Deployments使用ReplicaSet控制器同时启动所有Pods，这可能导致竞争条件

假设再次使用前面的YAML，**tkb-sts-0**将首先启动，并且在StatefulSet控制器启动**tkb-sts-1**之前必须是_running_和_ready_状态。同样的规则适用于后续Pods——**tkb-sts-1**需要在启动**tkb-sts-2**之前处于_running_和_ready_状态。参见图13.1。

```
图13.1
```

```
注意：Running和Ready是用来指示Pod中所有容器都在运行且Pod准备好服务请求的术语。
```

相同的启动规则也适用于StatefulSet的扩展操作。例如，从3个副本扩展到5个副本将启动一个名为**tkb-sts-3**的新Pod，并等待它运行并准备就绪后再创建**tkb-sts-4**。缩减副本数遵循相同的规则，控制器终止具有最高序数索引的Pod，并等待其完全终止后再终止具有下一个最高编号的Pod。

了解Pod将被缩减的顺序，以及Kubernetes不会并行终止它们的知识，对于有状态的应用程序至关重要。例如，如果多个副本同时终止，集群应用程序可能会丢失数据。StatefulSets保证这种情况永远不会发生。

最后，值得注意的是StatefulSet控制器会自行进行自我修复和扩展。这在架构上与Deployments不同，后者使用ReplicaSet控制器进行这些操作。

**删除StatefulSets**

关于删除StatefulSets，有两个重要的事项需要知道。


```
13: StatefulSets 200
```

首先，删除一个StatefulSet对象不会以有序的方式终止其Pod。这意味着在删除之前，您应该将StatefulSet的副本数缩减为0。您还可以使用terminationGracePeriodSeconds进一步控制Pod的终止方式。通常将其设置为至少10秒，以便应用程序可以刷新任何缓冲区并安全地提交仍在进行中的写入操作。

**StatefulSets和Volumes**

卷是StatefulSet Pod的重要组成部分，用于保持其粘性ID（状态）。

当StatefulSets创建Pod时，它们还会创建Pod所需的任何卷。为了帮助实现这一点，它们为卷提供特殊名称，Kubernetes使用这些名称将它们连接到正确的Pod上。图13.2显示了一个名为**tkb-sts**的StatefulSet请求三个具有单个卷的Pod。您可以看到Kubernetes如何使用卷名称将它们连接到正确的Pod上。

```
Figure13.2
```

尽管与特定Pod副本关联，但卷仍通过常规的持久卷索赔系统与Pod解耦。这意味着卷具有独立的生命周期，使其能够在Pod故障和Pod终止操作时幸存下来。例如，当一个StatefulSet Pod失败或被终止时，其关联的卷不受影响。这使得替代Pod可以连接到幸存的卷和数据，即使Kubernetes将替代Pod调度到不同的集群节点上。在扩容操作期间也是如此。如果缩减操作删除了一个StatefulSet Pod，后续的扩容操作将新的Pod附加到幸存的卷上。

```
13: StatefulSets 201
```

如果您意外删除了一个StatefulSet Pod，尤其是如果它是最后一个副本，这种行为可能会挽救您的生命！

**处理故障**

StatefulSet控制器观察集群的状态，并将观察到的状态与期望的状态进行协调。

最简单的例子是Pod故障。如果您有一个名为tkb-sts的StatefulSet，有五个副本，而tkb-sts-3副本失败了，控制器会启动一个具有相同名称的新Pod，并将其连接到幸存的卷上。

节点故障可能更复杂，一些较旧的Kubernetes设置需要手动干预来替换运行在故障节点上的Pod。这是因为Kubernetes很难知道节点是否已失败，或者它是否是一个短暂的事件，例如电源故障，节点将重新启动。如果“失败”的节点在Kubernetes替换其Pod之后恢复，那么您将得到尝试写入同一卷的相同Pod，这可能导致数据损坏。较新的Kubernetes版本比旧版本更好地处理这些情况，并且更快。

**网络ID和无头服务**

我们已经说过，StatefulSets适用于需要Pod可预测且持久的应用程序。这可能涉及应用程序连接到特定的Pod，而不是让服务在所有Pod之间执行循环负载平衡。为了实现这一点，StatefulSets使用一个_headless Service_来为每个Pod创建可靠且可预测的DNS名称。其他应用程序可以查询DNS（服务注册表）以获取完整的Pod列表并建立直接连接。下面的YAML片段显示了一个名为**mongo-prod**的无头服务在StatefulSet YAML中作为_governing Service_列出。

```
apiVersion: v1
kind: Service <<==== Service
metadata:
name: mongo-prod
spec:
clusterIP: None <<==== 将其设置为无头服务
selector:
app: mongo
env: prod
---
apiVersion: apps/v1
```

```
13: StatefulSets 202
```

```
kind: StatefulSet <<==== StatefulSet
metadata:
name: sts-mongo
spec:
serviceName: mongo-prod <<==== 掌控服务
```

```
让我们解释一下无头服务和掌控服务这两个术语。
无头服务是一个常规的Kubernetes Service对象，没有ClusterIP地址（ spec.clusterIP设置为None）。当您在StatefulSet配置中的spec.serviceName下列出它时，它将成为StatefulSet的掌控服务。
```
当你将一个无头服务与一个有状态副本集(StatefulSet)结合使用时，该服务会为与该服务的标签选择器匹配的每个Pod创建DNS SRV和DNS A记录。其他Pod和应用程序可以查询DNS，并获取所有StatefulSet Pod的名称和IP地址。稍后我们将会看到这个过程，但是开发人员必须编写应用程序以此方式查询DNS。

这基本上涵盖了大部分理论。让我们通过一个示例来演示并看看所有东西是如何组合在一起的。

### 亲自动手操作StatefulSets..

```
在本部分中，您将部署一个可工作的StatefulSet。
这些演示是在Google Kubernetes Engine (GKE)和本地Docker桌面集群上设计和测试的。如果您的集群在不同的云上，您将需要使用不同的StorageClass。我们会告诉您何时需要这样做。
如果您还没有这样做，请运行以下命令克隆本书的GitHub存储库。
```

```
$ git clone https://github.com/nigelpoulton/TheK8sBook.git
```

```
从statefulsets文件夹中运行所有剩余的命令。
您即将部署以下三个对象：
```

```
1.一个StorageClass
2.一个无头服务
3.一个StatefulSet
```

```
为了更容易跟踪，您将逐个部署和检查每个对象。
然而，您也可以将它们分组到一个单独的YAML文件中，并使用一个命令部署它们(请参阅存储库的statefulsets文件夹中的app.yml)。
```

13: StatefulSets 203

**部署StorageClass**

StatefulSets需要动态创建卷。为了做到这一点，它们需要：

- 一个StorageClass(SC)
- 一个PersistentVolumeClaim(PVC)

以下YAML来自于**gcp-sc.yml**文件，并定义了一个名为**flash**的StorageClass对象，它使用GKE持久磁盘CSI驱动在Google Cloud上动态提供SSD卷。它只适用于GKE或GCP集群。如果您使用的是Docker桌面集群，您应该使用**dd-sc.yml**文件。如果您的集群在不同的云上，您可以执行以下操作之一：

- 为您自己的云创建一个名为**flash**的新StorageClass - 您需要自己创建这个，并适当配置**provisioner**和**parameters**部分
- 使用您集群中现有的StorageClasses之一，并在稍后的步骤中更改PVC中的StorageClass名称

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: flash <<==== PVC引用此名称
provisioner: pd.csi.storage.gke.io <<==== GKE Persistent Disk CSI插件
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
parameters: <<==== GKE/GCP特定的设置
  type: pd-ssd

部署StorageClass。如果使用本地Docker桌面集群，请使用**dd-sc.yml**文件。

$ kubectl apply -f gcp-sc.yml
storageclass.storage.k8s.io/flash created

列出集群的StorageClasses，以确保您的StorageClass在列表中。

13: StatefulSets 204

$ kubectl get sc
NAME         PROVISIONER            RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
flash        pd.csi.storage.gke.io  Delete          WaitForFirstConsumer true                   2m

您的StorageClass存在，并且您稍后将使用它来动态创建新卷。

**创建一个主导的无头服务**

用一个头和一个尾来形象化Service对象是有帮助的。_head_是稳定的ClusterIP地址，而tail是它转发流量到的Pod列表。无头服务只是一个没有ClusterIP地址的Service对象。

无头服务的主要目的是为StatefulSet的Pod创建DNS SRV记录。客户端通过DNS查询个别Pod，并直接将查询发送到这些Pod，而不是通过Service的ClusterIP。这就是为什么无头服务没有ClusterIP的原因。

以下YAML来自于**headless-svc.yml**文件，并描述了一个名为**dullahan**的无头服务，没有IP地址(**spec.clusterIP: None**)。

apiVersion: v1
kind: Service <<==== 正常的Kubernetes Service
metadata:
  name: dullahan
  labels:
    app: web
spec:
  ports:
  - port: 80
    name: web
    clusterIP: None <<==== 将其设置为无头服务
  selector:
    app: web

与常规Service的唯一区别是无头服务的**clusterIP**被设置为**None**。
运行以下命令将无头服务部署到您的集群中。

$ kubectl apply -f headless-svc.yml
service/tkb-sts已创建

确保它存在。

13: StatefulSets 205

$ kubectl get svc
名称 类型 CLUSTER-IP EXTERNAL-IP 端口(S) 年龄
dullahan ClusterIP None <none> 80/TCP 11秒

**部署StatefulSet**

既然您有了StorageClass和无头服务，您可以部署StatefulSet。

以下的YAML文件来自于**sts.yml**文件，定义了StatefulSet。

apiVersion: apps/v1
kind: StatefulSet
metadata:
name: tkb-sts
spec:
replicas: 3
selector:
matchLabels:
app: web
serviceName: "dullahan"
template:
metadata:
labels:
app: web
spec:
terminationGracePeriodSeconds: 10
containers:

- name: ctr-web
  image: nginx:latest
  ports:
    - containerPort: 80
      name: web
      volumeMounts:
    - name: webroot
      mountPath: /usr/share/nginx/html
      volumeClaimTemplates:
- metadata:
  name: webroot
  spec:
  accessModes: [ "ReadWriteOnce" ]
  storageClassName: "flash"
  resources:
  requests:
  storage: 10Gi

有很多需要注意的地方，下面我们逐步了解重要的部分。

StatefulSet的名称是**tkb-sts**，将用于命名所有的Pod和关联的卷。

```
13: StatefulSets 206
```

```
Kubernetes将读取spec.replicas字段并创建3个副本，分别称为tkb-sts-0，tkb-
sts-1和tkb-sts-2。它也会按顺序创建它们，并在每个副本运行并准备就绪后再启动下一个。
spec.serviceName字段指定了管理Service。这是您在上一步中创建的无头服务的名称，并将为每个StatefulSet副本创建DNS SRV记录。我们称之为管理Service，因为它负责StatefulSet使用的DNS子域。稍后详细说明。
spec.template部分的其余内容定义了Pod模板。在这里，您定义了要使用的容器镜像和要公开的端口等内容。
最后但绝不是最不重要的是spec.volumeClaimTemplates部分。Kubernetes使用它为每个StatefulSet Pod创建唯一的PVC。由于请求了三个副本，Kubernetes将根据spec.template部分创建三个唯一的Pod和根据spec.volumeClaimTemplates部分创建三个唯一的PVC。它还确保Pod和PVC获得适当的名称以进行关联。
以下YAML显示了示例中的卷声明模板。它定义了一个名为webroot的声明模板，请求从flash StorageClass获取10GB的卷。
```

```
volumeClaimTemplates:
```

- metadata:
  name: webroot
  spec:
  accessModes: [ "ReadWriteOnce" ]
  storageClassName: "flash"
  resources:
  requests:
  storage: 10Gi

```
如果您的集群不在Google Cloud上，并且您正在使用云提供商内置的StorageClasses之一，您需要编辑sts.yml文件并更改storageClassName字段。如果您创建了自己的StorageClass并将其称为flash，那么您将没问题。
运行以下命令来部署StatefulSet。
```

```
$ kubectl apply -f sts.yml
statefulset.apps/tkb-sts已创建
```

观察StatefulSet，直到它升级到三个副本为止。所有三个Pod和关联的PVC创建大约需要一分钟左右的时间。

13: StatefulSets 207

$ kubectl get sts --watch
名称 准备就绪 时间
tkb-sts 0/3 14秒
tkb-sts 1/3 30秒
tkb-sts 2/3 60秒
tkb-sts 3/3 90秒

请注意，启动第一个副本需要30秒的时间。一旦第一个副本运行并准备就绪，启动第二个副本需要另外30秒，第三个副本需要30秒。这是StatefulSet控制器按顺序启动每个副本，并等待它们运行并准备就绪后再启动下一个。

现在，检查PVC。
```
$ kubectl get pvc
名称 状态 卷 容量 模式 存储类 创建时间
webroot-tkb-sts-0 已绑定 pvc-1146...f274 10Gi RWO flash 100s
webroot-tkb-sts-1 已绑定 pvc-3026...6bcb 10Gi RWO flash 70s
webroot-tkb-sts-2 已绑定 pvc-2ce7...e56d 10Gi RWO flash 40s

您有三个新的永久存储卷（PVC），每个都与一个Pod副本同时创建。如果仔细观察，您会发现每个PVC的名称包含了卷索取模板、有状态副本集和关联的Pod副本的名称。

```
卷索取模板名称 Pod名称 PVC名称
webroot tkb-sts-0 webroot-tkb-sts-0
webroot tkb-sts-1 webroot-tkb-sts-1
webroot tkb-sts-2 webroot-tkb-sts-2
```

恭喜，您的有状态副本集正在运行并管理着三个Pod和三个卷。

**测试对等发现**

让我们解释一下DNS主机名和DNS子域是如何与有状态副本集一起工作的。

在Kubernetes中，所有的对象都会在集群地址空间中获得一个名称。您可以在构建集群时指定自定义地址空间，但大多数情况下会使用`cluster.local`的DNS域。在该域内，Kubernetes会按照以下方式构建DNS子域：

- `<对象名称>.<服务名称>.<命名空间>.svc.cluster.local`

当前在默认的命名空间中，由`dullahan`无头服务管理着三个名为`tkb-sts-0`、`tkb-sts-1`和`tkb-sts-2`的Pod。这意味着Pod将具有以下可预测和可靠的完全限定DNS名称：

- `tkb-sts-0.dullahan.default.svc.cluster.local`
- `tkb-sts-1.dullahan.default.svc.cluster.local`
- `tkb-sts-2.dullahan.default.svc.cluster.local`

无头服务的任务是将这些Pod及其IP注册到`dullahan.default.svc.cluster.local`名称下。

您将通过部署一个预先安装了`dig`工具的跳板Pod来测试这一点。然后，您将通过`exec`进入Pod，并使用`dig`查询该服务的SRV记录。

运行以下命令从`jump-pod.yml`文件部署跳板Pod。

$ kubectl apply -f jump-pod.yml
pod/jump-pod 已创建

进入Pod。

$ kubectl exec -it jump-pod -- bash
root@jump-pod:/#

您的终端提示会改变，表示已连接到跳板Pod。从跳板Pod中运行以下`dig`命令。

# dig SRV dullahan.default.svc.cluster.local

<剪切>
;; QUESTION SECTION:
;dullahan.default.svc.cluster.local. IN SRV
;; ANSWER SECTION:
dullahan.default.svc.cluster.local. 30 IN SRV... tkb-sts-1.dullahan.default.svc.cluster.local.
dullahan.default.svc.cluster.local. 30 IN SRV... tkb-sts-0.dullahan.default.svc.cluster.local.
dullahan.default.svc.cluster.local. 30 IN SRV... tkb-sts-2.dullahan.default.svc.cluster.local.
;; ADDITIONAL SECTION:
tkb-sts-0.dullahan.default.svc.cluster.local. 30 IN A 10.60.0.5
tkb-sts-2.dullahan.default.svc.cluster.local. 30 IN A 10.60.1.7
tkb-sts-1.dullahan.default.svc.cluster.local. 30 IN A 10.60.2.12
<剪切>

输出显示，客户端查询`dullahan.default.svc.cluster.local`（QUESTION SECTION）将获取到三个有状态副本集Pod的DNS名称（ANSWER SECTION）和IP（ADDITIONAL SECTION）。为了清楚起见，ANSWER SECTION将`dullahan.default.svc.cluster.local`的请求映射到三个Pod，而ADDITIONAL SECTION将Pod名称映射到IP。

**扩展有状态副本集**
每当Kubernetes扩展一个StatefulSet时，它会创建新的Pod和PVC。然而，在缩小规模时，Kubernetes只会终止Pods。这意味着未来的扩展操作只需要创建新的Pod并将它们连接回原始的PVC。Kubernetes和StatefulSet控制器会在没有您帮助的情况下处理所有这些。

您当前有三个StatefulSet Pods和三个PVCs。编辑**sts.yml**文件，将副本数从3更改为2，并保存更改。完成后，运行以下命令将更新的配置重新发布到集群。如果您仍然登录到jump Pod中，您需要输入**exit**。

$ kubectl apply -f sts.yml
statefulset.apps/tkb-sts已配置

检查StatefulSet并验证Pod计数是否减少到2。

$ kubectl get sts tkb-sts
名称  就绪  年龄
tkb-sts  2/2  12小时

$ kubectl get pods
名称  就绪  状态  重启  年龄
tkb-sts-0  1/1  运行中  0  12小时
tkb-sts-1  1/1  运行中  0  12小时

您已成功将Pod数量缩小为2。如果您仔细观察，您会发现Kubernetes删除了具有最高索引序数的Pod，并且您仍然有3个PVCs。请记住，缩小StatefulSet不会删除PVCs。

进行验证。

$ kubectl get pvc
名称  状态  卷  容量  模式  存储类  年龄
webroot-tkb-sts-0  已绑定  pvc-5955...d71c  10Gi  RWO  flash  12小时
webroot-tkb-sts-1  已绑定  pvc-d62c...v701  10Gi  RWO  flash  12小时
webroot-tkb-sts-2  已绑定  pvc-2e2f...5f95  10Gi  RWO  flash  12小时

尽管**tkb-sts-2** Pod不再存在，但所有三个的状态仍显示为**已绑定**。如果对**webroot-tkb-sts-2** PVC运行**kubectl describe**命令，您会发现**Used by**字段显示为**<none>**。

事实上，仍然存在所有三个PVC意味着缩放回3个副本只需要一个新的Pod。StatefulSet控制器将创建新的Pod并将其连接到现有的PVC。

再次编辑**sts.yml**文件，将副本数增加回3，并保存更改。完成后，运行以下命令将YAML文件重新发布到API服务器。

13: StatefulSets 210

$ kubectl apply -f sts.yml
statefulset.apps/tkb-sts已配置

等待几秒钟以部署新的Pod，并使用以下命令进行验证。

$ kubectl get sts tkb-sts
名称  就绪  年龄
tkb-sts  3/3  12小时

您又回到了3个Pod。描述新的**tkb-sts-2** Pod并验证它是否挂载了**webroot-tkb-sts-2**卷。如果您使用Windows，请将**grep ClaimName**参数替换为**Select-String -Pattern 'ClaimName'**。

$ kubectl describe pod tkb-sts-2 | grep ClaimName
ClaimName: webroot-tkb-sts-2

恭喜您，新的Pod已自动连接到正确的卷。

值得注意的是，如果任何Pod处于失败状态，Kubernetes会暂停缩小规模的操作。这保护了应用程序的弹性和任何数据的完整性。

还可以通过调整其**spec.podManagementPolicy**属性来更改StatefulSet控制器启动和停止Pods的方式。

默认设置为**OrderedReady**，它会强制按顺序启动一个Pod，并在前一个Pod运行并就绪后再启动下一个。将值更改为**Parallel**将使StatefulSet更像一个_Deployment_，其中Pods是并行创建和删除的。例如，从2个扩展到5个Pods将立即创建所有三个新的Pods，而从5个缩小到2个将并行删除三个Pods。StatefulSet命名规则仍然适用，因为此设置仅适用于扩展操作，不影响发布和回滚。

**滚动更新**
有状态副本集（StatefulSets）支持滚动更新（也称为发布）。您可以在YAML文件中更新镜像版本并重新发布到API服务器，控制器将用新的Pod替换旧的Pod。然而，它总是从最高编号的Pod开始，逐个按顺序降低，直到所有Pod都使用新版本。控制器还会等待每个新的Pod运行并准备好后，再将下一个索引最低的Pod替换掉。

有关更多信息，请运行**kubectl explain sts.spec.updateStrategy**命令。

13: 有状态副本集 211

**测试Pod故障**

测试故障的最简单方法是手动删除一个Pod。有状态副本集控制器会注意到观察到的状态与期望的状态不一致，并开始创建一个新的Pod以进行调和。它还会将其连接到相同的PVC和卷。

让我们来测试一下。

确认您的有状态副本集中有三个健康的Pod。

$ kubectl get pods
NAME READY STATUS AGE
tkb-sts-0 1/1 Running 12h
tkb-sts-1 1/1 Running 12h
tkb-sts-2 1/1 Running 9m49s

让我们删除**tkb-sts-0** Pod，看看有状态副本集控制器是否会自动重新创建它。

$ kubectl delete pod tkb-sts-0
pod "tkb-sts-0" deleted

$ kubectl get pods --watch
NAME READY STATUS RESTARTS AGE
tkb-sts-1 1/1 Running 0 12h
tkb-sts-2 1/1 Running 0 12h
tkb-sts-0 0/1 Terminating 0 12h
tkb-sts-0 0/1 Pending 0 0s
tkb-sts-0 0/1 ContainerCreating 0 0s
tkb-sts-0 1/1 Running 0 8s

在命令中使用**--watch**可以让您看到有状态副本集控制器注意到终止的Pod并创建替代品。这是一个干净的故障，有状态副本集控制器立即创建了替代Pod。

您可以看到新的Pod与故障的Pod具有相同的名称，但它是否具有相同的PVC呢？

运行以下命令确认Kubernetes将新的Pod连接到原始的PVC（**webroot-tkb-sts-0**）。如果您使用Windows，请不要忘记将**grep ClaimName**参数替换为**Select-String -Pattern 'ClaimName'**。

$ kubectl describe pod tkb-sts-0 | grep ClaimName
ClaimName: webroot-tkb-sts-0

它起作用了。

13: 有状态副本集 212

从_潜在_节点故障中恢复要复杂得多，这取决于您的Kubernetes版本和设置。现代Kubernetes集群在自动替换来自故障节点的Pod方面表现得更好，而旧版本则需要手动干预。这是为了防止Kubernetes将短暂事件误诊为灾难性的节点故障。

**删除有状态副本集**

在本章前面，您学到了在删除有状态副本集时，Kubernetes不会按顺序终止Pod。因此，如果您的应用程序对有序关闭敏感，应在删除之前将有状态副本集缩放到零。

将您的有状态副本集缩放为0个副本并确认操作。可能需要几秒钟才能完全缩放为0个。

$ kubectl scale sts tkb-sts --replicas=0
statefulset.apps/tkb-sts scaled

$ kubectl get sts tkb-sts
NAME READY AGE
tkb-sts 0/0 13h

一旦达到零个副本，您可以立即删除有状态副本集。

$ kubectl delete sts tkb-sts
statefulset.apps "tkb-sts" deleted

随时可以使用**dig**命令在jump-pod上进行exec并运行另一个**dig**来证明Kubernetes也从集群DNS中删除了SRV记录。

### 清理。

您已经删除了有状态副本集及其Pod。然而，jump Pod、无头Service、卷和StorageClass仍然存在。如果您一直在跟随操作，请使用以下命令进行删除。如果不这样做，将产生意外的云成本。

删除jump Pod。

$ kubectl delete pod jump-pod

删除无头Service。

13: 有状态副本集 213

$ kubectl delete svc dullahan
删除PVCs。这将删除与之相关的PVs和Google Cloud上的后端存储。如果您使用了自己的StorageClass，您应该检查您的存储后端，以确认外部卷也被删除。

$ kubectl delete pvc webroot-tkb-sts-0 webroot-tkb-sts-1 webroot-tkb-sts-2

删除StorageClass。

$ kubectl delete sc flash

### 章节总结

在本章中，您学习了如何使用StatefulSets部署和管理需要持久化数据和状态的应用程序。

StatefulSets能够自我修复，进行水平扩展和缩减，并执行滚动升级。回滚操作需要手动操作。

每个StatefulSet Pod都有一个可预测和持久的名称、DNS主机名和独立的卷。这些名称将在Pod的整个生命周期中保持不变，包括故障、重启、扩展和其他调度操作。事实上，StatefulSet Pod的名称对于扩展操作和将其连接到正确的存储卷至关重要。

最后，StatefulSets只是一个框架。应用程序需要设计和编写以充分利用它们的工作方式。

## 14：API安全与RBAC

Kubernetes是以API为中心的，API通过API服务器提供。在本章中，您将了解典型的API请求经过各种与安全相关的检查的过程。

本章分为以下几个部分：

- API安全的大局观
- 认证
- 授权（RBAC）
- 准入控制

有关API的深入了解，请参阅第15章。

### API安全的大局观

以下所有内容都会向API服务器发出CRUD式的请求（创建、读取、更新、删除）：

- 使用**kubectl**的操作员和开发人员
- Pod
- Kubelet
- 控制平面服务
- 原生于Kubernetes的应用程序

图14.1显示了典型API请求通过标准检查的流程。无论请求的起源是什么，流程都是相同的。

```
图14.1
```

14：API安全与RBAC 215

考虑一个快速示例，其中一个名为**grant-ward**的用户试图在**terran**命名空间中创建一个名为**hive**的Deployment。

用户**grant-ward**使用**kubectl apply**命令在**terran**命名空间中创建**Deployment**。**kubectl**命令行工具会生成一个带有用户凭据的请求发送到API服务器。**kubectl**和API服务器之间的连接由TLS进行安全保护。一旦请求到达API服务器，_身份验证_模块确定请求是否来自**grant-ward**或冒名顶替者。假设是**grant-ward**，则_授权_模块（RBAC）确定**grant-ward**是否具有在**terran**命名空间中创建**Deployment**的权限。如果请求通过了身份验证和授权，_准入控制器_确保Deployment对象符合策略要求。只有在通过身份验证、授权和准入控制检查后，请求才会被执行。

这个过程类似于乘坐商业飞机。您前往机场并使用照片ID（通常是护照）进行身份验证。假设您通过了护照验证，然后您呈现一张授权您登机的机票。如果您通过了身份验证并被授权登机，准入控制可能会检查和应用航空公司的政策，如限制手提行李和禁止机舱内饮酒。经过所有这些步骤，您最终可以就座并飞往目的地。

让我们更详细地了解一下认证。

### 认证

认证是关于证明您的身份。您可能会看到或听到它简称为_authN_，发音为“auth en”。

凭证是认证的核心，**_所有对API服务器的请求都包括凭证_**。认证层的责任是验证这些凭证。如果验证失败，API服务器将返回HTTP 401并拒绝请求。如果验证成功，请求将继续进行授权。

Kubernetes中的认证层是可插拔的，常用的模块包括客户端证书、webhook和与外部身份管理系统（如Active Directory（AD）和基于云的身份访问管理（IAM））集成。实际上，Kubernetes**没有**内置的身份数据库。相反，它强制您使用外部系统。这避免了创建_又一个身份管理孤岛_。

在开箱即用的情况下，大多数Kubernetes集群支持客户端证书，但在实际环境中，您可能希望与所选择的云或企业身份管理系统集成。大多数托管的Kubernetes服务会自动与底层云的身份管理系统集成。
14: API安全性和RBAC 216

**检查您当前的身份验证设置**

您的集群详细信息和用户凭据存储在一个_kubeconfig_文件中。类似于**kubectl**的工具会读取此文件以确定要发送命令的集群和要使用的凭据。该文件通常存储在以下位置：

- Windows：C:\Users\<user>\.kube\config
- Linux/Mac：/home/<user>/.kube/config

下面是一个kubeconfig文件的样子。正如您所见，它定义了一个_cluster_和一个_user_，将它们组合成一个_context_，并为**kubectl**命令设置了默认上下文。输出被剪切以适应页面。

apiVersion: v1
kind: Config
clusters: <<==== 定义一个或多个集群和证书的_cluster_块

- cluster:
  name: prod-shield <<==== 此块定义了一个名为"prod-shield"的集群
  server: https://<url-or-ip-address-of-api-server>:443 <<==== 这是集群的URL
  certificate-authority-data: LS0tLS1C...LS0tCg== <<==== 集群的证书
  users: <<==== 定义一个或多个用户和凭据的_users_块
- name: njfury <<==== 名为njfury的用户
  user:
  as-user-extra: {}
  token: eyJhbGciOiJSUzI1NiIsImtpZCI6IlZwMzl...SZY3uUQ <<==== 用户的凭据
  contexts: <<==== _context_块。上下文是一个集群 + 用户
- context:
  name: shield-admin <<==== 此块定义了一个名为"shield-admin"的上下文
  cluster: prod-shield <<==== 集群
  user: njfury <<==== 用户
  namespace: default
  current-context: shield-admin <<==== kubectl使用的上下文

您可以看到，它分为四个顶层部分：

- 集群
- 用户
- 上下文
- 当前上下文

**集群**部分定义了一个或多个Kubernetes集群。每个集群都有一个友好的名称、一个API服务器端点和其证书颁发机构（CA）的公钥。

**用户**部分定义了一个或多个用户。每个用户都需要一个名称和令牌。令牌通常是由集群的CA（或集群信任的CA）签署的X.509证书。

```
14: API安全性和RBAC 217
```

```
**上下文**部分将用户和集群组合在一起，当前上下文是kubectl将用于所有命令的集群和用户。
假设之前的kubeconfig，所有kubectl命令都将发送到prod-shield集群，并以njfury用户身份进行身份验证。集群上的身份验证模块确定用户是否真正是njfury。
如果您的集群与外部IAM系统集成，它将将身份验证移交给该系统。
假设身份验证成功，请求将进入授权阶段。
```

### 授权（RBAC）..

```
授权在成功身份验证后立即发生，有时会缩写为authZ（发音为“auth zee”）。
Kubernetes的授权是可插拔的，您可以在单个集群上运行多个授权模块。然而，大多数集群使用RBAC。此外，如果您的集群有多个授权模块，一旦任何模块授权请求，它将立即转到准入控制。
本节涵盖以下内容：
```

- RBAC大局观
- 用户和权限
- 集群级别的用户和权限
- 预配置的用户和权限

**RBAC大局观**

```
最常见的授权模块是RBAC（基于角色的访问控制）。在最高级别上，RBAC涉及以下三个方面：
```

```
1.用户
2.操作
3.资源
```

哪些_user_可以对哪些_actions_执行哪些_resources_。

```
下表显示了一些示例。
```

14: API安全性和RBAC 218

```
用户（主体） 操作 资源 效果
Bao create Pods Bao可以创建Pods
Kalila list Deployments Kalila可以列出Deployments
Josh delete ServiceAccounts Josh可以删除ServiceAccounts
```

RBAC在大多数Kubernetes集群上都已启用，并且是一个_least-privilege deny-by-default system_。这意味着一切都被锁定，您需要创建_allow rules_来打开它们。事实上，Kubernetes不支持_deny rules_，它只支持_allow rules_。这可能看起来很小，但它使得Kubernetes RBAC的实施和故障排除变得更简单。

**用户和权限**

理解Kubernetes RBAC至关重要的是两个概念：
- 角色
- 角色绑定

_角色_定义了一组权限，并将它们与用户绑定起来。

以下资源清单定义了一个称为**read-deployments**的角色对象，它授予在**shield**命名空间中获取、监视和列出**Deployment**对象的权限。

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: shield
  name: read-deployments
rules:

- verbs: ["get", "watch", "list"] <<==== 允许的操作
  apiGroups: ["apps"] <<==== 资源所在的API组
  resources: ["deployments"] <<==== 此类型的资源

然而，角色在绑定到用户之前不会起作用。

以下角色绑定将前面的角色绑定到名为**sky**的用户。

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-deployments
  namespace: shield
subjects:

- kind: User
  name: sky <<==== 认证用户的名称
  apiGroup: rbac.authorization.k8s.io
  roleRef:
    kind: Role
    name: read-deployments <<==== 绑定到用户的角色
    apiGroup: rbac.authorization.k8s.io

将这两个对象部署到集群中将允许名为**sky**的用户运行命令，例如**kubectl get deployments -n shield**。

在角色对象中，有以下三个属性定义了允许对哪些对象执行哪些操作：

- 动词
- API组
- 资源

**动词**字段列出了允许的操作，而**API组**和**资源**字段标识了可以执行这些操作的对象。前面的角色YAML的以下片段允许对Deployment对象进行读取访问（**get,watch**和**list**）。

rules:

- verbs: ["get", "watch", "list"]
  apiGroups: ["apps"]
  resources: ["deployments"]

以下表格显示了一些可能的**API组**和**资源**组合。

```
14: API security and RBAC 220
```

```
apiGroup 资源 Kubernetes API路径
”” pods /api/v1/namespaces/{namespace}/pods
”” secrets /api/v1/namespaces/{namespace}/secrets
“storage.k8s.io” storageclass /apis/storage.k8s.io/v1/storageclasses
“apps” deployments /apis/apps/v1/namespaces/{namespace}/deployments
```

```
在apiGroups字段中的空双引号（“”）表示核心API组。您需要将所有其他API组指定为用双引号括起来的字符串。
以下表格列出了Kubernetes支持的完整一组用于对象访问的动词。它还通过将动词映射到标准的HTTP方法和HTTP响应代码，展示了API的基于REST的特性。
```

```
Kubernetes动词 HTTP方法 常见响应
create POST 201 创建，403 访问被拒绝
get, list, watch GET 200 OK，403 访问被拒绝
update PUT 200 OK，403 访问被拒绝
patch PATCH 200 OK，403 访问被拒绝
delete DELETE 200 OK，403 访问被拒绝
```

```
运行以下命令以显示所有API资源和支持的动词。当您构建规则定义时，此输出非常有用。
```

```
$ kubectl api-resources --sort-by name -o wide
NAME APIGROUP KIND VERBS
deployments apps Deployment [create delete ... get list patch update watch]
ingresses networking.k8s.io Ingress [create delete ... get list patch update watch]
pods Pod [create delete ... get list patch update watch]
secrets Secret [create delete ... get list patch update watch]
services Service [create delete get list patch update watch]
<Snip>
```

在构建规则时，您可以使用星号（*）来引用所有API组、所有资源和所有动词。例如，以下规则授予所有API组中的所有资源上的所有动作。这只是为了演示目的，您可能不应该创建这样的规则。

```
rules:
```

- verbs: ["*"]
  resources: ["*"]
  apiGroups: ["*"]

**集群级用户和权限**

```
到目前为止，您已经看到了角色和角色绑定。然而，Kubernetes还有四个RBAC对象：
```

14: API security and RBAC 221

- 角色
- 角色绑定
- 集群角色
- 集群角色绑定
角色和角色绑定是有命名空间的对象。这意味着您将它们应用于特定的命名空间。另一方面，集群角色和集群角色绑定是全局对象，适用于所有命名空间。这四个对象都定义在同一个API子组中，它们的YAML结构几乎相同。

一个强大的模式是使用集群角色来定义集群级别的角色，然后使用角色绑定将它们绑定到特定的命名空间。这样可以在特定的命名空间中定义一次常见的角色，然后在不同的命名空间中重用，如图14.2所示。

```
图14.2-组合集群角色和角色绑定
```

下面的YAML定义了之前的**read-deployments**角色，但这次是在集群级别上。然后，您可以通过角色绑定在选择的命名空间中使用它 - 每个命名空间一个角色绑定。

14：API安全性和RBAC 222

apiVersion：rbac.authorization.k8s.io/v1
kind：ClusterRole <<==== 集群范围的角色
metadata：
name：read-deployments
rules：

- verbs：["get", "watch", "list"]
  apiGroups：["apps"]
  resources：["deployments"]

如果您仔细观察YAML，与之前的区别仅在于这个YAML的**kind**属性设置为ClusterRole，并且没有**metadata.namespace**属性。

**预创建的用户和权限**

大多数集群都有预创建的角色和绑定，以帮助进行初始配置和入门。

下面的示例显示了Docker Desktop的Kubernetes集群如何使用集群角色和集群角色绑定将集群管理员权限授予kubeconfig文件中配置的用户。如果您使用我们在第3章中展示如何构建的Docker Desktop Kubernetes集群，您可以跟随操作。其他集群可能会有略微不同的操作方式，但原则是相似的，这个示例将给您一个大致的工作原理。

Docker Desktop使用一个使用客户端证书与Kubernetes进行身份验证的管理员用户来配置您的kubeconfig文件。

运行以下命令查看kubeconfig文件中的用户条目。输出已经被修剪。

$ kubectl config view
<Snip>
users：

- name：docker-desktop
  user：
  client-certificate-data：DATA+OMITTED
  client-key-data：DATA+OMITTED
  <Snip>

用户条目名为**docker-desktop**。然而，这不是**kubectl**在与Kubernetes进行身份验证时使用的用户名。**kubectl**使用的用户名嵌入在客户端证书中。

运行以下长命令解码嵌入在kubeconfig文件中的客户端证书中的用户名和群组成员身份。该命令仅适用于类似Linux系统，并且需要安装**jq**实用程序。您还需要确保kubeconfig的当前上下文设置为您的Docker Desktop集群。

$ kubectl config view --raw -o json \
| jq ".users[] | select(.name==\"docker-desktop\")" \
| jq -r '.user["client-certificate-data"]' \
| base64 -d | openssl x509 -text | grep "Subject:"

```
Subject: O = system:masters, CN = docker-for-desktop
```

输出显示**kubectl**命令将作为属于**system:masters**组的**docker-for-desktop**用户进行身份验证。证书由集群的CA签名。

```
注意：kubeconfig文件中的CN属性列出了用户，O属性列出了群组。
```

让我们把注意力转向集群端，看看Kubernetes如何使用集群角色和集群角色绑定为**docker-for-desktop**用户在集群上授予权限。请记住，**docker-for-desktop**用户是**system:masters**组的成员。

运行以下命令查看内置的**cluster-admin**集群角色具有哪些访问权限。

$ kubectl describe clusterrole cluster-admin
Name: cluster-admin
Labels: kubernetes.io/bootstrapping=rbac-defaults
Annotations: rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
Resources Non-Resource URLs Resource Names Verbs

--------- ----------------- -------------- -----

*.* [] [] [*]
[*] [] [*]

PolicyRule部分显示此角色对所有命名空间中的所有资源的所有操作具有访问权限。这相当于root，是一组强大而危险的权限。

运行以下命令查看集群角色是否被任何集群角色绑定引用。

14：API安全性和RBAC 224
$ kubectl get clusterrolebindings | grep cluster-admin
名称 角色
cluster-admin ClusterRole/cluster-admin

**cluster-admin** ClusterRole与同名的ClusterRoleBinding绑定。

如果您描述**cluster-admin** ClusterRoleBinding，您会看到它映射到所有属于**system:masters**组的用户。

$ kubectl describe clusterrolebindings cluster-admin
名称: cluster-admin
标签: kubernetes.io/bootstrapping=rbac-defaults
注释: rbac.authorization.kubernetes.io/autoupdate: true
角色:
类型: ClusterRole
名称: cluster-admin
主体:
类型 名称 命名空间

---- ---- ---------

Group system:masters <<==== 绑定到该组的已认证成员

这是很多信息，下面的总结或许会有所帮助。

如图14.3所示，Docker Desktop使用集群的证书颁发机构（CA）对您的kubeconfig文件进行配置，并签署了一个客户端证书。该证书标识了一个名为**docker-for-desktop**的用户，该用户属于**system:masters**组。Docker Desktop Kubernetes集群有一个名为**cluster-admin**的ClusterRoleBinding，它将已认证为**system:masters**组成员的用户绑定到一个也叫**cluster-admin**的ClusterRole。该**cluster-admin** ClusterRole对所有命名空间中的所有对象具有管理员权限。

14: API安全和RBAC 225

```
图14.3-将kubectl用户映射到cluster-admin
```

**授权总结**

授权确保已认证的用户被允许执行操作。RBAC是一个流行的Kubernetes授权模块，它基于拒绝为默认的模型实现了最小特权访问，除非您创建了一个允许操作的规则。

Kubernetes RBAC使用Roles和ClusterRoles来创建权限，并使用RoleBindings和ClusterRoleBindings将这些权限授予用户。

一旦请求通过身份验证和授权，它就会移动到准入控制。

### 准入控制

准入控制在成功的身份验证和授权后立即运行，它与_策略_有关。

Kubernetes支持两种类型的准入控制器：

- 修改型
- 验证型

名称已经告诉了你很多。_修改型_控制器检查合规性并可以修改请求，而_验证型_控制器只能检查合规性但不能修改请求。

14: API安全和RBAC 226

修改型控制器始终先运行，而两种类型仅适用于试图修改集群状态的请求。读取请求不受准入控制的约束。

举个快速的例子，您可能有一个生产集群，其中一个策略是所有新建和更新的对象必须具有**env=prod**标签。修改型控制器可以检查新建和更新的对象是否有该标签，并在不存在时添加该标签。然而，验证型控制器只能在标签不存在时拒绝请求。

下面的命令在Docker Desktop集群上显示API服务器配置为使用**NodeRestriction**准入控制器。

$ kubectl describe pod kube-apiserver-docker-desktop \
--namespace kube-system | grep admission

--enable-admission-plugins=NodeRestriction

大多数真实世界的集群将运行更多的准入控制器。**AlwaysPullImages**准入控制器就是一个很好的例子。它是一个修改型控制器，将所有新建Pod的**spec.containers.imagePullPolicy**设置为**Always**。这样可以防止Pod使用本地缓存的镜像，并强制从注册表中拉取所有镜像。这要求所有节点具有拉取镜像的有效凭据。

如果任何准入控制器拒绝请求，则该请求将立即被拒绝，而不会检查其他准入控制器。这意味着在请求在集群上运行之前，所有准入控制器都必须批准该请求。

如前所述，有很多准入控制器，并且它们在真实世界的生产集群中变得越来越重要。

### 章节总结

在本章中，您了解到所有对API服务器的请求都包含凭证，并且必须通过身份验证、授权，然后再经过准入控制检查。客户端与API服务器之间的连接也是通过TLS进行安全保护。

身份验证层验证请求的身份，大多数集群支持客户端证书。然而，生产集群应该使用企业级身份和访问管理（IAM）解决方案。

授权层检查已认证用户是否有执行特定操作的权限。该层也是可插拔的，最常见的授权模块是RBAC。RBAC包括四个对象，让您定义权限并将其分配给用户。

准入控制器在授权后发挥作用，负责执行策略。验证型准入控制器拒绝不符合策略要求的请求，而修改型准入控制器可以修改请求以符合策略要求。

14: API安全和RBAC 227
## 15：Kubernetes API

要掌握Kubernetes，您需要了解API及其工作原理。然而，它非常庞大和复杂，如果您对API和“RESTful”等术语感到陌生，可能会感到困惑。如果您有这种情况，本章将消除困惑，并使您快速了解Kubernetes API的基础知识。

本章内容如下：

- Kubernetes API的全貌
- API服务器
- API

在开始之前，让我们简单提及一些事情。

我在本章中包含了很多行话，这样您就能熟悉它们。

我强烈建议您完成实践部分，因为这将有助于巩固理论知识。

最后，Pods、Services、StatefulSets、StorageClasses等都是API中的“资源”。然而，在部署到集群后，通常将它们称为“对象”。我们将使用“资源”和“对象”这两个术语互换使用。

### Kubernetes API的全貌。。

Kubernetes是以API为中心的 - 所有的资源都在API中定义，所有的通信都经过API服务器。

管理员和客户端发送请求来创建、读取、更新和删除Pods和Services等对象。在大多数情况下，您将使用**kubectl**来发送这些请求。然而，您也可以在代码中编写请求或通过API测试和开发工具来生成请求。重点是，无论您如何生成请求，它们始终会发送到API服务器进行身份验证和授权。如果通过了身份验证测试，它们将在集群上执行。如果是创建请求，对象将部署到集群中，并以序列化的状态持久化到集群存储中。

图15.1展示了这个高级过程，并突出了API和API服务器的核心地位。

```
15: Kubernetes API 229
```

```
图15.1
```

```
让我们开始解释一些行话。
```

**JSON序列化**

将对象以其序列化状态持久化到集群存储中意味着什么？

```
序列化是将对象转换为字符串或字节流的过程，以便可以通过网络发送并持久化到数据存储中。将字符串或字节流转换为对象的过程称为反序列化。
Kubernetes将对象（如Pods和Services）序列化为JSON字符串，并通过HTTP在网络上发送。这个过程在两个方向上进行：
```

- 客户端（如**kubectl**）在将对象发送到API服务器时对其进行序列化
- API服务器将响应序列化回客户端

```
除了将对象序列化以在网络上传输，Kubernetes还将其序列化以存储在集群存储中。
然而，除了JSON，Kubernetes还支持Protobuf作为序列化模式。与JSON相比，Protobuf更快、更高效，而且扩展性更好。但是，当涉及内省和故障排除时，它不如JSON用户友好。在撰写本文时，Kubernetes通常使用JSON与外部客户端通信，使用Protobuf处理内部集群流量。
关于序列化的最后一点。当客户端向API服务器发送请求时，它们使用Content-Type头来列出它们支持的序列化模式。例如，只支持JSON的客户端将在所有请求的HTTP头中指定Content-Type: application/json。Kubernetes将用JSON进行序列化回应以满足这个要求。
在一些示例中您将看到这一点。
```

```
15: Kubernetes API 230
```

**API类比**

考虑一个可能帮助您理解Kubernetes API的快速类比。

亚马逊销售各种商品：

```
1.这些商品存储在仓库中，并在亚马逊网站上展示
2.您使用浏览器和应用程序等工具搜索网站并购买商品
3.第三方通过亚马逊出售自己的商品，您使用相同的浏览器和网站
4.当您通过网站购买商品时，商品会被送到您那里，您可以开始使用它
5.亚马逊网站可以让您跟踪商品在准备和配送过程中的情况
6.一旦商品送达，您可以继续使用亚马逊来订购更多商品或退回商品
```

嗯，Kubernetes和这很相似。

```
Kubernetes有很多资源（商品），如Pods、Services和Ingresses：
```
```
1. 这些资源在API中定义，并通过API服务器公开
2. 您可以使用kubectl等工具与API服务器通信并请求资源
3. 第三方甚至可以在Kubernetes中定义自己的资源，您可以使用相同的kubectl和API服务器来请求它们
4. 当您通过API服务器请求资源时，它将在您的集群上创建，并且您可以开始使用它
5. API服务器允许您观察其创建过程
6. 创建完成后，您可以使用API服务器创建更多的资源，甚至删除内容
```

```
图15.2显示了比较情况，您可以在下表中看到一对一的特性比较。然而，请记住这只是一个类比，并不是所有的东西都完全匹配。
```

15: Kubernetes API 231

```
图15.2
```

```
亚马逊Kubernetes
物品 资源/对象
仓库 API
浏览器 kubectl
亚马逊网站 API服务器
```

简要总结一下。所有可部署的对象，如Pod、Service、Ingress等，都在API中定义为资源。如果API中不存在某个对象，您无法部署它。与亚马逊一样，您只能购买网站上列出的物品。

API资源具有可检查和配置的属性。例如，当部署Pod时，您可以配置以下所有属性（它们比我们展示的更多）：

- metadata（名称、标签、命名空间、注释等）
- 重启策略
- 服务账户名称
- 运行时类
- 容器
- 卷

这与在亚马逊购物相同。例如，购买USB数据线时，您可以配置选项，如USB类型、线缆长度，甚至线缆颜色。

要部署Pod，您需要将Pod的YAML文件发送到API服务器。假设YAML文件有效，并且您有权限创建Pod，它将被部署到集群中。之后，您可以查询API服务器以获取其当前状态。当需要删除它时，您发送删除请求给API服务器。

这与在亚马逊购买物品相同。要购买先前提到的USB数据线，您需要输入所有颜色、长度和类型的选项，并将它们提交给亚马逊网站。假设有库存并且您提供了资金，它将被发货给您。之后，您可以使用网站跟踪货物。如果您需要退货或投诉，您也可以通过亚马逊网站完成所有这些事务。

类比已经足够了。让我们更详细地了解API服务器。

### API服务器

API服务器通过RESTful HTTPS接口公开API。它充当API的前端，并且有点像Kubernetes的"大中央车站"，通过REST API调用与其他所有事物进行通信。例如：

- 所有**kubectl**命令都发送到API服务器（创建、检索、更新和删除对象）
- 所有kubelet都会监视API服务器以获取新任务，并向API服务器报告状态
- 所有控制平面服务都通过API服务器相互通信

让我们深入挖掘并解释更多术语。

API服务器是Kubernetes控制平面服务，一些集群将其作为**kube-system**命名空间中一组Pod来运行。如果您构建和管理自己的集群，您需要确保控制平面具有高可用性，并具备足够的性能以确保API服务器能够快速响应请求。如果您使用的是托管的Kubernetes，API服务器的实现，包括性能和可用性，将被隐藏。

API服务器的主要工作是将API暴露给集群内外的客户端。它使用TLS来加密客户端连接，并利用身份验证和授权机制来确保只有有效的请求被接受和执行。内部和外部来源的请求都必须通过相同的身份验证和授权。

API是RESTful的。这是现代Web API的术语，通过标准的HTTP方法接受CRUD风格的请求。CRUD风格操作是简单的创建、读取、更新、删除操作，它们映射到标准的POST、GET、PUT、PATCH和DELETE HTTP方法。

下表显示了CRUD操作、HTTP方法和**kubectl**命令的对应关系。如果您已经阅读了有关API安全性的章节，您将知道我们使用"动词"一词来指代CRUD操作。

15: Kubernetes API 233
```
如您所见，CRUD动词名称、方法名称和kubectl子命令名称并不总是匹配的。例如，kubectl edit命令使用了update CRUD动词，并发送了一个HTTP PATCH请求。

API服务器通常暴露在443端口或6443端口上，但您可以配置它在任何您需要的端口上运行。

运行以下命令查看您的Kubernetes集群所暴露的地址和端口。

$ kubectl cluster-info
Kubernetes控制平面运行在https://kubernetes.docker.internal:6443
CoreDNS运行在https://kubernetes.docker.internal:6443/api/v1...

对于REST和RESTful，有几点需要注意。

您会经常听到REST和RESTful这两个术语。REST是表现状态转移（Representational State Transfer）的缩写，是与基于Web的API通信的事实标准。使用REST的系统，例如Kubernetes，通常被称为RESTful。

REST请求由动词和资源路径组成。动词与操作相关，并映射到前面表格中看到的标准HTTP方法。路径是API中资源的URI路径。

术语说明：我们经常使用动词这个词来指代CRUD操作以及HTTP方法。基本上，每当我们说动词时，我们指的是一个动作。

以下示例展示了一个kubectl命令和相关的REST请求，用于列出shield命名空间中的所有Pod。kubectl命令将命令转换为REST请求，注意REST请求具有我们刚刚提到的动词和路径。

$ kubectl get pods --namespace shield

GET /api/v1/namespaces/shield/pods

运行以下命令启动一个kubectl代理会话。这将在您的本地适配器上公开API，并处理所有身份验证。您可以自由选择使用其他端口。

$ kubectl proxy --port 9000 &
[1] 27533
Starting to serve on 127.0.0.1:9000

代理运行后，您可以使用类似curl的工具向API服务器发送请求。

运行以下命令列出shield命名空间中的所有Pod。该命令发出一个HTTP GET请求，URI是指向shield命名空间中Pod的路径。

$ curl -X GET http://localhost:9000/api/v1/namespaces/shield/pods
{
"kind": "PodList",
"apiVersion": "v1",
"metadata": {
"resourceVersion": "9524"
},
"items": []
}

由于shield命名空间中没有Pod，所以该命令返回一个空列表。尝试另一个请求以列出所有命名空间。

$ curl -X GET http://localhost:9000/api/v1/namespaces
{
"kind": "NamespaceList",
"apiVersion": "v1",
"metadata": {
"resourceVersion": "9541"
},
"items": [
{
"metadata": {
"name": "kube-system",
"uid": "f5d39dd2-ccfe-4523-b634-f48ba3135663",
"resourceVersion": "10",
<Snip>
}

正如您在本章前面学到的，Kubernetes使用JSON作为首选的序列化模式。这意味着之前的kubectl get pods --namespace shield命令将生成一个content type设置为application/json的请求。它将得到HTTP200(OK)响应代码，并且Kubernetes将以序列化的JSON列表形式响应shield命名空间中所有Pod的请求。

再次运行之前的curl命令之一，但添加-v标志以查看发送和接收的头部信息。以下示例已经修剪以适应本书，并将您的注意力引向最重要的部分。

$ curl -v -X GET [http://localhost:9000/api/v1/namespaces/shield/pods](http://localhost:9000/api/v1/namespaces/shield/pods)

> GET /api/v1/namespaces/shield/pods HTTP/1.1 <<==== HTTP GET方法对应Pod的REST路径
> Accept: */* <<==== 接受所有序列化模式
<限制>
请根据英文内容直接翻译，维持原有的格式，不省略任何信息。

<翻译>
< HTTP/1.1 200 OK <<==== 接受请求并开始响应
< Content-Type: application/json <<==== 使用JSON序列化进行响应
< X-Kubernetes-Pf-Flowschema-Uid: d50...
< X-Kubernetes-Pf-Prioritylevel-Uid: 828...
<
{ <<==== 响应开始（序列化对象）
"kind": "PodList",
"apiVersion": "v1",
"metadata": {
"resourceVersion": "34217"
},
"items": []
}

以**>**开头的行是由**curl**发送的请求头数据。以**<**开头的行是API服务器返回的响应头数据。

**>**行显示**curl**向**/api/v1/namespaces/shield/pods** REST路径发送GET请求，并告知API服务器可以使用任何有效的序列化模式（Accept: */*）进行响应。以**<**开头的行显示API服务器返回HTTP响应代码并使用JSON进行响应。**X-Kubernetes**行是Kubernetes特定的优先级和公平性设置。

**关于CRUD的一点说明**

CRUD是用于操作和持久化对象的Web API使用的四个基本函数的首字母缩写——**C**reate（创建）、**R**ead（读取）、**U**pdate（更新）、**D**elete（删除）。正如前面提到的，Kubernetes API通过常见的HTTP方法公开和实现了CRUD风格的操作。

让我们来看一个例子。

下面的JSON来自本书GitHub存储库的**api**文件夹中的**ns.json**文件。它定义了一个名为**shield**的新命名空间对象。

```
15: The Kubernetes API 236
```

```
{
"kind": "Namespace",
"apiVersion": "v1",
"metadata": {
"name": "shield",
"labels": {
"chapter": "api"
}
}
}
```

```
您可以使用kubectl apply -f ns.json命令创建它，但不要这样做。您将在稍后的步骤中创建它。
在幕后，kubectl将使用HTTP POST方法向API服务器发出请求。这就是为什么您偶尔会听到人们提到POST到API服务器。POST方法创建指定资源类型的新对象。在这个例子中，它将创建一个名为shield的新命名空间。
以下是请求头的简化示例。请求体将是JSON文件的内容。
请求头：
```

```
POST https://<api-server>/api/v1/namespaces
Content-Type: application/json
Accept: application/json
```

```
如果请求成功，响应将包括标准的HTTP响应代码、内容类型和实际有效载荷。
```

```
HTTP/1.1 200 (OK)
Content-Type: application/json
{
...
}
```

运行以下**curl**命令将**ns.json**文件发送到API服务器。它依赖于您之前仍在运行的**kubectl proxy**进程（ **kubectl proxy --port 9000 &** ），并且您需要从包含**ns.json**文件的目录运行该命令。如果**shield**命名空间已经存在，则需要在继续之前将其删除。
Windows用户需要将反斜杠替换为反引号，并在@符号之前立即放置一个反引号。

15: The Kubernetes API 237

$ curl -X POST -H "Content-Type: application/json" \
--data-binary @ns.json [http://localhost:9000/api/v1/namespaces](http://localhost:9000/api/v1/namespaces)

{
"kind": "Namespace",
"apiVersion": "v1",
"metadata": {
"name": "shield",
<Snip>

**-X POST**参数强制**curl**使用HTTP POST方法。**-H "Content-Type..."**告诉API服务器请求包含序列化的JSON。**--data-binary @ns.json**指定清单文件，URI是API服务器由**kubectl proxy**公开的地址，包括REST路径。

您可以通过运行**kubectl get namespaces**命令来验证是否创建了新的命名空间。

名称 状态 年龄
kube-system 活动 47小时
kube-public 活动 47小时
kube-node-lease 活动 47小时
default 活动 47小时
shield 活动 14秒

现在，通过运行指定DELETE HTTP方法的curl命令来删除命名空间。
$ curl -X DELETE \
-H "Content-Type: application/json" [http://localhost:9000/api/v1/namespaces/shield](http://localhost:9000/api/v1/namespaces/shield)
{
"kind": "Namespace",
"apiVersion": "v1",
"metadata": {
"name": "shield",
<Snip>
},
"spec": {
"finalizers": [
"kubernetes"
]
},
"status": {
"phase": "Terminating"
}
}

简而言之，API服务器通过安全的RESTful接口公开API，使您能够操作和查询集群上对象的状态。它运行在控制平面上，需要具备高可用性和足够的性能以快速处理请求。

### API

API是定义所有Kubernetes资源的地方。它是庞大、模块化和RESTful的。

当最初创建Kubernetes时，API是单体的，所有资源存在于一个全局命名空间中。然而，随着Kubernetes的增长，我们将API拆分为更小、更易管理的组。

图15.3显示了API的简化视图，将资源分成了多个组。

图15.3-简化的Kubernetes API视图

该图显示了API有四个组。实际上有更多的组，但为了简单起见，图中只显示了四个。

API组有两种类型：

- 核心组
- 命名组

核心API组

核心组中的资源是在Kubernetes早期创建API并在API被拆分为多个组之前就存在的成熟对象。它们往往是基本对象，如Pods、Nodes、Services、Secrets和ServiceAccounts。它们位于位于`/api/v1` REST路径下的API中。以下表格列出了核心组中一些资源的示例路径。

资源 REST路径
Pods /api/v1/namespaces/{namespace}/pods/
Services /api/v1/namespaces/{namespace}/services/
Nodes /api/v1/nodes/
Namespaces /api/v1/namespaces/

注意，有些对象是有命名空间的，有些则没有。有命名空间的对象的REST路径较长，因为您需要包含两个额外的片段-`../namespaces/{namespace}/..`。例如，列出**shield**命名空间中的所有Pods需要以下路径。

GET /api/v1/namespaces/shield/pods/

对于读取请求，预期的HTTP响应代码是**200:OK**或**401:Unauthorized**。

关于REST路径，GVR代表group、version和resource，可以作为记住API REST路径结构的好方法。图15.4显示了一个简单的例子，但有命名空间的对象的路径更长。

图15.4

您不应该期望任何新的资源被添加到核心组中。我们总是将新的资源添加到命名组中。

命名API组

命名API组是API的未来，所有新的资源都会被添加到命名组中。有时，我们将其称为子组。

每个命名组都是一组相关资源的集合。例如，**apps**组定义了管理应用工作负载的资源，如Deployments、StatefulSets和DaemonSets。同样，我们在**networking.k8s.io**组中定义Ingresses、Ingress Classes和Network Policies。这种模式的显著例外是在命名组出现之前就存在的核心组中的旧资源。例如，Pods和Services都属于核心组。然而，如果我们今天发明它们，可能会将Pods放在**apps**组中，将Services放在**networking.k8s.io**组中。

命名组中的资源位于`/apis/{group-name}/{version}/` REST路径下。以下表格列出了一些示例。

资源路径
Ingress /apis/networking.k8s.io/v1/namespaces/{namespace}/ingresses/
ClusterRole /apis/rbac.authorization.k8s.io/v1/clusterroles/
StorageClass /apis/storage.k8s.io/v1/storageclasses/
注意到命名组的URI路径以**/apis**（复数形式）开头，并包含组的名称。这与核心组不同，核心组以**/api**（单数形式）开头，并不包含组名称。事实上，在一些地方，你会看到核心API组被空的双引号（""）所指代。这是因为在最初设计API时没有考虑到组的问题 - 一切都是“只是在API中”。

将API分成较小的组使其更具可扩展性和易于导航和扩展。

**检查API**

以下命令是查看集群中与API相关信息的好方法。

**kubectl api-resources**命令列出了集群支持的所有API资源和组。它还显示资源的短名称以及它们是否命名空间或集群范围。输出已进行了调整以适应本书，并显示了来自不同组的资源的混合。

$ kubectl api-resources
名称 短名称 API版本 是否命名空间 类型
namespaces ns v1 false Namespace
nodes no v1 false Node
pods po v1 true Pod
deployments deploy apps/v1 true Deployment
replicasets rs apps/v1 true ReplicaSet
statefulsets sts apps/v1 true StatefulSet
cronjobs cj batch/v1 true CronJob
jobs batch/v1 true Job
ingresses ing networking.k8s.io/v1 true Ingress
networkpolicies netpol networking.k8s.io/v1 true NetworkPolicy
storageclasses sc storage.k8s.io/v1 false StorageClass

下一个命令显示了集群支持的API版本。它不列出哪些资源属于哪些API，但可以用于查看集群是否启用了**alpha** API等信息。注意，一些API组具有多个启用的版本，例如beta和稳定版，或v1和v2。

15: Kubernetes API 241

$ kubectl api-versions
admissionregistration.k8s.io/v1
apiextensions.k8s.io/v1
apps/v1
<Snip>
autoscaling/v1
autoscaling/v2
v1

下一个命令更复杂，只列出了支持的资源的**kind**和**version**字段。输出已被修剪以给您一个大致的概念。它不适用于Windows。

$ for kind in `kubectl api-resources | tail +2 | awk '{ print $1 }'`; \
do kubectl explain $kind; done | grep -e "KIND:" -e "VERSION:"

KIND: Binding
VERSION: v1
KIND: ComponentStatus
VERSION: v1
<Snip>
KIND: HorizontalPodAutoscaler
VERSION: autoscaling/v2
KIND: CronJob
VERSION: batch/v1
KIND: Job
VERSION: batch/v1
<Snip>

如果您仍然保留着之前的**kubectl proxy**会话，可以运行以下命令。

运行以下命令列出**core** API组下可用的所有API版本。

$ curl [http://localhost:9000/api](http://localhost:9000/api)
{
"kind": "APIVersions",
"versions": [
"v1"
],
"serverAddressByClientCIDRs": [
{
"clientCIDR": "0.0.0.0/0",
"serverAddress": "172.21.0.4:6443"
}
]
}

运行此命令列出所有命名的API和组。输出已被修剪以节省空间。

15: Kubernetes API 242

$ curl [http://localhost:9000/apis](http://localhost:9000/apis)
{
"kind": "APIGroupList",
"apiVersion": "v1",
"groups": [
<Snip>
{
"name": "apps",
"versions": [
{
"groupVersion": "apps/v1",
"version": "v1"
}
],
"preferredVersion": {
"groupVersion": "apps/v1",
"version": "v1"
}
},
<Snip>

您可以列出集群上特定对象实例或对象列表。以下命令返回集群上所有命名空间的列表。
$ curl [http://localhost:9000/api/v1/namespaces](http://localhost:9000/api/v1/namespaces)
{
"kind": "NamespaceList",
"apiVersion": "v1",
"metadata": {
"resourceVersion": "35234"
},
"items": [
{
"metadata": {
"name": "kube-system",
"uid": "05fefa13-cbec-458b-aece-d65eb1972dfb",
"resourceVersion": "4",
"creationTimestamp": "2021-12-29T12:32:48Z",
"labels": {
"kubernetes.io/metadata.name": "kube-system"
},
"managedFields": [
{
"manager": "Go-http-client",
"operation": "Update",
"apiVersion": "v1",
<Snip>

您可以随意尝试。您可以将相同的URI路径输入到浏览器和API工具（如Postman）中。

15：Kubernetes API 243

保持**kubectl proxy**进程运行，因为您将在本章后面再次使用它。

**Alpha、Beta和稳定版**

Kubernetes对接受新API资源有严格的过程。新资源以_alpha_形式出现，经过_beta_进展，并最终毕业为_Generally Available (GA)_。我们有时将GA称为_稳定版_。

**Alpha**资源是实验性的，应被视为_不稳定和有风险_。预计会有错误，预计会有功能在没有警告的情况下被删除，并且预计很多东西在转入beta时会发生变化。许多集群默认关闭alpha API。

经历两个alpha版本的**apps** API组中称为**xyz**的新资源将具有以下API名称：

- /apis/apps/ **v1alpha1** /xyz
- /apis/apps/ **v1alpha2** /xyz

alpha之后的阶段是beta。

**Beta**资源被视为_预发布_，并开始看起来像最终的GA产品。但是，在升级为GA时，您应该预期会有一些小的变化。大多数集群默认启用beta API，并且一些人在生产中使用beta资源。但是，这并不是建议，您需要自行决定。

在**apps** API组中经历两个beta版本的相同**xyz**资源将通过以下API提供：

- /apis/apps/ **v1beta1** /xyz
- /apis/apps/ **v1beta2** /xyz

beta之后的最终阶段是_Generally Available (GA)_，有时也称为_稳定版_。

**GA**资源被认为是可用于生产环境的，并且Kubernetes对其有强烈的长期承诺。

大多数GA资源是**v1**。但是，有些资源继续演进并升级到**v2**。如果您想创建一个资源的**v2**版本，您必须将其置于相同的孵化和毕业过程中。例如，**apps** API中的**xyz**资源在到达**v2**之前会经历相同的alpha和beta过程。

- /apis/apps/ **v2alpha1** /xyz
- <Snip>
- /apis/apps/ **v2beta1** /xyz

15：Kubernetes API 244

- <Snip>
- /apis/apps/ **v2** /xyz

稳定资源的路径示例包括以下内容：

- /apis/networking.k8s.io/ **v1** /ingresses
- /apis/batch/ **v1** /cronjobs
- /apis/autoscaling/ **v2** /horizontalpodautoscalers

通常，您可以通过一个API部署对象，然后通过更近期的API读取和管理它。例如，您可以通过**v1beta2** API部署对象，然后通过稳定的**v1** API更新和管理它。

**资源废弃**

如前一节所述，alpha和beta对象在升级为GA之前会经历许多变化。但是，一旦对象变为GA，它就不会变化，并且Kubernetes坚决致力于长期可用性和支持。

目前，Kubernetes对beta和GA资源有以下承诺：

- **Beta：**处于beta状态的资源有9个月的窗口期来发布新版本或毕业为GA。这是为了防止资源永久停留在beta状态。例如，Ingress资源在超过15个Kubernetes版本中仍然处于beta状态！
- **GA：**GA资源预计具有长期寿命。在被废弃时，Kubernetes将继续为GA对象提供服务和支持，时间为12个月或三个版本，以较长者为准。
最近的Kubernetes版本在使用被弃用资源时返回弃用警告信息。例如，从旧的**extension-s/v1beta1** API部署Ingress会导致以下弃用警告，而**v1beta1** API已被弃用。

$ kubectl apply -f deprecate.yml
警告：extensions/v1beta1 Ingress在v1.14+中已被弃用，在v1.22+中不可用；请使用networking.k8s.io/v1 Ingress

15：Kubernetes API 245

**扩展API**

Kubernetes附带了一系列内置控制器，用于部署和管理内置资源。但是，您可以通过添加自己的资源和控制器来扩展Kubernetes。

第三方扩展Kubernetes API的一个示例是存储空间，其中供应商通过Kubernetes API中的自定义资源公开高级功能，例如快照计划。在这种模型中，存储通过CSI驱动程序在Kubernetes中提供，Pod通过内置的Kubernetes资源（如StorageClasses和PersistentVolumeClaims）消耗存储，但是可以通过自定义API资源和控制器来管理快照计划等高级功能。这样做的原因是可以通过**kubectl**和常规的YAML文件等在Kubernetes中部署和管理自定义功能。

扩展API的高级模式涉及两个主要内容：

- 创建自定义资源
- 编写自定义控制器

Kubernetes有一个CustomResourceDefinition（CRD）对象，可以让您在API中创建看起来和感觉像原生Kubernetes资源的新资源。您可以将自定义资源创建为CRD，然后使用**kubectl**创建实例并像处理原生资源一样检查它们。您的自定义资源甚至可以在API中拥有自己的REST路径。

以下YAML来自书籍GitHub存储库中的**api**文件夹中的**crd.yml**文件。它在**nigelpoulton.com** API组中通过**v1**路径定义了一个新的集群范围的自定义资源**books**。

apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
name: books.nigelpoulton.com
spec:
group: nigelpoulton.com <<==== 命名的API组
scope: Cluster <<==== 可以是"Namespaced"或"Cluster"
names:
plural: books <<==== 所有资源需要一个复数和一个单数名字
singular: book <<==== 单数名字用于CLI和命令输出
kind: Book <<==== 用于YAML文件的kind属性
shortNames:

- bk <<==== kubectl使用的短名称
  versions: <<==== 资源可以通过多个API版本提供
- name: v1
  served: true <<==== 如果设置为false，将不会提供"v1"
  storage: true <<==== 将对象实例存储为此版本

15：Kubernetes API 246

```
schema: <<==== 此块定义资源的属性
openAPIV3Schema:
type: object
properties:
spec:
type: object
properties:
<Snip>
```

如果您还没有这样做，请执行以下命令克隆书籍的GitHub存储库。

$ git clone https://github.com/nigelpoulton/TheK8sBook.git

进入**api**目录。

$ cd TheK8sBook/api

如果您正在跟随，请使用以下命令部署自定义资源。

$ kubectl apply -f crd.yml
customresourcedefinition.apiextensions.k8s.io/books.nigelpoulton.com已创建

恭喜，新资源存在于API中，您可以从中部署对象。此资源将在以下REST路径上提供服务。

apis/nigelpoulton.com/v1/books/

验证它是否存在于API中。如果您使用Windows，请将**grep books**参数替换为**Select-String -Pattern 'books'**。

$ kubectl api-resources | grep books
NAME SHORTNAMES APIGROUP NAMESPACED KIND
books bk nigelpoulton.com false Book

$ kubectl explain book
KIND: Book
VERSION: nigelpoulton.com/v1
DESCRIPTION:
<empty>
FIELDS:
<Snip>

以下YAML来自**api**文件夹中的**kcna.yml**文件，定义了一个名为**kcna**的新的_Book_对象。请注意**spec**部分中的字段与自定义资源中定义的名称和类型匹配。

15：Kubernetes API 247

apiVersion: nigelpoulton.com/v1
kind: Book
metadata:
name: kcna
spec:
bookTitle: "The KCNA Book"
topic: Certifications
edition: 1
使用以下命令部署它。

$ kubectl apply -f kcna.yml
book.nigelpoulton.com/kcna已创建

现在可以使用常规命令列出和描述它。以下命令使用资源的**bk**缩写。

$ kubectl get bk
名称 标题 版本
kcna The KCNA Book. 1

最后，您可以使用**curl**等工具查询新的API组和资源。

以下命令启动**kubectl proxy**进程并列出新的**nigelpoulton.com**命名组下的所有资源。如果之前章节中的代理仍在运行，您无需启动另一个代理。

$ kubectl proxy --port 9000 &
[1] 14784
开始在127.0.0.1:9000上提供服务

$ curl [http://localhost:9000/apis/nigelpoulton.com/v1/](http://localhost:9000/apis/nigelpoulton.com/v1/)
{
"kind": "APIResourceList",
"apiVersion": "v1",
"groupVersion": "nigelpoulton.com/v1",
"resources": [
{
"name": "books",
"singularName": "book",
"namespaced": false,
"kind": "Book",
"verbs": [
"delete",
"deletecollection",
"get",
"list",
"patch",
"create",
"update",
"watch"
],
"shortNames": [
"bk"
],
"storageVersionHash": "F2QdXaP5vh4="
}
]
}

这很好而且有趣。但是，除非创建自定义控制器来处理它们，否则自定义资源没有任何有用的功能。编写自己的控制器超出了本章的范围，但您已经了解了很多关于Kubernetes API以及其工作原理的知识。

### 清理。

如果您一直在跟随操作，那么您需要清理以下资源：

- **kubectl proxy**进程
- kcna书资源
- books.nigelpoulton.com自定义资源（CRD）

运行以下命令之一获取**kubectl proxy**进程的进程ID（PID）。

// Linux和Mac命令

$ ps | grep kubectl proxy
PID TTY TIME CMD
27533 ttys001 0:03.13 kubectl proxy --port 9000

// Windows命令

> tasklist | Select-String -Pattern 'kubectl'
> Image Name PID Session Name Session#
> ============= ===== ============ ========
> kubectl.exe 19776 Console 1

运行以下命令之一终止它，并记住使用您系统的PID。

// Linux和Mac命令

$ kill -9 27533
[1] + 27533 killed kubectl proxy --port 9000

// Windows命令

> taskkill /F /PID 19776
> SUCCESS：PID为19776的进程已终止。

运行以下命令删除**kcna**书对象。

$ kubectl delete book kcna
book.nigelpoulton.com "kcna"已删除

现在删除**books.nigelpoulton.com** CRD。

$ kubectl delete crd books.nigelpoulton.com
customresourcedefinition.apiextensions.k8s.io "books.nigelpoulton.com"已删除

### 章节总结。

现在您已经阅读了本章，以下内容应该都有意义。但如果其中一些仍然令人困惑，不要担心。API可能很难理解，Kubernetes API又庞大又复杂。

无论如何，以下是内容...

Kubernetes是一个以API驱动的平台，API通过API服务器在内部和外部公开。

API服务器作为控制平面服务运行，所有内部和外部客户端都通过API服务器与API交互。这意味着您的控制平面需要具备高可用性和高性能。如果没有，您可能面临API响应缓慢或完全无法访问的风险。
Kubernetes API是一个现代化的基于资源的RESTful API，通过统一的HTTP方法（如POST、GET、PUT、PATCH和DELETE）接受CRUD-style操作。为了方便和可扩展性，它被分为不同的命名组。在Kubernetes早期创建的旧资源存在于原始的"core"组中，可以通过"/api/v1" REST路径访问。所有新的对象都进入命名组。例如，新的网络资源定义在可在"/apis/networking.k8s.io/v1/" REST路径中访问的"networking.k8s.io"子组中。

Kubernetes API中的资源通常是"objects"。但它们也可以是"lists"或"operations"。绝大多数情况下是"objects"，因此我们有时使用"resources"和"objects"这两个术语表示相同的意思。通常将它们的API定义称为资源或资源定义，而在集群上运行的实例通常称为对象。例如，Pod资源存在于核心API组中，而在默认的Namespace中运行了五个Pod对象。

所有新的资源以alpha方式进入API，然后逐渐转为beta版本，最终进入GA版本。Alpha资源可能会发生变化，并且在许多集群中被禁用。Beta资源更可靠，并包含预期进入GA版本的功能。大多数集群默认启用beta资源，但在生产环境中使用时要谨慎。GA资源被认为是"production-grade"，Kubernetes对它们有着坚定的承诺，并有明确的弃用政策，保证在弃用公告后至少12个月或三个版本内提供支持。

最后，Kubernetes API正在成为事实上的云API，许多第三方技术通过扩展它来将自己的技术暴露出来。Kubernetes通过CustomResourceDefinitions使扩展API变得容易，使您的自定义资源看起来和感觉像原生的Kubernetes资源。

希望这样讲述能够让您明白，但如果您对其中一些内容仍然不确定，不用担心。我建议您尽可能多地尝试这些示例。您还应该考虑明天再次阅读本章节-新概念通常需要一段时间来学习。

如果您喜欢本书的这一章节或者其他章节，请跳转到亚马逊并给本书一个快速的评价。云原生之神会对您微笑的;-)

## 16：Kubernetes的威胁建模

```
安全性比以往任何时候都更加重要，Kubernetes也不例外。幸运的是，您可以采取许多措施来保护Kubernetes，在下一章中您将看到其中的一些方法。然而，在这之前，模拟一些常见威胁是个好主意。
```

### 威胁建模

```
威胁建模是识别漏洞的过程，以便采取措施来预防和减轻它们的影响。本章介绍了流行的STRIDE模型，并展示如何将其应用于Kubernetes。
STRIDE定义了六个潜在的威胁类别：
```

- 欺骗
- 篡改
- 否认
- 信息泄露
- 拒绝服务
- 提权

虽然这个模型很好，并提供了一种结构化的评估方式，但没有一个模型能保证涵盖所有的威胁。
在本章的其余部分，我们将分别讨论这六个威胁类别。对于每个类别，我们将给出一个简要的描述，然后看看它们如何适用于Kubernetes。本章不尝试覆盖所有内容，目标是给您提供一些思路并让您入门。

### 欺骗

```
欺骗是假装成他人以获取额外权限的行为。让我们看看Kubernetes如何防止不同类型的欺骗。
```

16: 威胁建模Kubernetes 252

**保护与API服务器的通信**

Kubernetes由许多小组件组成，它们共同工作。其中包括API服务器、控制器管理器、调度器、集群存储等。它还包括节点组件，如kubelet和容器运行时。每个组件都有自己的权限，允许它与集群进行交互和修改。尽管Kubernetes实施了最小特权模型，但欺骗任何一个组件的身份都会引起问题。

如果您阅读了RBAC和API安全性章节，您会知道Kubernetes要求所有组件通过加密签名的证书（mTLS）进行身份验证。这是很好的，Kubernetes通过自动轮换证书使其变得容易。但是，您必须考虑以下内容：
限制

一个典型的Kubernetes安装会自动生成一个自签名的证书颁发机构（CA），用于向所有集群组件颁发证书。虽然这比没有要好，但单凭这一点在生产环境中是不够的。

互相认证的传输层安全（mTLS）只有在颁发证书的CA安全的情况下才能保证安全性。破坏了CA可能会使整个mTLS层失效。因此，保护好CA的安全至关重要！

一个良好的做法是确保由内部Kubernetes CA颁发的证书仅在Kubernetes集群内部使用和信任。这需要仔细审批证书签名请求，并确保Kubernetes CA不会被添加为集群外部任何系统的受信任CA。

正如前几章中提到的，对API服务器的所有内部和外部请求都要进行身份验证和授权检查。因此，API服务器需要一种方法来对内部和外部来源进行身份验证（信任）。一个很好的方法是拥有两个受信任的密钥对：

- 一个用于认证内部系统
- 第二个用于认证外部系统

在这种模式下，您可以使用集群的自签名CA向内部系统颁发密钥。然后，您可以配置Kubernetes来信任一个或多个受信任的第三方CA来认证外部系统。

保护Pod之间的通信

除了伪造对集群的访问外，还存在伪造应用程序之间通信的威胁。在Kubernetes中，这可能是一个Pod伪造了另一个Pod。幸运的是，Pod可以使用证书来验证其身份。

每个Pod都有一个关联的ServiceAccount，用于为Pod提供身份。这是通过将一个服务账户令牌作为秘密自动挂载到每个Pod中来实现的。有两点需要注意：

1. 服务账户令牌允许访问API服务器
2. 大多数Pod可能不需要访问API服务器

基于这两点，对于不需要与API服务器通信的Pod，您应该将**automountServiceAccountToken**设置为**false**。下面的Pod清单展示了如何做到这一点。

```
apiVersion: v1
kind: Pod
metadata:
  name: service-account-example-pod
spec:
  serviceAccountName: some-service-account
  automountServiceAccountToken: false <<==== 这一行
  <剪切>
```

如果Pod确实需要与API服务器通信，可以考虑以下非默认配置：

- **expirationSeconds**
- **audience**

这些配置可以强制让令牌在指定时间后过期，并限制其可以使用的实体。以下示例是从官方Kubernetes文档中的示例，设置了一个为一个小时的到期时间，并将其限制为投影卷中的vault受众。

```
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    volumeMounts:
    - mountPath: /var/run/secrets/tokens
      name: vault-token
  serviceAccountName: my-pod
  volumes:
  - name: vault-token
    projected:
      sources:
      - serviceAccountToken:
          path: vault-token
          expirationSeconds: 3600 <<==== 这一行
          audience: vault <<==== 还有这一行
```

篡改

篡改是恶意地改变某物以导致以下情况之一：

- **拒绝服务**：篡改资源使其无法使用
- **提升权限**：篡改资源以获取额外权限

篡改往往难以避免，因此常见的对策是在发生篡改时让其变得明显。一个非Kubernetes的常见例子是药品包装 - 大多数非处方药都有防篡改封条，如果产品被篡改就会变得明显。

**篡改Kubernetes组件**

对以下任何Kubernetes组件进行篡改都可能会导致问题：

- etcd
- API服务器、控制器管理器、调度器、etcd和kubelet的配置文件
- 容器运行时二进制文件
- 容器镜像
- Kubernetes二进制文件

一般来说，篡改可能发生在传输中或静止状态下。在传输中是指数据在网络上传输时，而在静止状态下是指存储在内存或磁盘上的数据。
传输层安全协议（TLS）是一种保护数据在传输过程中防止篡改的重要工具，它提供了内置的完整性保证，当数据被篡改时会发出警告。

以下建议还可以帮助防止数据在Kubernetes中处于静止状态时被篡改：

- 限制访问运行Kubernetes组件的服务器，特别是控制平面组件。

```
16：威胁建模Kubernetes 255
```

- 限制访问存储Kubernetes配置文件的仓库。
- 仅通过SSH进行远程引导（记得保持SSH密钥安全）。
- 对下载的内容始终运行SHA-2校验和。
- 限制访问您的镜像注册表及其关联的仓库。

```
这不是一个详尽的清单，但实施这些措施将显著减少数据在静止状态下被篡改的可能性。
除了上面列出的项目外，为重要的二进制文件和配置文件配置审计和警报也是良好的生产卫生习惯。如果正确配置和监控，这些措施可以帮助检测潜在的篡改攻击。
下面的示例使用了一个常见的Linux审计守护程序来审计对docker二进制文件的访问。它还审计了更改二进制文件文件属性的尝试。
```

```
$ auditctl -w /usr/bin/docker -p wxa -k audit-docker
```

我们将在本章后面提到这个例子。

**篡改运行在Kubernetes上的应用程序**

```
恶意行为者也会针对应用程序组件和基础设施组件进行攻击。
防止对运行中的Pod进行篡改的一种好方法是将其文件系统设置为只读。这可以通过Pod清单文件的securityContext部分进行配置，保证文件系统的不可变性。
通过将readOnlyRootFilesystem属性设置为true，您可以使容器的根文件系统只读。通过allowedHostPaths属性，您可以对其他容器文件系统进行相同的设置。
以下YAML示例展示了如何在Pod清单中配置这两个设置。在示例中，allowedHostPaths部分确保任何挂载在/test下的内容都是只读的。
```

16：威胁建模Kubernetes 256

apiVersion: v1
kind: Pod
metadata:
name: readonly-test
spec:
securityContext:
readOnlyRootFilesystem: true <<==== 根文件系统只读
allowedHostPaths: <<==== 使下方所有内容只读

- pathPrefix: "/test" <<==== 这个挂载点
  readOnly: true <<==== 只读（R/O）
  <Snip>

### 否认

在非常高的层次上，否认会产生关于某件事的怀疑。不可否认提供关于某事的证明。在信息安全的背景下，不可否认是**证明**某些个体进行了某些行动。

更深入地看，不可否认包括证明以下能力：

- 发生了什么
- 什么时候发生的
- 谁让它发生的
- 它在哪里发生的
- 为什么发生的
- 它是如何发生的

回答最后两个问题可能是最困难的，通常需要在一段时间内相关事件的关联。

审计Kubernetes API服务器事件可以帮助回答这些问题。以下是一个API服务器审计事件的示例（您可能需要在API服务器上启用审计）。

```
16：威胁建模Kubernetes 257
```

```
{
"kind":"Event",
"apiVersion":"audit.k8s.io/v1",
"metadata":{ "creationTimestamp":"2022-11-11T10:10:00Z" },
"level":"Metadata",
"timestamp":"2022-11-11T10:10:00Z",
"auditID":"7e0cbccf-8d8a-4f5f-aefb-60b8af2d2ad5",
"stage":"RequestReceived",
"requestURI":"/api/v1/namespaces/default/persistentvolumeclaims",
"verb":"list",
"user": {
"username":"fname.lname@example.com",
"groups":[ "system:authenticated" ]
},
"sourceIPs":[ "123.45.67.123" ],
"objectRef": {
"resource":"persistentvolumeclaims",
"namespace":"default",
"apiVersion":"v1"
},
"requestReceivedTimestamp":"2022-11-11T10:10:00.123456Z",
"stageTimestamp":"2022-11-11T10:10:00.123456Z"
}
```
API服务器不是唯一一个需要审核以确保不可否认性的组件。至少，您应该从容器运行时、kubelet和集群上运行的应用程序收集审核日志。您还应该审核非Kubernetes基础设施，如网络防火墙。
一旦开始对多个组件进行审核，您就需要一个集中的位置来存储和关联事件。一个常见的方法是通过DaemonSet将代理部署到所有节点上。代理收集日志（运行时、kubelet、应用程序等）并将其发送到安全的中央位置。
如果这样做，集中式日志存储必须是安全的。如果不安全，您将无法信任日志，它们的内容可以被否认。
为了提供与二进制文件和配置文件篡改相关的不可否认性，可以使用一个审核守护程序来监视Kubernetes控制平面节点和工作节点上特定文件和目录的写入操作。例如，在本章的前面部分，您看到了启用对“docker”二进制文件更改的审核的方法。启用后，使用“docker run”命令启动新容器将生成如下事件：

```
16: 威胁建模Kubernetes 258
```

```
type=SYSCALL msg=audit(1234567890.123:12345): arch=abc123 syscall=59 success=yes \
exit=0 a0=12345678abca1=0 a2=abc12345678 a3=a items=1 ppid=1234 pid=12345 auid=0 \
uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=pts0 ses=1 comm="docker" \
exe="/usr/bin/docker" subj=system_u:object_r:container_runtime_exec_t:s0 \
key="audit-docker" type=CWD msg=audit(1234567890.123:12345): cwd="/home/firstname"\
type=PATH msg=audit(1234567890.123:12345): item=0 name="/usr/bin/docker"\
inode=123456 dev=fd:00 mode=0100600 ouid=0 ogid=0 rdev=00:00...
```

当与Kubernetes的审核功能结合和关联在一起时，像这样的审核日志可以创建一个全面且可信的画面，无法被否认。

###信息泄露。

```
信息泄露是指敏感数据的泄露。常见的例子包括被黑客攻击的数据存储和意外暴露敏感数据的API。
```

**保护集群数据**

```
一个Kubernetes集群的整个配置存储在集群存储中（通常是etcd）。这包括网络和存储配置、密码、集群CA等。这使得集群存储成为信息泄露攻击的主要目标。
至少，您应该限制和审核访问托管集群存储的节点。正如接下来的段落所示，获得对集群节点的访问权限可以让登录的用户绕过一些安全层。
Kubernetes 1.7引入了Secrets的加密功能，但默认情况下不启用。即使在这成为默认设置之后，数据加密密钥（DEK）仍存储在与Secrets相同的节点上！这意味着获得对节点的访问权限可以绕过加密。这在托管集群存储（etcd节点）的节点上尤其令人担忧。
幸运的是，Kubernetes 1.11启用了一个试验性功能，允许您将密钥加密密钥（KEK）存储在Kubernetes集群之外。这些类型的密钥用于加密和解密数据加密密钥，应该得到安全保护。您应该认真考虑使用硬件安全模块（HSM）或基于云的密钥管理存储（KMS）来存储密钥加密密钥。
请关注即将发布的Kubernetes版本，以获取有关Secrets加密的进一步改进。

```

**保护Pod中的数据**

```
如前所述，Kubernetes有一个名为Secret的API资源，是存储和共享敏感数据（如密码）的首选方式。例如，访问加密后端数据库的前端容器可以将解密数据库的密钥作为Secret挂载。这比将解密密钥存储在纯文本文件或环境变量中要好得多。
通常还会将数据和配置信息存储在Pod和容器之外的持久卷和ConfigMaps中。如果这些数据加密，您应该将解密它们的密钥存储在Secrets中。
尽管如此，您必须考虑之前部分所提到的关于Secrets以及它们的加密密钥存储的注意事项。您不希望在锁好房子的同时把钥匙留在门上。

### 拒绝服务攻击

拒绝服务（DoS）攻击的目的是使某个系统无法使用。

有许多种类型的DoS攻击，但其中一种比较常见的变体是通过过载系统使其无法处理请求。在Kubernetes世界中，一个潜在的攻击可能是通过过载API服务器使集群操作停滞（即使内部系统也使用API服务器进行通信）。

让我们来看看一些可能成为DoS攻击目标的Kubernetes系统，以及一些保护和减轻攻击的方法。

**保护集群资源免受DoS攻击**

将关键服务复制到多个节点上以实现高可用性是一种历史悠久的最佳实践（HA）。Kubernetes也不例外，您应该在生产环境中以HA配置运行多个控制平面节点。这样做可以防止任何一个控制平面节点成为单点故障。对于某些类型的DoS攻击，攻击者可能需要攻击多个控制平面节点才能产生实质性的影响。

您还应该在可用性区域之间复制控制平面节点。这可以防止特定可用性区域的DoS攻击导致整个控制平面崩溃。

相同的原则也适用于工作节点。拥有多个工作节点不仅可以让调度程序将您的应用程序分布在多个可用性区域，还可以使对单个节点或区域的DoS攻击无效（或减少效果）。

您还应该为以下内容配置适当的限制：

- 内存
- CPU
- 存储

这些限制可以帮助防止关键系统资源被枯竭，从而预防潜在的DoS攻击。

限制_Kubernetes对象_也是一个好的实践。这包括限制特定命名空间中的ReplicaSets、Pods、Services、Secrets和ConfigMaps的数量。

以下是一个示例清单，限制了**skippy**命名空间中Pod对象的数量为100。

apiVersion: v1
kind: ResourceQuota
metadata:
  name: pod-quota
  namespace: skippy
spec:
  hard:
    pods: "100"

另外一个功能是**podPidsLimit**，它限制了一个Pod可以创建的进程数量。

假设一个Pod是fork bomb攻击的目标，其中一个恶意进程试图通过创建足够多的进程来消耗所有系统资源以使系统崩溃。如果您配置了带有**podPidsLimit**的Pod以限制其可以创建的进程数量，您将防止它耗尽节点资源并将攻击的影响限制在Pod内部。如果Pod耗尽了其**podPidsLimit**，Kubernetes通常会重新启动该Pod。

这也确保单个Pod不会耗尽节点上所有其他Pod（包括kubelet）的PID范围。但是，设置正确的值需要合理估计每个节点上同时运行的Pod数量，没有一个粗略估计可能会过度或不足地为每个Pod分配PID。

**保护API服务器免受DoS攻击**

API服务器通过TCP套接字公开了一个RESTful接口，因此成为基于僵尸网络的DoS攻击的目标。

以下措施可能有助于预防或减轻此类攻击：

- 高可用性控制平面节点：在多个可用性区域的多个节点上运行多个API服务器的副本
- 基于合理阈值的API服务器请求的监控和警报
- 使用防火墙限制API服务器对互联网的暴露

除了僵尸网络DoS攻击，攻击者还可能尝试欺骗用户或其他控制平面服务以造成过载。幸运的是，Kubernetes具有强大的身份验证和授权控制来防止欺骗。然而，即使具有强大的RBAC模型，您也必须保护高权限帐户的访问。

**保护集群存储免受DoS攻击**

Kubernetes将集群配置存储在etcd中，因此保证etcd的可用性和安全性至关重要。以下建议有助于实现这一目标：

- 配置一个具有3个或5个节点的HA etcd集群
- 监控和警报etcd的请求
- 在网络层面上隔离etcd，以便只有控制平面的成员可以与其交互
Kubernetes的默认安装在与其余控制平面一起的服务器上安装了etcd。这对于开发和测试来说是可以的。然而，大型生产集群应该认真考虑使用专用的etcd集群。这将提供更好的性能和更高的弹性。

在性能方面，etcd是大型Kubernetes集群中最常见的瓶颈。考虑到这一点，您应该进行测试以确保基础架构能够在规模上维持性能 - 性能不佳的etcd可能会像持续的拒绝服务攻击下的etcd集群一样糟糕。运行专用的etcd集群还通过保护其免受可能被入侵的控制平面的其他部分的影响，提供了额外的弹性。

监视和警报etcd应基于合理的阈值，并且监视etcd日志条目是一个好的起点。

**保护应用程序组件免受拒绝服务攻击**

大多数Pod在网络上公开其主要服务，如果没有额外的控制措施，任何具有网络访问权限的人都可以对Pod进行拒绝服务攻击。幸运的是，Kubernetes提供了Pod资源请求限制，以防止这种攻击耗尽Pod和节点资源。另外，以下措施也会有所帮助：

- 定义Kubernetes网络策略以限制Pod对Pod和Pod对外部通信的权限

16: 威胁建模 Kubernetes 262

- 使用互联传输层安全（mutual TLS）和基于API令牌的身份验证进行应用程序级身份验证（拒绝任何未经身份验证的请求）

为了更全面的防御，您还应该实施实施最少特权的应用程序层授权策略。

图16.1展示了如何将这些措施结合起来，使攻击者难以成功对应用程序进行拒绝服务攻击。

```
图16.1
```

### 提权

提权是指获得比所授予的更高权限的行为。其目的是造成损害或获取未经授权的访问。

让我们来看看在Kubernetes环境中防止这种情况发生的几种方法。

**保护API服务器**

Kubernetes提供了几种授权模式，有助于保护对API服务器的访问。这些包括：

- 基于角色的访问控制（RBAC）
- Webhook
- 节点

16: 威胁建模 Kubernetes 263

您应该同时运行多个授权器。例如，通常使用RBAC和节点授权器。

RBAC模式允许您将API操作限制为用户的子集。这些“用户”可以是常规用户帐户或系统服务。其思想是所有对API服务器的请求都必须经过身份验证**和**授权。身份验证确保请求来自经过验证的用户，而授权确定经过验证的用户是否可以执行所请求的操作。例如，Mia是否可以创建Pod？在这个例子中，Mia是用户，create是操作，Pods是资源。身份验证确保请求确实来自Mia，而授权确定她是否被允许创建Pods。

Webhook模式允许您将授权委托给外部基于REST的策略引擎。然而，构建和维护外部引擎需要额外的工作。它还使得外部引擎成为每个对API服务器的请求的潜在单点故障。例如，如果外部Webhook系统不可用，您可能无法向API服务器发送任何请求。考虑到这一点，您应该严格审查和实施任何Webhook授权服务。

节点授权是关于授权kubelets（节点）发出的API请求。kubelets向API服务器发出的请求类型与常规用户通常发出的请求类型明显不同，节点授权器旨在帮助处理这个问题。

**保护Pods**

接下来的几个部分将讨论几种技术，有助于降低对Pod和容器的提权攻击风险。我们将讨论以下内容：

- 防止进程以root身份运行
- 丢弃功能
- 过滤系统调用
- 防止提权

在进行这些部分时，重要的是要记住Pod只是一个或多个容器的执行环境。其中一些术语使用将对Pod和容器进行互换使用，但通常我们指的是容器。

**不要以root身份运行进程**

root用户是Linux系统上最强大的用户，其用户ID（UID）始终为0。这意味着将应用程序进程以root身份运行几乎总是一个坏主意，因为它授予应用程序进程对容器的完全访问权限。更糟糕的是，容器的root用户有时还对主机系统具有无限制的root访问权限。如果这都不让您感到害怕，那就没有什么可以让您害怕的了！

```
16: 威胁建模 Kubernetes 264
```
```
幸运的是，Kubernetes允许您强制容器进程以非特权非root用户身份运行。
以下Pod配置文件将此Pod中的所有容器配置为以UID 1000的身份运行进程。如果Pod有多个容器，则所有容器进程都将以UID 1000的身份运行。
```

```
apiVersion: v1
kind: Pod
metadata:
  name: demo
spec:
  securityContext: <<==== 适用于此Pod中的所有容器
    runAsUser: 1000 <<==== 非root用户
  containers:
  - name: demo
    image: example.io/simple:1.0
```

```
runAsUser属性是属于PodSecurityContext（spec.securityContext）类别中的众多设置之一。
可以配置两个或多个Pod使用相同的runAsUser UID。当发生这种情况时，来自这些Pod的容器将以相同的安全上下文运行，并且可能可以访问相同的资源。如果它们是同一个Pod的副本，那么这可能是可以接受的。然而，如果它们不是副本，这很可能会导致问题。例如，具有对同一个卷的读写访问权限的两个不同容器可能会导致数据损坏（在没有协调写操作的情况下同时写入同一数据集）。共享的安全上下文也增加了被入侵容器篡改不应访问的数据集的可能性。
```

有鉴于此，可以在容器级别而不是Pod级别使用securityContext.runAsUser属性：

```
apiVersion: v1
kind: Pod
metadata:
  name: demo
spec:
  securityContext: <<==== 适用于此Pod中的所有容器
    runAsUser: 1000 <<==== 非root用户
  containers:
  - name: demo
    image: example.io/simple:1.0
    securityContext:
      runAsUser: 2000 <<==== 覆盖Pod级别的设置
```

```
此示例在Pod级别将UID设置为1000，但在容器级别进行了覆盖，以使demo容器中的进程以UID 2000的身份运行。除非另有指定，否则Pod中的所有其他容器将使用UID 1000。
```

```
16: 威胁建模Kubernetes 265
```

```
解决多个Pod和容器使用相同UID的问题的其他几个方法包括：
```

- 用户命名空间
- 维护UID使用映射

```
用户命名空间是Linux内核技术，允许进程在容器内以root权限运行，但在容器外以不同的用户身份运行。例如，进程可以在容器内以UID 0（root用户）的身份运行，但在主机上映射为UID 1000。这对于需要在容器内以root权限运行的进程可能是一个很好的解决方案。但是，您应该检查它是否得到您的Kubernetes版本和容器运行时的完全支持。
```

```
能力降级
```

虽然大多数应用程序不需要完整的root权限集，但它们通常需要比典型非root用户更多的权限。

我们需要的是一种授予进程运行所需确切权限集的方法。这就是所谓的"capabilities"。
现在是快速了解的时候了。
我们已经说过，root用户是Linux系统上最强大的用户。然而，它的能力是由许多小特权组合而成的，我们称之为“capabilities”。例如，**SYS_TIME** capability允许用户设置系统时钟，而**NET_ADMIN** capability允许用户执行与网络相关的操作，如修改本地路由表和配置本地接口。root用户拥有所有的“capability”，因此非常强大。
拥有一组模块化的capabilities使您在授予权限时可以非常细粒度。与全有或全无（root vs non-root）的方式不同，您可以授予一个进程所需的精确capabilities集合。
目前有30多个capabilities可供选择，选择正确的capabilities可能会令人困惑。考虑到这一点，许多容器运行时实现了一组“sensible defaults”，以便大多数进程可以运行而不会“敞开所有的门”。虽然这样的合理默认值比没有好，但通常不足以适用于生产环境。
找到应用程序所需的绝对最小capabilities集合的常见方法是，在测试环境中运行它并丢弃所有capabilities。这会导致应用程序失败并记录有关缺少权限的日志消息。然后将这些权限映射到capabilities，将其添加到应用程序的Pod规范中，并再次运行应用程序。重复此过程，直到应用程序以最小的capabilities集合正确运行。

```
16: 威胁建模Kubernetes 266
```

```
尽管如此，还有一些需要考虑的问题。
首先，您必须对每个应用程序进行广泛的测试。您不希望在测试环境中没有考虑到的生产边缘情况。这样会导致应用程序在生产环境中崩溃！
其次，每个应用程序修订版都需要对capability集进行同样广泛的测试。
```

考虑到这些问题，您必须拥有能够处理所有这些的测试程序和生产发布流程。
默认情况下，Kubernetes实现了您选择的容器运行时的默认capabilities集合（例如containerd）。但是，您可以通过容器的**securityContext**字段来覆盖这一点。
以下Pod清单展示了如何向容器添加**NET_ADMIN**和**CHOWN** capabilities。

```
apiVersion: v1
kind: Pod
metadata:
  name: capability-test
spec:
  containers:
  - name: demo
    image: example.io/simple:1.0
    securityContext:
      capabilities:
        add: ["NET_ADMIN", "CHOWN"]
```

```
过滤系统调用
```

```
Seccomp（安全计算）与capabilities的概念类似，但工作方式是通过过滤系统调用而不是capabilities。
应用程序向Linux内核发出操作请求是通过发出系统调用。seccomp允许您控制特定容器可以向主机内核发出哪些系统调用。与capabilities一样，您应该实现最小权限模型，其中容器可以进行的系统调用仅限于其运行所需的系统调用。
Kubernetes 1.19中引入了seccomp，并且您可以根据以下seccomp配置文件以不同方式使用它：
```

1. **非阻塞**：允许Pod运行，但记录每个系统调用到一个审计日志中，您可以用来创建自定义配置文件。这个想法是在开发/测试环境中广泛测试应用程序Pod。然后，您将获得一个列出了Pod运行所需的每个系统调用的日志文件。然后，使用此日志文件创建仅允许这些系统调用的自定义配置文件（最小权限）。

```
16: 威胁建模Kubernetes 267
```
2. **阻止**：阻止所有系统调用。非常安全，但会阻止Pod执行任何有用的操作。
3. **RuntimeDefault**：强制Pod使用其容器运行时定义的seccomp配置文件。如果您仍然需要创建自定义配置文件，这是一个常见的起点。容器运行时提供的配置文件旨在在“可用”和“安全”之间取得平衡。它们也经过了充分的测试。
4. **Custom**：仅允许应用程序所需的系统调用以实现运行。其他一切都被阻止。通常在开发/测试环境中使用非阻塞配置文件广泛测试您的应用程序，并记录所有系统调用到审计日志中。然后使用此日志标识应用程序的系统调用并构建定制配置文件。这种方法的危险性在于您的应用程序在测试期间可能会遗漏一些边缘情况。如果发生这种情况，当应用程序遇到一个未在测试期间捕获的边缘情况并使用了系统调用时，它可能会在生产环境中失败。

从安全角度来看，自定义配置文件采用“最小权限”模型，是首选方法。

**通过容器防止权限提升**

在Linux中，创建新进程的唯一方法是一个进程克隆自身，然后将新指令加载到新进程中。我们过于简化了，但原始进程称为“父”进程，副本称为“子”进程。

默认情况下，Linux允许“子”进程比其“父”进程拥有更多的权限。这通常是个坏主意。事实上，您通常希望子进程具有与其父进程相同或更少的权限。对于容器来说尤其如此，因为它们的安全配置是根据其初始配置而不是潜在的提升的权限定义的。

幸运的是，可以通过单个容器的**securityContext**属性来防止权限提升，如示例所示。

apiVersion: v1
kind: Pod
metadata:
  name: demo
spec:
  containers:
  - name: demo
    image: example.io/simple:1.0
    securityContext:
      allowPrivilegeEscalation: false <<==== 此行

16: 使用PSS和PSA标准化Pod安全性

现代Kubernetes集群实现了两项技术来帮助强制执行Pod安全设置：

- **PodSecurityStandards(PSS)**：指定所需的Pod安全设置的策略
- **PodSecurityAdmission(PSA)**：在创建Pod时执行一个或多个PSS策略

这两者共同工作，以实现对Pod安全性的有效集中管理-您可以选择应用哪些PSS策略，而PSA则强制执行这些策略。

**Pod安全标准（PSS）**

每个Kubernetes集群都会获得以下三个由社区维护和更新的PSS策略：

- Privileged
- Baseline
- Restricted

**Privileged**是一个允许所有操作的策略。

**Baseline**实现了合理的默认值。它比_privileged_策略更安全，但比_restricted_策略更不安全。

**Restricted**是实施当前Pod安全最佳实践的黄金标准。不过，需要注意的是，它非常受限制，很多Pod将无法满足其严格要求。

在撰写本文时，您无法调整或修改任何这些策略，并且无法导入其他策略或创建自己的策略。

**Pod安全审查（PSA）**

Pod安全审查（PSA）执行所需的PSS策略。它在命名空间级别工作，并作为一个验证式审查控制器实施。

PSA提供了三种执行模式：

- **警告**：允许创建违反策略的Pod，但会发出面向用户的警告
- **审计**：允许创建违反策略的Pod，但会记录审计事件
- **强制**：如果违反策略，则拒绝创建Pod

将每个命名空间至少配置为具有**基线**策略的**警告**或**审计**是一个好习惯。这样可以开始收集哪些Pod未能满足策略以及原因的数据。下一步是强制执行**基线**策略，并在**受限制**策略上发出警告和审计。

没有Pod安全配置的任何命名空间都是安全配置中的漏洞，您应尽快附加一个策略，即使只是警告和审计。

将以下标签应用于命名空间将为其应用**基线**策略。它将允许违反策略的Pod运行，但会生成面向用户的警告。

pod-security.kubernetes.io/warn: baseline
标签的格式为**<前缀>/<模式>: <策略>**，具体选项如下：

- 前缀始终为**pod-security.kubernetes.io**
- 模式为**warn**、**audit**或**enforce**之一
- 策略始终为**privileged**、**baseline**或**restricted**之一

PSAs作为验证性准入控制器运行，意味着它们不能修改Pods。它们也不能对正在运行的Pods产生任何影响。

**PSA示例**

让我们通过一些示例演示Pod Security Admission的工作原理。您将完成以下步骤：

```
1.创建一个名为psa-test的命名空间
2.将标签应用于强制执行基线PSS策略
3.尝试部署一个运行特权容器的Pod（将失败）
4.修改Pod以符合PSS策略并重新部署它（将成功）
5.测试切换到受限策略的潜在影响
6.切换到受限策略
7.测试对现有Pods的任何影响
```

如果您想跟随操作，请确保您有**kubectl**、一个Kubernetes集群以及这本书的GitHub存储库的本地克隆。如果需要这些，请参阅第3章。

您可以使用以下命令克隆这本书的GitHub存储库。

16: 威胁建模Kubernetes 270

$ git clone https://github.com/nigelpoulton/TheK8sBook

请确保从**psa**目录运行以下命令。

运行以下命令创建一个名为**psa-test**的新命名空间。

$ kubectl create ns psa-test

将**pod-security.kubernetes.io/enforce=baseline**标签添加到新的命名空间。这将阻止创建任何违反**baseline** PSS策略的新Pods。

$ kubectl label --overwrite ns psa-test \
pod-security.kubernetes.io/enforce=baseline

验证标签是否正确应用。

$ kubectl describe ns psa-test

名称: psa-test
标签: kubernetes.io/metadata.name=psa-test
pod-security.kubernetes.io/enforce=baseline <<==== 标签已正确应用
注释: <none>
状态: 活跃

已创建命名空间并执行了**baseline**策略。

以下YAML来自**psa-pod.yml**文件，定义了一个违反**baseline**策略的特权容器。

apiVersion: v1
kind: Pod
metadata:
name: psa-pod
namespace: psa-test <<==== 将其部署到新的psa-test命名空间
spec:
containers:

- name: psa-ctr
  image: nginx
  securityContext:
  privileged: true <<==== 违反了基线策略

使用以下命令部署它。

16: 威胁建模Kubernetes 271

$ kubectl apply -f psa-pod.yml

来自服务器的错误（Forbidden）：创建"psa-pod.yml"时出错：pods "psa-pod"被禁止：违反了PodSecurity "baseline:latest"：privileged（容器"psa-ctr"不能设置securityContext.privileged=true）

输出显示禁止了Pod的创建，并列出了原因。

编辑**psa-pod.yml**，将容器的**securityContext.privileged**更改为**false**并保存更改。

apiVersion: v1
kind: Pod
<Snip>
spec:
containers:

- name: psa-ctr
  image: nginx
  securityContext:
  privileged: false <<==== 从true更改为false

现在尝试部署Pod。

$ kubectl apply -f psa-pod.yml
pod/psa-pod created

它符合**baseline**策略的要求，并成功部署了。

您可以使用**--dry-run=server**标志测试将PSS策略应用于命名空间的影响。使用此标志**不会**应用策略。

$ kubectl label --dry-run=server --overwrite ns psa-test \
pod-security.kubernetes.io/enforce=restricted

警告：命名空间"psa-test"中的现有Pod违反了新的PodSecurity强制执行级别"restricted:latest"
警告：psa-pod：allowPrivilegeEscalation != false，具有不受限制的权限，runAsNonRoot != true，seccompProfile
<Snip>

输出显示**psa-pod** Pod未满足四个策略要求：

- **allowPrivilegeEscalation**属性未设置为false
- 它正在运行不受限制的权限
- **runAsNonRoot**字段未设置为true
- 它未通过**seccompProfile**测试

继续将策略应用于命名空间，并查看它是否会影响已经运行的**psa-pod**。

16: 威胁建模Kubernetes 272
$ kubectl label --overwrite ns psa-test \
pod-security.kubernetes.io/enforce=restricted

警告：命名空间“psa-test”中的现有Pod违反了新的PodSecurity强制执行级别“restricted:latest”
警告：psa-pod：allowPrivilegeEscalation != false，无限制的capabilities，
runAsNonRoot != true，seccompProfile
命名空间/psa-test已标记

$ kubectl get pods --namespace psa-test

名称    就绪   状态    重启   年龄
psa-pod 1/1  运行中   0    3m9s

您将收到相同的警告消息，但不会终止现有的Pod。这是因为PSA作为一个准入控制器运行，因此只对Pod的创建和修改起作用。

最后，可以针对单个命名空间配置多个策略和模式。事实上，这是一种常见的做法。

以下示例为**psa-test**命名空间应用了三个标签。它们_强制执行_**基线**策略，并对**restricted**策略进行_警告_和_审计_。这是实施**基线**策略并为**restricted**做准备的好方法。

$ kubectl label --overwrite ns psa-test \
pod-security.kubernetes.io/enforce=baseline \
pod-security.kubernetes.io/warn=restricted \
pod-security.kubernetes.io/audit=restricted

您可以运行**kubectl describe ns psa-test**命令来确保标签已应用。

**Pod安全准入的替代方案**

如前所述，PSS和PSA存在一些限制。其中包括实现为验证准入控制器，无法修改、导入或创建自己的策略。如果您需要更多功能，请考虑以下第三方解决方案：

- OPA Gatekeeper
- Kubewarden
- Kyverno

还有其他解决方案。

16: Kubernetes的威胁建模 273

**朝着更安全的Kubernetes**

正如以下示例所示，Kubernetes正在不断追求更好的安全性。

从Kubernetes v1.26开始，用于构建Kubernetes集群的所有二进制工件和容器镜像都经过了加密签名。

Kubernetes社区维护一个官方源，用于公开宣布的Kubernetes漏洞（CVE）。自v1.27以来，提供了一个JSON和RSS源，当有新的CVE宣布时会自动刷新。

从Kubernetes 1.27开始，所有容器都继承自容器运行时的默认seccomp配置文件，该配置文件实现了合理的安全默认值。这要求在每个kubelet上使用**--seccomp-default**。

许多云提供商实现了诸如_机密计算_服务（如_机密虚拟机_和_机密容器_）之类的服务，Kubernetes可以利用这些服务来保护通过启用容器工作负载的内存加密等方式使用的数据。一些云提供商甚至将其作为托管Kubernetes服务的一部分提供。

基于Kubernetes 1.24，一个最新的第三方安全审计报告^11于2023年4月发布。这是第二份此类报告，继续自2019年的原始报告。这些报告是识别Kubernetes环境潜在威胁以及缓解方法的重要工具。

最后，值得阅读《云原生安全白皮书》^12，以获得关于保护云原生环境（如Kubernetes）的更全面的视角。

### 章节总结。

本章向您展示了如何使用STRIDE模型对Kubernetes进行威胁建模。您逐步了解了六个威胁类别，并了解了一些预防和缓解它们的方法。

您看到一个威胁通常会导致另一个威胁，并且存在多种方式来缓解单个威胁。一如既往，深度防御是一种关键策略。

本章最后讨论了Pod Security Admission是实施Pod安全默认值的首选方式。

在下一章中，您将了解在生产环境中运行Kubernetes时的一些最佳实践和经验教训。

(^11) https://research.nccgroup.com/2023/04/17/public-report-kubernetes-1-24-security-audit/
(^12) https://github.com/cncf/tag-security/tree/main/security-whitepaper/v2

## 17: 现实世界中的Kubernetes安全

```
前一章向您展示了如何使用STRIDE模型对Kubernetes进行威胁建模。在本章中，您将了解在实际应用Kubernetes时可能遇到的与安全相关的挑战。
本章的目标是从安全架构师的高层视角向您展示事物。它不提供烹饪书式的解决方案。
本章分为以下四个部分：
```
- 软件交付流程中的安全性
- 工作负载隔离
- 身份与访问管理
- 安全监控与审计

### 软件交付流程中的安全性。

```
容器彻底改变了我们构建、发布和运行应用程序的方式。不幸的是，这也使得运行危险代码变得比以往更加容易。
让我们看一些可以确保将应用代码从开发者的笔记本电脑传输到生产服务器的供应链安全的方法。
```

**镜像仓库**

我们将镜像存储在公共和私有仓库中，将其划分为“仓库”。

```
公共仓库位于互联网上，是推送和拉取镜像的最简单方式。然而，在使用公共仓库时，您需要非常小心：
```

```
1. 您需要适当保护存储在公共仓库中的镜像。
2. 您不应信任从公共仓库中拉取的镜像。
```

```
一些公共仓库有官方镜像和社区镜像的概念。一般来说，官方镜像比社区镜像更安全，但您始终应该进行尽职调查。
```

```
17: 真实世界中的Kubernetes安全 275
```

```
官方镜像通常由产品供应商提供，并经过严格的审查流程以确保质量。您应该期望它们实施良好的实践，定期扫描漏洞，并包含最新的补丁和修复程序。其中一些甚至可能得到产品供应商或托管镜像的公司的支持。社区镜像没有经过严格的审查，使用时应格外谨慎。
```

在考虑这些要点的同时，您应该实施一种标准化的方式供开发者获取和使用镜像。您还应该尽可能地使该过程无摩擦，以便开发者不会感到有必要绕过该过程。让我们讨论一些可能有所帮助的事情。

**使用批准的基础镜像**

```
大多数镜像都从基础层开始，然后添加其他层以形成一个有用的镜像。图17.1展示了一个简化的例子，其中有三个层的镜像。基础层中包含核心操作系统和文件系统组件，中间层包含库和依赖项，顶层包含您的应用程序。这三者的组合就是镜像，包含了运行应用程序所需的一切。
```

```
图17.1-镜像分层
```

```
通常情况下，维护一小组批准的基础镜像是一个好的做法。这些基础镜像通常源自官方镜像，并根据您的企业政策和需求进行加固。例如，您可以基于经过调整以符合您要求的官方Alpine Linux镜像创建一小组批准的基础镜像（包括补丁、驱动程序、审计设置等）。图17.2展示了三个构建在两个批准的基础镜像之上的应用程序。左边的应用程序构建在您批准的Alpine Linux基础镜像之上，而其他两个应用程序是构建在您批准的Alpin+NGINX基础镜像之上的Web应用程序。
```

```
17: 真实世界中的Kubernetes安全 276
```

```
图17.2-使用批准的基础镜像
```

虽然您需要投入一定的努力来创建批准的基础镜像，但它们带来了以下好处：

- 标准的驱动程序集
- 已知的补丁
- 标准化的审计设置
- 减少软件扩散（减少非官方基础镜像）
- 简化测试（针对一小组已知基础镜像进行测试）
- 简化更新（较少的基础镜像需要打补丁）
- 简化故障排除（一个被广泛理解且数量有限的基础镜像）

```
拥有一组批准的基础镜像还可以使开发人员专注于应用程序，而无需关心与操作系统相关的事物。它还可以帮助您减少需要处理的支持合同和供应商数量。
```

**管理对非标准基础镜像的需求**

```
尽管拥有一小组批准的基础镜像是一个很好的做法，但您可能仍然有对定制配置的合理需求。在这些情况下，您将需要良好的流程：
```

- 确定为什么不能使用现有的批准基础镜像
- 确定是否可以更新现有的批准基础镜像以满足需求（包括是否值得付出努力）
- 确定将全新镜像引入环境所带来的支持影响

```
在大多数情况下，您会希望更新现有的基础镜像，例如添加用于GPU计算的设备驱动程序，而不是引入全新的镜像。
```

```
17: 真实世界中的Kubernetes安全 277
```

**控制对镜像的访问**
保护组织的图像有几种方法。
一种安全而实用的选择是在自己的防火墙内托管私有注册表。这样可以控制注册表的部署方式、复制方式和修补方式。您还可以创建适合组织需求的存储库和策略，并将其与现有的身份管理提供商（如Active Directory）集成。
如果无法管理自己的私有注册表，可以将图像托管在公共注册表上的私有存储库中。然而，并非所有公共注册表都是相同的，您需要仔细选择合适的注册表并正确配置它。

无论选择哪种解决方案，您只应托管经组织批准使用的图像。这些通常来自经过信任的来源，并由信息安全团队审核。您应在存储库上设置访问控制，以便只有经批准的用户可以推送和拉取图像。
除了注册表本身外，您还应该：

- 限制哪些集群节点可以访问互联网，记住您的图像注册表可能位于互联网上
- 配置访问控制，只允许授权用户和节点推送到存储库

如果您使用公共注册表，您可能需要授予集群节点访问互联网的权限，以便它们可以拉取图像。在这种情况下，限制互联网访问权限仅限于您的注册表使用的地址和端口是一个良好的实践。您还应在注册表上实施严格的RBAC规则，以控制谁可以从哪个存储库推送和拉取图像。例如，您可能会限制开发人员只能在开发和测试存储库中推送和拉取，而允许运维团队在生产存储库中推送和拉取。
最后，您可能只希望一部分节点（构建节点）能够推送图像。您甚至可能希望限制只有自动化构建系统才能推送到特定存储库。

**将图像从非生产环境移至生产环境**

许多组织在开发、测试和生产环境中有不同的环境。
一般来说，开发环境的规则较少，是开发人员可以进行实验的地方。这可能涉及开发人员最终想在生产环境中使用的非标准图像。

17: 真实世界的Kubernetes安全 278

以下章节概述了一些措施，以确保只有安全的图像获得生产批准。

**漏洞扫描**

在允许图像进入生产环境之前，首要任务是进行漏洞扫描。这些服务以二进制级别扫描您的图像，并将其内容与已知安全漏洞（CVE）的数据库进行对比。

您应将漏洞扫描整合到CI/CD流水线中，并实施策略，如果图像包含特定类别的漏洞，则自动失败构建并隔离图像。例如，您可以实施一个构建阶段，扫描图像并自动失败任何使用已知“关键”漏洞图像的内容。

然而，某些扫描解决方案比其他解决方案更好，并允许您创建高度可定制的策略。

例如，一个执行TLS验证的Python方法可能会在“通用名称”包含大量通配符时容易受到拒绝服务攻击。然而，如果您从未以这种方式使用Python，您可能不认为该漏洞相关，并希望将其标记为误报。并非所有的扫描解决方案都允许您这样做。

**以代码形式配置**

扫描应用程序代码以发现漏洞被广泛接受作为良好的生产卫生习惯。然而，扫描您的Dockerfiles、Kubernetes YAML文件、Helm图表和其他配置文件的做法并不广泛。

一个众所周知的例子是，当一个IBM数据科学实验在其容器图像中嵌入私有TLS密钥时，没有审核配置文件。这意味着攻击者可以拉取图像并获得托管容器的节点的root访问权限。如果他们对Dockerfiles进行了安全审查，整个事情将很容易避免。

对于这些检查的自动化工具仍在不断进步，实现了以代码为规则的策略。

**签署容器图像**

在今天的世界中，信任是一件大事，通过加密签署软件交付流水线中的每个阶段的内容正在成为常态。幸运的是，Kubernetes和大多数容器运行时都支持加密签署和验证图像。

在这种模式下，开发人员对其图像进行加密签名，当消费者拉取和运行图像时进行加密验证。这使消费者能够确信他们正在使用正确的图像，并且图像没有被篡改。
图17.3显示了签署和验证图像的高级流程。

图17.3
### 镜像签名和验证

通常，镜像的签名和验证是由容器运行时实现的。您应该寻找一些工具，允许您定义和执行全企业范围的签名策略，这样就不会由个别用户决定了。

### 镜像推广流程

在我们讨论的所有内容中，您的构建流水线应该包括尽可能多的以下步骤：

1. 强制使用签名镜像的策略。
2. 限制哪些节点可以推送和拉取镜像的网络规则。
3. 保护镜像仓库的角色基于访问控制规则。
4. 使用批准的基础镜像。
5. 对已知漏洞进行镜像扫描。
6. 基于扫描结果的镜像推广和隔离。
7. 审查和扫描基础设施即代码配置文件。

当然，您还可以采取其他措施，上述的清单并不是一个确切的工作流程。

### 工作负载隔离

本节将向您展示一些隔离工作负载的方法。我们将从集群层面开始，然后转向运行时层面，最后再看外部的像网络防火墙这样的基础设施。

**集群级别的工作负载隔离**

直截了当地说，Kubernetes并不支持安全的多租户集群。唯一将两个工作负载隔离的方法是在它们自己的集群上运行它们自己的硬件。让我们仔细看看。

将Kubernetes集群划分的唯一方式是创建命名空间。然而，这些命名空间只是一种对资源进行分组和应用限制、配额和访问控制规则等的方式。

命名空间并不能阻止一个命名空间中的受损工作负载对其他命名空间中的工作负载产生影响。这意味着您永远不应该在同一个Kubernetes集群上运行恶意的工作负载。尽管如此，Kubernetes的命名空间仍然非常有用，您应该使用它们，只是不要把它们当作安全边界。

**命名空间和软多租户**

对于我们的目的来说，软多租户是指在共享基础设施上托管多个受信任的工作负载。通过受信任，我们指的是不需要绝对保证一个工作负载不会影响到另一个工作负载的工作负载。

例如，受信任的工作负载可能是一个具有Web前端服务和后端推荐服务的电子商务应用程序。由于它们属于同一个应用程序，它们之间并不是敌对的。但是，您可能希望每个工作负载都有自己的资源限制，由不同的团队管理。在这种情况下，一个拥有前端服务的命名空间和另一个拥有后端服务的命名空间的单个集群可能是一个好的解决方案。

**命名空间和硬多租户**

我们将硬多租户定义为在共享基础设施上托管不受信任且可能具有敌对意图的工作负载。然而，正如我们之前所说，目前Kubernetes并不支持这种模式。这意味着需要强大安全边界的工作负载需要在单独的Kubernetes集群上运行！一些示例包括：

- 隔离生产和非生产工作负载
- 隔离不同的客户
- 隔离敏感项目和业务功能

当然还有其他的例子，但重要的是需要强分离的工作负载需要拥有自己的集群。

注意：Kubernetes项目有一个专门的多租户工作组正在积极研究多租户模型。这意味着未来的Kubernetes版本可能会有更好的硬多租户解决方案。

**节点隔离**

有时候，您的应用程序可能需要非标准权限，比如以root身份运行或执行非标准的系统调用。将这些应用程序单独隔离在自己的集群中可能过于严格，但您可以在一部分工作节点上对它们进行隔离。这样做将限制受损的工作负载只对同一节点上的其他工作负载产生影响。

您还应该应用深度防御原则，通过在运行具有非标准权限的工作负载的节点上启用更严格的审计日志记录和更严格的运行时防御选项。

Kubernetes提供了几种技术，比如标签、亲和性和反亲和性规则以及污点，帮助您将工作负载定位到特定的节点上。

**运行时隔离**

容器与虚拟机曾经是一个有争议的话题。然而，就工作负载隔离而言，只有一个赢家......虚拟机。

大多数容器平台实现了命名空间容器。这是一种模型，其中每个容器共享主机的内核，并且通过内核构造（如命名空间和cgroups）提供隔离，而这些构造从来没有被设计为强大的安全边界。
Docker、containerd和CRI-O是实现命名空间容器的常见容器运行时和平台的例子。
这与虚拟化器模型非常不同，虚拟机模型中每个虚拟机都有自己专用的内核，并通过硬件执行强制隔离以与其他虚拟机隔离。
然而，现在可以更容易地通过安全相关技术来增强容器，使其更安全并实现更强的工作负载隔离。这些技术包括AppArmor、SELinux、seccomp、capabilities和用户命名空间，大多数容器运行时和托管的Kubernetes服务都能很好地实现它们的合理默认值。但是，它们仍然可能很复杂，特别是在故障排除时。
您还应考虑不同类型的容器运行时。两个例子是gVisor和KataContainers，它们都提供更强的工作负载隔离级别，并且由于容器运行时接口（CRI）和运行时类的存在，与Kubernetes集成起来很容易。
还有一些项目可以让Kubernetes编排其他类型的工作负载，如虚拟机、无服务器函数和WebAssembly。

虽然这些可能会让人感到不知所措，但在确定工作负载所需的隔离级别时，您需要考虑所有这些。

总结一下，存在以下工作负载隔离选项：

1. **虚拟机：**每个工作负载都有自己的专用内核。它提供了很好的隔离性，但速度相对较慢且占用资源较多。
2. **命名空间容器：**所有容器共享主机的内核。这些容器速度快、轻量级，但需要额外的工作来改善工作负载隔离。
3. **每个容器运行在自己的虚拟机中：**这类解决方案试图通过在每个容器中运行其自己的专用虚拟机来结合容器的灵活性和虚拟机的安全性。尽管使用了专门的轻量级虚拟机，但这些解决方案丧失了容器的很多吸引力，并且不太受欢迎。
4. **使用不同的运行时类：**这允许您将所有工作负载都作为容器运行，但将需要更强隔离的工作负载定向到适当的容器运行时。
5. **Wasm容器：**Wasm容器将Wasm（WebAssembly）应用程序打包到OCI容器中，可以在Kubernetes上执行。这些应用程序仅使用容器进行打包和调度，在运行时它们在一个安全的拒绝默认的Wasm主机内执行。详情请参见第9章。

**网络隔离**

防火墙是任何分层信息安全系统的重要组成部分。其目标只是允许授权通信。

在Kubernetes中，Pods通过一个称为“Pod网络”的内部网络进行通信。然而，Kubernetes并没有实现“Pod网络”，而是实现了一个名为容器网络接口（CNI）的插件模型，允许第三方供应商实现“Pod网络”。存在许多CNI插件，但它们可分为两个广泛的类别：

- Overlay（覆盖）
- BGP（边界网关协议）

每种插件对防火墙实施和网络安全有不同的影响。

**Kubernetes和覆盖网络**

大多数Kubernetes环境将“Pod网络”实现为一个简单的平面“覆盖网络”，它隐藏了集群节点之间的任何网络复杂性。例如，您可以将集群节点部署在由路由器连接的十个不同网络上，但Pods连接到平面Pod网络并且在不需要了解主机网络的任何复杂性的情况下进行通信。图17.4显示了两个不同网络上的四个节点以及连接到单个覆盖Pod网络的Pods。

```
图17.4
```

覆盖网络使用VLXAN技术封装流量以在现有的第3层基础设施之上进行简单的平面第2层网络传输。如果对网络术语不太了解，您只需要知道覆盖网络封装容器发送的数据包。此封装隐藏了原始源和目标IP地址，使防火墙难以了解发生的情况。请参见图17.5。

**Kubernetes和BGP**

BGP是驱动互联网的协议。然而，从本质上讲，它是一个简单且可扩展的协议，用于创建用于共享路由和执行路由的对等关系。

以下类比可能有所帮助。想象一下，您想给一位您失去联系并且不再知道他们地址的朋友寄一张生日卡。然而，你的孩子在学校有一个朋友，他们的父母仍然与你的老朋友保持联系。在这种情况下，您将卡片交给您的孩子，并要求他们将其交给他们在学校的朋友。这个朋友将其交给他们的父母，然后将其交给您的朋友。
BGP路由类似，并通过互相帮助寻找路由的"对等体"网络进行。从安全角度来看，重要的是BGP不封装数据包。这使得防火墙的工作变得更简单。图17.6展示了使用BGP的相同设置，注意没有封装的情况。

```
图17.6-在BGP网络上没有封装
```

```
17:现实世界的Kubernetes安全 285
```

```
这对防火墙的影响
```

我们已经说过，防火墙根据源地址和目标地址来允许或拒绝流量。例如：

- 允许来自10.0.0.0/24网络的流量
- 不允许来自192.168.0.0/24网络的流量

```
假设您的Pod网络是一个覆盖网络。在这种情况下，所有流量都将被封装，只有能够打开数据包并检查其内容的防火墙才能对是否允许或拒绝流量做出有用的决策。如果您的防火墙无法做到这一点，您可能需要考虑使用BGP Pod网络。
您还应该考虑是部署物理防火墙、基于主机的防火墙还是两者结合。物理防火墙是专用的网络硬件设备，通常由一个中央团队管理。基于主机的防火墙是操作系统（OS）功能，通常由部署和管理OS的团队管理。这两种解决方案都有优缺点，结合两者通常是最安全的。然而，您应该考虑您的组织是否有一个长而复杂的程序来实施对物理防火墙的更改。如果有这样的程序，它可能不适合您的Kubernetes环境的特性。
```

```
数据包捕获
```

```
在网络和IP地址方面，Pod IP地址不仅有时会被封装隐藏，而且它们也是动态的，并且可以由不同的Pod重新使用。我们称之为IP变动，它减少了IP地址在识别系统和工作负载方面的有用性。考虑到这一点，在执行诸如数据包捕获等操作时，将IP地址与Kubernetes特定标识符（如Pod ID、服务别名和容器ID）关联起来可以非常有用。让我们转换一下方向，看看一些控制用户访问Kubernetes的方法。
```

### 身份和访问管理（IAM）..

```
在任何生产环境中，控制用户访问Kubernetes都非常重要。幸运的是，Kubernetes拥有一个强大的RBAC子系统，可以与现有的IAM提供商（如Active Directory、其他LDAP系统和基于云的IAM解决方案）集成。
大多数组织已经拥有一个集中的IAM提供商，该提供商与公司的人事系统集成，以简化员工生命周期管理。
```

```
幸运的是，Kubernetes利用现有的IAM提供商，而不是实施自己的解决方案。这意味着新员工将在企业IAM数据库中获得一个身份，假设您将他们加入适当的组，他们将自动获得Kubernetes的权限。同样，当员工离开组织时，HR流程将自动从IAM数据库中删除他们的身份，他们的Kubernetes访问将停止。
自v1.8以来，RBAC一直是一个稳定的Kubernetes功能，您应该充分利用其功能。
```

**管理对集群节点的远程SSH访问**

```
您将通过对API服务器进行REST调用来执行几乎所有的Kubernetes管理操作。这意味着用户很少需要对Kubernetes集群节点进行远程SSH访问。实际上，对集群节点的远程SSH访问只应用于以下类型的活动：
```

- 无法通过Kubernetes API执行的节点管理活动
- 紧急情况的活动，例如API服务器宕机时
- 深入故障排除

```
多因素认证（MFA）
```

伴随着强大的力量而来的是巨大的责任。

```
具有对API服务器和集群节点的root访问权限的帐户非常强大，是攻击者和不满意的员工的首要目标。因此，您应该通过多因素认证（MFA）来保护其使用。这意味着用户必须输入用户名和密码，然后进行第二阶段的身份验证。例如：
```

- 第一阶段：测试用户名和密码的知识
- 第二阶段：测试像一次性密码这样的物品的拥有权

```
您还应该保护安装了kubectl的工作站和用户配置文件的访问。
```

### 安全监控和审计。

```
```
**限制**

请根据英文内容直接翻译，维持原有的格式，不省略任何信息。

**翻译后的文章**

```
没有任何系统是100％安全的，您应该始终考虑到系统被入侵的可能性。当发生入侵时，至少应能够做到以下两点：
```

```
17：现实世界的Kubernetes安全287
```

```
1.确认已发生入侵
2.建立无法抵赖的事件详细时间表
```

```
审计对于这两点至关重要，建立可靠的时间表能帮助回答以下事件后的问题：
```

- 发生了什么
- 是如何发生的
- 何时发生的
- 谁做的

```
在极端情况下，这些信息可以在法庭上使用。
良好的审计和监控解决方案还可以帮助识别安全系统中的漏洞。
```

记住这些要点，您应确保强大的审计和监控是您的优先事项之一，并且不应在生产环境中没有它们的情况下上线。

**基准最佳实践**

```
有各种工具和检查可以帮助您确保根据最佳实践和公司政策配置Kubernetes环境。
信息安全中心（CIS）发布了一个用于Kubernetes安全的行业标准基准，并且Aqua Security（aquasec.com）编写了一个名为kube-bench^13的易于使用的工具，用于运行CIS测试以针对集群生成报告。
不幸的是，kube-bench无法检查托管Kubernetes服务的控制平面节点。
您应考虑将kube-bench作为节点配置过程的一部分运行，并根据结果通过或失败节点配置。
您还可以使用kube-bench报告作为事故后的基准。这样，您可以比较事故发生前后的kube-bench报告，确定是否以及在哪里发生了任何配置更改。
```

**容器和Pod生命周期事件**

```
Pod和容器是短暂存在的对象，经常来去。这意味着您将看到许多宣布新对象的事件以及许多宣布终止对象的事件。
```

(^13) https://github.com/aquasecurity/kube-bench

```
17: 真实世界的Kubernetes安全288
```

考虑将日志保留配置为保留终止Pod的日志，以便即使在终止后也可以进行检查。
您的容器运行时可能还会保存与容器生命周期事件相关的日志。

**取证检查点**

```
取证是收集和审查可用证据以构建事件链的科学，尤其是当您怀疑存在恶意行为时。
容器的短暂性过去使此过程具有挑战性。然而，最近的技术，例如用户空间中的检查点/还原（CRIU），使得在沙箱环境中更轻松地静默捕获运行中容器的状态并恢复它们。撰写本文时，CRIU是Kubernetes的alpha功能，目前唯一支持它的运行时是CRI-O。
```

**应用程序日志**

```
应用程序日志在识别潜在的与安全相关问题时也很重要。
然而，并非所有应用程序都将其日志发送到相同位置。有些应用程序将其日志发送到其容器的标准输出（stdout）或标准错误（stderr）流中，以便您的日志工具可以在容器日志旁边捕获它们。但是，有些应用程序将日志发送到专有日志文件的特定位置。请确保为每个应用程序研究这一点，并进行配置，以便不会错过日志。
```

**用户执行的操作**

```
大部分Kubernetes的配置和管理将通过API服务器完成，所有请求都应该被记录。然而，恶意行为者也有可能获得远程SSH访问控制平面节点，并直接操纵Kubernetes对象。这可能包括访问集群存储和etcd节点。
```

我们已经说过您应该限制对集群节点的SSH访问，并加强安全性通过多因素身份验证（MFA）。但是，您还应记录所有SSH活动并将其发送到安全的日志聚合器。您还应考虑要求至少有两个熟练的人在所有SSH访问控制平面节点时都在场。

**管理日志数据**

```
容器的一个关键优势是应用程序密度 - 您可以在服务器和数据中心上运行更多的应用程序。这导致产生大量的日志数据和审计数据，如果没有专用工具来对其进行排序和理解，则会变得不可接受。幸运的是，存在高级工具，不仅存储数据，还可以将其用于主动分析和事后反应性分析。
```

**安全相关事件的警报**


除了用于事件后分析和否认的目的，一些事件也足够重要，需要立即进行调查。其中包括：

- 人工用户创建特权 Pod：特权 Pod通常可以在节点上获得root级别访问权限，您通常会制定策略来防止其创建。在极少数情况下，它们可能会由带有服务账户的自动化流程创建。
- 人工用户执行会话：执行会话提供类似shell的访问容器的功能，通常仅用于故障排除。您应该调查不是用于故障排除的执行会话，并考虑删除它们以防止篡改。
- 尝试从互联网访问集群：通常会禁止从互联网访问控制平面。因此，您应该监控从互联网连接到控制平面的成功和不成功的尝试，成功的尝试通常表示安全配置错误，您应该进行修复。

**将现有应用迁移到Kubernetes**

在将应用程序迁移到Kubernetes时，使用逐步推进的策略可能会很有用：

1. 逐步推进：对现有应用进行威胁建模将有助于了解其当前的安全状态。例如，您现有的应用程序中哪些使用了TLS进行通信，哪些没有使用TLS进行通信。
2. 逐步推进：在迁移到Kubernetes时，确保这些应用程序的安全状态保持不变。例如，如果某个应用程序没有使用TLS进行通信，则在迁移过程中**不要**对其进行更改。
3. 推进：在迁移后开始改善应用程序的安全性。从简单的非关键应用程序开始，并逐步提升到关键应用程序。您还可以有条不紊地部署更深层次的安全措施，例如最初配置应用程序使用单向TLS进行通信，然后逐步改为使用双向TLS进行通信。

关键点是在将应用程序迁移到Kubernetes时不要改变其安全状态。这是因为进行迁移**和**进行更改可能会导致更容易误诊问题——是安全更改还是迁移的原因？

```
第17章：真实世界的Kubernetes安全 290页
```

### 真实世界的例子。

```
2019年2月发生了一个与容器相关的易受攻击的漏洞，通过实施我们讨论过的一些最佳实践，这个漏洞本可以很容易地被预防。
CVE-2019-5736允许作为root运行的容器进程获取工作节点和主机上运行的所有容器的root访问权限。
尽管这个漏洞非常危险，但本章所涵盖的以下内容将会防止这个问题的发生：
```

- 镜像漏洞扫描
- 不以root身份运行进程
- 启用SELinux

```
由于该漏洞有一个CVE编号，扫描工具本应该发现并报告该漏洞。即使扫描平台未能发现它，也会防止root容器和标准SELinux策略的政策阻止漏洞的利用。
```

### 章节总结。

```
本章的目的是介绍一些影响许多Kubernetes的真实世界安全考虑因素。
```

我们首先从保护软件交付流程的方式开始，并讨论了一些与镜像相关的最佳实践。这些包括保护您的镜像注册表、扫描镜像中的漏洞以及对镜像进行加密签名和验证。然后，我们看了一些存在于基础设施堆栈不同层级的工作负载隔离选项。特别是，我们看了集群级别的隔离、节点级别的隔离以及一些不同的运行时隔离选项。我们讨论了身份和访问管理，包括可能有用的额外安全措施的地方。然后我们谈论了审计，并以一个可以通过实施已经涵盖的一些最佳实践来避免的真实世界问题结束。希望您已经掌握足够的内容，可以开始保护自己的Kubernetes集群。

## 术语

这个术语表定义了本书中使用的一些最常见的与Kubernetes相关的术语。如果您认为我漏掉了重要的内容，请告诉我：

- tkb@nigelpoulton.com

```
术语 定义（根据Nigel）
准入控制器 用于验证或修改资源以执行策略的代码。作为API准入链的一部分运行，在身份验证和授权之后立即运行。
```

```
注释 可用于公开alpha或beta功能或与第三方系统集成的对象元数据。
```

```
API 应用程序编程接口。在Kubernetes的情况下，所有资源都在API中定义，它是RESTful的，并通过API服务器公开。
```
```
API组 一组相关的API资源。例如，网络资源通常位于networking.k8s.io API组中。
```

```
API资源 所有Kubernetes对象，如Pods、Deployments和Services，都在API中定义为资源。
```

```
API服务器 在安全端口上通过HTTPS公开API。运行在控制平面上。
```

```
云控制器管理器 与底层云平台集成的控制平面服务。例如，创建负载均衡器服务时，云控制器管理器实现了底层云的面向互联网的负载均衡器的逻辑。
```

```
云原生 一个充满争议的术语，对不同的人有不同的含义。云原生是一种设计、构建和处理现代应用和基础设施的方式。我个人认为，如果一个应用程序能够自我修复、按需扩展、执行滚动更新，可能还支持回滚，那么它就是云原生的。
```

术语 292

```
术语 定义（根据Nigel）
集群 一组工作节点和控制平面节点，共同运行用户应用程序。
```

```
集群存储 控制平面功能，保存集群和应用程序的状态。通常基于etcddistributed数据存储，并运行在控制平面上。可以将其部署到独立的集群中，以获得更高的性能和可用性。
```

```
配置映射 Kubernetes对象，用于保存非敏感的配置数据。在运行时向通用容器添加自定义配置数据的好方法，而无需编辑镜像。
```

```
容器 轻量级环境，用于运行现代应用程序。每个容器都是一个虚拟操作系统，具有自己的进程树、文件系统、共享内存等。一个容器运行一个应用程序进程。
```

```
容器网络接口（CNI） 可插拔接口，用于支持不同的网络拓扑和架构。第三方提供CNI插件，可以实现覆盖网络、BGP网络以及各种实现方式。
```

```
容器运行时 负责拉取容器镜像、启动容器、停止容器和其他低级容器操作的每个集群节点上运行的底层软件。通常为containerd、Docker或cri-o。Docker在Kubernetes 1.20中已被弃用，并在1.24中移除了对其的支持。
```

```
容器运行时接口（CRI） 低级别的Kubernetes功能，允许容器运行时可插拔。使用CRI，您可以根据自己的需求选择最佳的容器运行时（Docker、containerd、cri-o、kata等）。
```

```
容器存储接口（CSI） 接口，使外部第三方存储系统能够与Kubernetes集成。存储供应商编写一个作为一组Pods在集群上运行的CSI驱动/插件，将存储系统的增强功能提供给集群和应用程序。
```

```
containerd 大多数Kubernetes集群中使用的行业标准容器运行时。由Docker公司捐赠给CNCF。发音为“container dee”。
```

术语 293

```
术语 定义（根据Nigel）
控制器 运行作为调谐循环的控制平面进程，监视集群并进行必要的更改，使集群的观察状态与期望状态匹配。
```

```
控制平面 每个Kubernetes集群的核心。包括API、API服务器、调度器、所有控制器等。这些组件在每个集群的所有控制平面节点上运行。
```

```
控制平面节点 托管控制平面服务的集群节点。通常不运行用户应用程序。为了提高可用性，应部署3个或5个。
```

```
cri-o 容器运行时。常用于基于OpenShift的Kubernetes集群。
```

```
CRUD 许多存储系统使用的四个基本操作：创建（Create）、读取（Read）、更新（Update）和删除（Delete）。
```

```
自定义资源定义（CRD） 用于将自定义资源添加到Kubernetes API的API资源。
```

```
数据平面 集群中托管用户应用程序的工作节点。
```

```
部署 控制器，用于部署和管理一组无状态Pod。执行滚动部署和回滚，并能够自我修复。使用ReplicaSet控制器执行扩展和自我修复操作。
```

```
期望状态 集群和应用程序应该具有的状态。例如，应用程序微服务的期望状态可能是五个监听8080/tcp端口的xyz容器的副本。对于调谐很重要。
```
```
终端对象是一个最新的匹配服务标签选择器的健康Pod列表。基本上，它是服务将要发送流量的Pod列表。可能会被EndpointSlices替代。
```

```
etcd是在大多数Kubernetes集群上用作集群存储的开源分布式数据库。
Ingress是一种API资源，可以通过单个面向外部的LoadBalancer服务公开多个内部服务。它在第7层操作，并实现基于路径和基于主机的HTTP路由。
```

术语294

```
Ingress class是一种API资源，允许您在集群上指定多个不同的Ingress控制器。
```

```
Init container是一个在主应用容器启动之前运行并完成的专用容器。通常用于检查/初始化主应用容器的环境。
JSON是JavaScript对象表示法。Kubernetes使用的首选格式，用于发送和存储数据。
```

```
K8s是写Kubernetes的简写方式。数字“8”替代了Kubernetes的“K”和“s”之间的八个字符。发音为“Kates”。人们之所以说Kubernetes的女友叫Kate的原因。
```

```
kubectl是Kubernetes的命令行工具。通过API服务器发送命令并通过API服务器查询状态。
```

```
Kubelet是在每个集群节点上运行的主要Kubernetes代理。它监视API服务器以获取新的工作任务，并维护一个报告通道。
```

```
Kube-proxy在每个集群节点上运行，并实施处理从服务到Pod的流量路由的低级规则。您将流量发送到稳定的服务名称，kube-proxy确保流量到达Pod。
```

```
标签是应用于对象的元数据，用于分组。与标签选择器一起使用以将Pod与更高级别的控制器进行匹配。例如，服务根据一组匹配标签的Pod发送流量。
```

```
标签选择器用于识别要执行操作的Pod。例如，当部署执行滚动更新时，它根据其标签选择器知道要更新哪些Pod - 只有具有与部署的标签选择器匹配的Pod将被替换和更新。
```

```
清单文件是保存一个或多个Kubernetes对象的配置的YAML文件。例如，服务清单文件通常是一个保存服务对象的配置的YAML文件。当您将清单文件发布到API服务器时，其配置将部署到集群中。
```

术语295

```
微服务是现代应用的设计模式。应用功能被分解成自己的小应用程序（微服务/容器），并通过API进行通信。它们共同组成一个有用的应用程序。
```

```
命名空间是将单个Kubernetes集群划分为多个虚拟集群的一种方式。适用于在单个集群上应用不同的配额和访问控制策略。不适用于强大的工作负载隔离。
```

```
节点也称为工作节点。运行用户应用程序的集群中的节点。运行kubelet进程、容器运行时和kube-proxy。
```

```
观察状态也称为当前状态或实际状态。集群和运行应用程序的最新视图。控制器始终在努力使观察状态与期望状态相匹配。
```

```
编排器是一种部署和管理应用程序的软件。现代应用程序由许多小的微服务组成，它们共同组成一个有用的应用程序。Kubernetes编排/管理这些微服务，保持它们的健康状态，按需缩放等等... Kubernetes是基于容器的微服务应用程序的事实上的编排器。
```

```
持久卷（PV）是用于映射集群上的存储卷的Kubernetes对象。在应用程序可以使用之前，必须将外部存储资源映射到PV上。
```

```
持久卷声明（PVC）类似于允许应用程序使用持久卷（PV）的票据/凭证。没有有效的PVC，应用程序无法使用PV。与StorageClasses结合使用以实现动态卷创建。
```

```
Pod是Kubernetes上调度的最小单位。在Kubernetes上运行的每个容器都必须在Pod内运行。Pod提供了共享的执行环境 - IP地址、卷、共享内存等。
```

```
RBAC是基于角色的访问控制。授权模块，确定经过身份验证的用户是否可以对集群资源执行操作。
```

术语296
术语定义（根据Nigel的说法）
和解循环：通过API服务器监视集群状态的控制器进程，确保观察到的状态与期望的状态相匹配。像部署控制器这样的大多数控制器都是作为和解循环运行的。
副本集：作为控制器运行并执行自愈和扩展操作。部署中使用。
REST：表述性状态传输。创建基于Web的API的最常见架构。使用常见的HTTP方法（GET、POST、PUT、PATCH、DELETE）来操作和存储对象。
密钥：用于存储敏感配置数据的配置映射。一种在容器映像之外存储敏感数据并在运行时将其插入到容器中的方式。
服务：以大写字母“S”开头。用于为运行在Pod中的应用程序提供网络访问的Kubernetes对象。通过在一组Pod之前放置一个服务，即使Pod失败、扩展和替换，访问它们的网络端点也不会改变。可以与云平台集成并提供面向互联网的负载均衡器。
服务网格：能够实现Pod与Pod之间流量的加密、增强的网络遥测和高级路由等功能的基础设施软件。与Kubernetes一起使用的常见服务网格包括Consul、Istio、Linkerd和Open Service Mesh。还有其他的。
边车容器：运行在主要应用容器旁边并增强其功能的特殊容器。服务网格通常被实现为注入到Pod中并添加网络功能的边车容器。
有状态副本集：部署和管理有状态Pod的控制器。类似于部署，但用于有状态应用程序。
存储类（SC）：在集群上创建不同存储层级/类别的方式。您可以有一个名为“fast”的SC，它创建基于NVMe的存储，以及另一个名为“medium-three-site”的SC，它创建跨三个站点复制的较慢的存储。
卷：用于持久性存储的通用术语。
WebAssembly（Wasm）：用于执行应用程序的安全沙箱虚拟机格式。
工作节点：用于运行用户应用程序的集群节点。有时被称为“节点”或“工作节点”。
YAML：又一种标记语言。通常用于编写Kubernetes配置文件。它是JSON的超集。

结束语
感谢阅读我的书。现在您已经准备好在云原生世界中蓬勃发展了。

关于封面的说明
我喜欢这本书的封面，感谢那些在设计中投票的数百人。

左侧的YAML代码代表了书籍的技术性质。Kubernetes轮子代表了主题。右侧的垂直符号是云原生图标，采用了《黑客帝国》电影中的数字雨代码风格。里面还有用星际迷航的博格语言写的隐藏信息。

关于书中的图表
在以下GitHub存储库中，有一套很棒的Kubernetes社区图标。

https://github.com/kubernetes/community/tree/master/icons

我喜欢它们，并在博客和视频课程中广泛使用。然而，在印刷版书籍中，它们看起来并不好看。因此，我为书籍创建了自己的类似图标。创建它们花费了很长时间，所以希望您喜欢。

我并不试图取代社区图标或说它们不好。只是在印刷版书籍中，它们看起来不好看。

联系我
我很希望能与您联系，谈论Kubernetes和其他酷炫的技术。

您可以通过以下任何方式与我联系：

- Twitter：twitter.com/nigelpoulton
- LinkedIn：linkedin.com/in/nigelpoulton
- Mastodon：@nigelpoulton@hachyderm.io
- 网站：nigelpoulton.com
- YouTube：youtube.com/nigelpoulton
- 删除我

反馈和评论
书籍的生死取决于评论和评分。

我花了一年多的时间编写这本书并保持其更新。因此，如果您在亚马逊、Goodreads或其他购买书籍的地方留下评论，我会很高兴的。

如果您想提供内容或修复建议，请随时发送电子邮件至ttb@nigelpoulton.com。

索引
占位符，只是为了创建索引。
